This thesis would not have been possible without the support of so many people.
I would like to begin by thanking my advisor, Kilian Weinberger, who encouraged Jake Gardner and I go down this crazy Gaussian process rabbit hole.
I am constantly humbled by Kilian's briliance and ability to synthesize many sub-areas of machine learning, while always finding simple and element solution to his research problems.
Beyond his sound scientific advice, Kilian always offered much-needed support at the most challenging and frustrating moments of my PhD.
Thank you for always being an inspiring mentor, a patient teacher, and a ``funny guy.''

I have been truly fortunate to work with many amazing collaborators.
Andrew Wilson was a guiding force for much of my PhD research, and it was amazing to learn from his deep knowledge of Bayesian methods.
Thank you for your encouragement and for the many insightful discussions about all things machine learning.
To Karthik Sridharan, who always has remarkable insights no matter what the topic is.
To Anil Damel and David Bindel, who have taught me much about numerical linear algebra, and who I can trust to call me out when I make misleading claims.
To Martin Jankowiak and David Eriksson, who---in addition to their brilliance---have been tremendous fun to work with.
(Thanks also for putting up with my coding/math errors.)

Jake Gardner has been an invaluable mentor and a trusty co-pilot on most of the research in this thesis.
He introduced me to much of what I know about Gaussian processes and numerical linear algebra, and was very patient while I struggled through many of the harder concepts.
The genesis of much of this research came from a summer where Jake and I decided to write our own Gaussian process library.
Though Jake was the more senior student, our collaboration back then (and now) felt like an equal partnership.
It has been a true joy working with him, watching our small implementation blossom into several research projects, and I look forward to continuing our work together.

The research presented in this thesis has been developed into an open-source Gaussian process library called GPyTorch.
I would like to thank so many wonderful contributors who have helped with the library's development and research.
To Max Balandat, who has contributed more than anyone else (and who has taught me how to be a better Python developer).
To Etyan Bakshi, who has facilitated a wonderful collaboration and helped expand the applicability of our work.
To Alex Wang, who is always there to help out with a pull request or an issue or a last minute NeurIPS sprint.

It has been a pleasure to be part of Kilian's extended lab group.
To Felix Wu, for his never-give-up attitude; Chuan Guo, for his devotion to rigor; Yu Sun, for his jovial spirit; Gao Huang, for his amazing insightfulness;
Matt Kusner, for his positivity and encouragement; Stephen Tyree, for our fun and effortless collaborations;
Harry Chao, for our always stimulating conversations;
Ruihan Wu, for her stellar mathematical abilities;
Yan Wang, for his ``surprisingly simple'' solutions;
Yurong You, for his computer vision wizardry;
Tianyi Zhang, for his magical ability to balance 10 projects at once;
and Varsha Kishore, for her willingness to join me on my crazy ideas.

Finally, I would like to thank my friends and family, for helping me celebrate the successes and overcome the failures.
My final thank you goes to Andrea Bruns, who's love and consistent support has made it possible for me to cross the finish line.
