%!TEX root=../main.tex
\chapter{Convergence Analysis of Preconditioned mBCG}

\section{Proof of Theorems in Section~\ref{sec:piv_chol_precond}}
\label{app:proofs}
Here we include theorems and proofs about the partial pivoted Cholesky preconditioner applied to mBCG.
All theorems are restated from \cref{sec:piv_chol_precond}.


\subsection{Proof of Lemma~\ref{thm:condition_number}}
\newtheorem*{thm:condition_number}{Lemma~\ref{thm:condition_number} (Restated)}
\begin{thm:condition_number}
  Let $\bar\bL_{R}$ be the rank-$R$ pivoted Cholesky factor of kernel matrix $\bK_{\bX\bX} \in \reals^{N \times N}$.
  If the first $R$ eigenvalues $\lambda_1$, $\ldots$, $\lambda_R$ of $\bK_{\bX\bX}$ satisfy
	\begin{equation*}
		4^{i}\lambda_{i} \leq \bigo{e^{-Bi}}, \quad i \in \{ 1, \:\: \ldots, \:\: R \},
		\tag{\ref{eqn:pcp_condition}}
	\end{equation*}
	for some $B>0$, then the condition number $\kappa(\trainP^{-1}\trainK) \triangleq \Vert \trainP_{k}^{-1}\trainK \Vert_{2} \Vert \trainK^{-1}\trainP_{k} \Vert_{2}$
	satisfies the following bound:
  \begin{align}
    \kappa \left( \trainP^{-1}\trainK \right)
    &\leq \Bigl( 1 + \bigo{\sigma^{-2}_\text{obs} N e^{-BR}} \Bigr)^2
		\nonumber
  \end{align}
	where $\trainP = \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)$ and $\trainK = \left( \bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)$.
\end{thm:condition_number}

\begin{proof}
  Let $\bE$ be the difference between $\bK_{\bX\bX}$ and its rank-$R$ pivoted Cholesky approximation $\bar\bL_R \bar\bL_R^\top$:
  \[
    \bE = \bK_{\bX\bX} - \bar\bL_R \bar\bL_R^\top = \begin{bmatrix} \bzero & \bzero \\ \bzero & \bar \bS_{R+1} \end{bmatrix}
  \]
  where $\bar \bS_{R+1}$ is the Schur compliment that arises as the Cholesky error term---\cref{eqn:chol_error}.
  The error $\bE$ is therefore a positive semi definite matrix.
  By definition, the condition number $\kappa ( \trainP^{-1}\trainK )$ is given by
  \begin{align*}
    \kappa \left( \trainP^{-1}\trainK \right)
    &\triangleq \left\Vert \trainP^{-1}\trainK \right\Vert_{2} \left\Vert \trainK^{-1}\trainP \right\Vert_{2}
  \end{align*}
  %
  The left term can be rewritten as:
  %
  \begin{align*}
    \\
    \left\Vert \trainP^{-1}\trainK \right\Vert_{2}
    &= \left\Vert \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)^{-1} \left(\bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right) \right\Vert_{2}
    \\
    &= \left\Vert \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)^{-1} \left(\bar\bL_R \bar\bL_R^\top + \bE + \sigma^2_\text{obs} \bI \right) \right\Vert_{2}
    \\
    &= \left\Vert \bI + \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)^{-1} \bE \right\Vert_{2}
  \end{align*}
  %
  Similarly, the right term is:
  %
  \begin{align*}
    \left\Vert \trainK^{-1}\trainP \right\Vert_{2}
    &= \left\Vert \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right) \left(\bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)^{-1} \right\Vert_{2}
    \\
    &= \left\Vert \left( \bK_{\bX\bX} - \bE + \sigma^2_\text{obs} \bI \right) \left(\bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)^{-1} \right\Vert_{2}
    \\
    &= \left\Vert \bI - \left(\bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)^{-1} \bE \right\Vert_{2}
  \end{align*}
  %
  Since $\bK_{\bX\bX}$ and $\bar\bL_R \bar\bL_R^\top$ are both positive semi-definite,
  $(\bK_{\bX\bX} + \sigma^2_\text{obs})$ and $(\bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs})$ will both have a minimum eigenvalue $\lambda_\text{min} \geq \sigma^2_\text{obs}$.
  Therefore,
  %
  \[
    \left\Vert \left(\bK_{\bX\bX} + \sigma^2_\text{obs} \right)^{-1} \right\Vert_2 \leq \sigma^{-2}_\text{obs},
    \quad
    \left\Vert \left(\bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \right)^{-1} \right\Vert_2 \leq \sigma^{-2}_\text{obs}.
  \]
  %
  Applying these bound, along with Cauchy-Schwarz and the triangle inequality, gives us
  %
  \begin{align}
    \kappa \left( \trainP^{-1}\trainK \right) \!
    &\leq \! \left( 1 + \left\Vert \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)^{-1} \right\Vert_2 \left\Vert \bE \right\Vert_{2} \right)
      \!\!
      \left( 1 + \left\Vert \left( \bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)^{-1} \right\Vert_2 \left\Vert \bE \right\Vert_{2} \right)
    \nonumber \\
    &\leq
    \left( 1 + \sigma^{-2}_\text{obs} \Vert \bE \Vert_2 \right)
    \left( 1 + \sigma^{-2}_\text{obs} \Vert \bE \Vert_2 \right)
    \nonumber \\
    &= \left( 1 + \sigma^{-2}_\text{obs} \Vert \bE \Vert_2 \right)^2.
    \label{eqn:cond_number_bound}
  \end{align}
  Since $\bE$ is positive semi-definite, we have that $\Vert \bE \Vert_2 \leq \tr{\bE}$.
  The eigenvalue condition from \cref{eqn:pcp_condition} allows us to bound $\tr{\bE}$ using \cref{thm:harbrecht}:
  %
  \begin{equation}
    \Vert \bE \Vert_2 \leq \tr{\bE}
    =
    \tr{ \bK_{\bX\bX} - \bar\bL_R \bar\bL_R^\top } \leq \bigo{N e^{-BR}}.
    \label{eqn:error_trace_bound}
  \end{equation}
  %
  Plugging \cref{eqn:error_trace_bound} into \cref{eqn:cond_number_bound} completes the proof.
\end{proof}




\subsection{Proof of Theorem~\ref{thm:precond_mbcg_solves}}
\newtheorem*{thm:precond_mbcg_solves}{\cref{thm:precond_mbcg_solves} (Restated)}
\begin{thm:precond_mbcg_solves}
  Let $\bK_{\bX\bX} \in \reals^{N \times N}$ be a $N \times N$ kernel that satisfies the eigenvalue condition of \cref{eqn:pcp_condition},
	and let $\bar\bL_R$ be its rank-$R$ pivoted Cholesky factor.
	After $J$ iterations of mBCG with preconditioner $\trainP = (\bar\bL_R \bar\bL_R^\top + \sigma_\text{obs}^2 \bI)$,
	the difference between $\bc_J$ and true solution $\trainK^{-1} \by$ is bounded by:
	%
  \begin{equation*}
    \left \Vert \trainK^{-1} \by - \bc_{J} \right \Vert_{\trainK}
    \leq \Bigg[ \frac 1 {1 + \bigo{\sigma^{2}_\text{obs} e^{RB}/N}} \Bigg]^{J}
    \left \Vert \trainK^{-1} \by \right \Vert_{\trainK},
		\nonumber
  \end{equation*}
	%
	where $\trainK = (\bK_{\bX\bX} + \sigma^2_\text{obs} \bI)$ and $B > 0$ is a constant.
\end{thm:precond_mbcg_solves}

\begin{proof}
Since \cref{eqn:pcp_condition} holds, we can simply plug \cref{thm:condition_number} into the standard CG convergence bound (\cref{thm:cg_convergence}):
%
\begin{align*}
  \left \Vert \trainK^{-1} \by - \bc_{J} \right \Vert_{\trainK}
  &\leq
  2 \left[ \frac{ \sqrt{ \kappa\left( \trainP^{-1} \trainK \right)}  - 1 }{\sqrt{ \kappa \left( \trainP^{-1} \trainK \right)} + 1} \right]^J
  \left \Vert \trainK^{-1} \by \right \Vert_{\trainK}
  \\
  &\leq
  2 \Bigg[ \frac{ 1 + \bigo{ \sigma^{-2}_\text{obs} N e^{-BR} } - 1 }{ 1 + \bigo{ \sigma^{-2}_\text{obs} N e^{-BR} } + 1} \Bigg]^J
  \left \Vert \trainK^{-1} \by \right \Vert_{\trainK}
  \\
  &=
  \Bigg[ \frac 1 {1 + \bigo{\sigma^{2}_\text{obs} e^{RB}/N}} \Bigg]^{J}
  \left \Vert \trainK^{-1} \by \right \Vert_{\trainK}.
\end{align*}
\end{proof}




\subsection{Proof of Theorem~\ref{thm:precond_mbcg_logdet}}
\newtheorem*{thm:precond_mbcg_logdet}{\cref{thm:precond_mbcg_logdet} (Restated)}
\begin{thm:precond_mbcg_logdet}
  Assume $\bK_{\bX\bX} \in \reals^{N \times N}$ satisfies the eigenvalue condition of \cref{eqn:pcp_condition}.
	Suppose we estimate $\Gamma \approx \log \vert \trainP^{-1} \trainK \vert$ using \cref{eqn:slq_precond} with:
	\begin{itemize}
		\item $J \geq \mathcal{O} \left[ (1 + \sigma^{-2}_\text{obs} N e^{-BR}) \log \left( ( 1 + \sigma^{-2}_\text{obs} N e^{-BR} ) / \epsilon \right) \right]$ iterations of mBCG (for some constant $B > 0$), and
		\item $T \geq \frac{32}{\epsilon^2} \log \left( \frac 2 \delta \right)$ random $\bz^{(i)} \sim \normaldist{\bzero}{\trainP}$ vectors.
	\end{itemize}
  Then the error of the stochastic Lanczos quadrature estimate $\Gamma$ is probabilistically bounded by:
  \begin{equation*}
    \textrm{Pr}\left[\Bigl\vert \log \vert \trainP^{-1} \trainK \vert - \Gamma \Bigr\vert \leq \epsilon N \right] \geq \left( 1 - \delta \right).
  \end{equation*}
\end{thm:precond_mbcg_logdet}

\begin{proof}
Since \cref{eqn:pcp_condition} holds, we can simply plug \cref{thm:condition_number} into the stochastic Lanczos quadrature bound of \citet{ubaru2017fast} (\cref{thm:slq_convergence}).
\end{proof}









\section{Applying Theorems~\ref{thm:precond_mbcg_solves} and \ref{thm:precond_mbcg_logdet} to Univariate RBF Kernels}
\label{app:univariate_rbf}

Our convergence theory depends on the assumption in \cref{thm:condition_number} that the eigenvalues of $\bK_{\bX\bX}$ decay exponentially.
A natural question is when this assumption holds in practice.
%There has been an enormous amount of work understanding the spectra of kernel functions (e.g., \cite{wathen2015spectral}).
It so happens that one can prove a concrete bound on the eigenvalue distribution of univariate RBF kernels:
%
\begin{lemma}[Lemma 3 of \citet{gardner2018gpytorch}]
\label{thm:eigenvalue_bound}
Given $x^{(1)}, \ldots, x^{(N)} \in [0, 1]$, the univariate RBF kernel matrix\footnote{
  Here we drop the multiplicative outputscale parameter $o^2$ without loss of generality.
}
$\bK_{\bX\bX} \in \reals^{N \times N}$ with $K^{(ij)} = \exp \left(-(x^{(i)} - x^{(j)})^{2} / \ell^2 \right)$ has eigenvalues $\lambda_1, \ldots, \lambda_k, \ldots, \lambda_N$ bounded by:
\begin{equation*}
  \lambda_{2k+1} \leq
  2N e^{-\ell^2/4} I_{k+1}(\gamma/4) \sim
  \frac{2N e^{-\ell^2/4}}{\sqrt{\pi}\ell}
  \left( \frac{e\ell^2}{8(k+1)} \right)^{k+1},
\end{equation*}
where $I_{k+1}$ denotes the modified Bessel function of the first kind with parameter $k + 1$.
\end{lemma}
%
\noindent
In other words, the eigenvalues of an RBF kernel matrix $\bK_{\bX\bX}$ decay \emph{super-exponentially}, meeting the requirements of \cref{eqn:pcp_condition} in \cref{thm:condition_number}.
Therefore, the bounds given by \cref{thm:precond_mbcg_solves,thm:precond_mbcg_logdet} apply.

A proof of \cref{thm:eigenvalue_bound} was included in a version of \cref{chapter:bbmm} that was published at NeurIPS 2018 \citep{gardner2018gpytorch}.
The proof itself is the work of my co-authors David Bindel and Jacob R. Gardner---therefore I choose not to claim credit for it as part of this thesis.
It can be found in Appendix E of \citep{gardner2018gpytorch}.

We would also note that, while many kernels do not meet the eigenvalue criterion of \cref{thm:condition_number}, most kernels have rapidly decaying eigenvalues and therefore achieve significantly faster convergence with the partial pivoted Cholesky preconditioner.
This is demonstrated by the empirical results in \cref{sec:bbmm_results} and \cref{sec:largeexact_results}.
