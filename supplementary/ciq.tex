\chapter{Details on the Cauchy Integral Quadrature}
\label{app:quadrature}

\section{Selecting Quadrature Locations and Weights}
Here we briefly describe the quadrature formula derived by \citet{hale2008computing} for use with Cauchy's integral formula.
We refer the reader to the original publication for more details.

Assume that $\bK$ is a positive semi-definite matrix, and thus has real non-negative eigenvalues.
Our goal is to approximate Cauchy's integral formula with a quadrature estimate:
%
\begin{align}
	f(\bK)
  &= \frac{1}{2 \pi i} \oint_\Gamma f(\tau) \left( \tau \bI - \bK \right)^{-1} \intd \tau
  \label{eqn:contour_integral_2}
  \\
  &\approx
  \frac{1}{2 \pi i} \sum_{q=1}^Q \widetilde w_q f(\tau_q) \left( \tau_q \bI - \bK \right)^{-1},
  \label{eqn:contour_integral_quad_2}
\end{align}
%
where $f(\cdot)$ is an arbitrary matrix function, and $\widetilde w_q$ and $\tau_q$ are quadrature weights and locations respectively.
Note that \cref{eqn:contour_integral_2} holds true for any contour $\Gamma$ in the complex plane that encompasses the spectrum of $\bK$.

\paragraph{A na\"ive approach with uniformly-spaced quadrature.}
For now, assume that $\lambda_\text{min}$ and $\lambda_\text{max}$---the minimum and maximum eigenvalues of $\bK$---are known.
(We will later address how they can be efficiently estimated.)
A na\"ive first approach to \cref{eqn:contour_integral_quad_2} is to uniformly place the quadrature locations in a circle that surrounds the eigenvalues:
%
\[
  \tau_q = \lambda_\text{min} + \frac 1 2 \left( \lambda_\text{max} - \lambda_\text{min} \right) \left( 1 + e^{2 i \pi \left( q / Q \right)} \right),
  \quad
  \widetilde w_q = \frac 1 Q.
\]
%
This corresponds to a standard midpoint quadrature rule.
However, \citet{hale2008computing} demonstrate that the convergence of this quadrature rule depends linearly on the condition number $\kappa(\bK) = \lambda_\text{max} / \lambda_\text{min}$.
As many kernel matrices tend to be approximately low-rank and therefore ill-conditioned, this simple quadrature rule would require a large $Q$ for numerical accuracy.

\paragraph{Improving convergence with a change of variables.}
Rather than uniformly spacing the quadrature points, it makes more sense to place more quadrature points near $\lambda_\text{min}$ and fewer near $\lambda_\text{max}$.
This can be accomplished by using the above midpoint quadrature rule in a \emph{transformed parameter space} that is ``stretched'' near $\lambda_\text{min}$ and contracted near $\lambda_\text{max}$.
In particular, we will use a change-of-variables that exploits the geometry of the square root function for rapid convergence.

\subsection{A Specific Quadrature Formula for $f(\bK) = \bK^{-1/2}$}
\citet{hale2008computing} suggest performing a change of variables that projects \cref{eqn:contour_integral_2} onto an annulus.
Uniformly spaced quadrature points inside the annulus will cluster near $\lambda_\text{min}$ when projected back into the complex plane.
This change of variables has a simple analytic formula involving Jacobi elliptic functions (see \citep[][Sec. 2]{hale2008computing} for details.)
In the special case of $f(\bK) = \bK^{-1/2}$, we can utilize an additional change of variables for an even more efficient quadrature formulation \citep[][Sec. 4]{hale2008computing}.
Setting $\sigma = \tau^{1/2}$, we have
%
\begin{align}
	\bK^{-\frac 1 2}
  &= \frac{1}{\pi i} \oint_{\Gamma_s} \left( \sigma^2 \bI - \bK \right)^{-1} \intd \sigma.
  \nonumber
  \\
  &\approx
  \frac{1}{\pi i} \sum_{q=1}^Q \widetilde w_q \left( \sigma_q^2 \bI - \bK \right)^{-1},
  \label{eqn:contour_integral_quad_3}
\end{align}
%
where $\Gamma_\sigma$ is a contour that surrounds the spectrum of $\bK^{1/2}$.
Since the integrand is symmetric with respect to the real axis, we only need to consider the imaginary portion of $\Gamma_\sigma$.
Consequentially, all the $\tau_q$ quadrature locations (back in the original space) will be real-valued and negative.
Combining this square-root change-of-variables with the annulus change-of-variables results in the following quadrature weights/locations:
%
\begin{equation}
  \begin{split}
    \sigma_q^2
    &= \lambda_\text{min}^{-1} \Bigl( \text{sn}(i u_q \mathcal{K}'(k) \mid k) \Bigr)^2,
    \\
    \widetilde w_q
    &= -\frac{ 2 }{ \pi Q \sqrt{\lambda_\text{min}} }
    \:\: \left[
    \mathcal{K}'( k )
    \:\:\: \text{cn} \left( i u_q \mathcal{K}'(k) \mid k \right)
    \:\:\: \text{dn} \left( i u_q \mathcal{K}'(k) \mid k \right)
    \right],
  \end{split}
  \label{eqn:quad_points_and_locations}
\end{equation}
%
where we adopt the following notation:
\begin{itemize}
  \item $k = \sqrt{ \lambda_\text{min} / \lambda_\text{max} } = \sqrt{ \kappa(\bK^{-1}) }$;
  \item $\mathcal{K}'(k)$ is the complete elliptic integral of the first kind with respect to the complimentary elliptic modulus $\sqrt{1 - k^2}$;
  \item $u_q = \frac{1}{Q}(q - \frac 1 2)$; and
  \item $\text{sn}(\cdot \mid k)$, $\text{cn}(\cdot \mid k )$, and $\text{dn}(\cdot \mid k)$ are the Jacobi elliptic functions with respect to elliptic modulus $k$.
\end{itemize}
%
The weights $\widetilde w_q$ and locations $\sigma_q^2$ from \cref{eqn:quad_points_and_locations} happen to be real-valued and negative.
Setting $t_q = -\sigma_q^2$ and $w_q = -\widetilde w_q$ gives us:
%
\begin{equation}
	\bK^{-\frac 1 2} \approx \sum_{q=1}^Q w_q \left( t_q \bI + \bK \right)^{-1}, \quad w_q = -\widetilde w_q > 0, \quad t_q = -\sigma_q^2 > 0.
  \label{eqn:contour_integral_quad_4}
\end{equation}
%
Note that the shifted matrices $(t_q \bI + \bK)$ are all positive definite.

\paragraph{Convergence of the quadrature approximation.}
Due to the double change-of-variables, the convergence of this quadrature in \cref{eqn:quad_points_and_locations} is extremely rapid---even for ill-conditioned matrices.
\citeauthor{hale2008computing} prove the following error bound:
%
\begin{lemma}[\citet{hale2008computing}, Thm. 4.1]
  Let $t_1$, $\ldots$, $t_Q > 0$ and $w_1$, $\ldots$, $w_Q > 0$ be the locations and weights of \citeauthor{hale2008computing}'s quadrature procedure (see \cref{app:quadrature}).
  The error of \cref{eqn:contour_integral_quad} is bounded by:
  \[
    \left\Vert \bK \sum_{q=1}^Q w_q \left( t_q \bI + \bK \right)^{-1} - \bK^{\frac 1 2} \right\Vert_2
    \leq \bigo{\exp\left( -\frac  {2 Q \pi^2}{\log \kappa(\bK) + 3} \right)},
  \]
  where $\kappa(\bK) = \lambda_\text{max} / \lambda_\text{min}$ is the condition number of $\bK$.
\label{lemma:hale}
\end{lemma}
%
\noindent
Remarkably, the error of \cref{eqn:contour_integral_quad} is \emph{logarithmically} dependent on the conditioning of $\bK$.
Consequentially, $Q\approx10$ quadrature points is even sufficient for ill-conditioned matrices (e.g. $\kappa(\bK) \approx 10^4$).

\subsection{Estimating the Minimum and Maximum Eigenvalues}
The equations for the quadrature weights/locations requires on knowing the extreme eigenvalues $\lambda_\text{max}$ and $\lambda_\text{min}$.
Using the Lanczos algorithm (\cref{sec:lanczos}), we can obtain accurate estimates of these extreme eigenvalues through $\approx 10$ matrix-vector multiplies with $\bK$.
Recall that the Lanczos algorithm forms the relation $\bQ^\top_J \bK \bQ_J = \bT_J$, where $\bQ_J$ is orthonormal and $\bT_J$ is tridiagonal.
To estimate $\lambda_\text{min}$ and $\lambda_\text{max}$ from Lanczos, we perform an eigendecomposition of $\bT_J$.
If $J$ is small (i.e. $J \approx 10$) then this eigendecomposition will require minimal computational resources.
A well-known convergence result of the Lanczos algorithm is that the extreme eigenvalues of $\bT_J$ tend to converge rapidly to $\lambda_\text{min}$ and $\lambda_\text{max}$ of $\bK$ \citep[e.g.][]{saad2003iterative,golub2012matrix}.


\subsection{The Complete Quadrature Algorithm}
\cref{alg:quadrature} obtains the quadrature weights $w_q$ and locations $t_q$ corresponding to \cref{eqn:quad_points_and_locations,eqn:contour_integral_quad_4}.
Computing these weights requires $\approx 10$ matrix-vector multiplies with $\bK$---corresponding to the Lanczos iterations---for a total time complexity of $\bigo{N}$.
All computations involving elliptic integrals can be readily computed using routines available in e.g. the SciPy library.

\input algorithms/quadrature

\section{Convergence Analysis of CIQ}

To prove the convergence result in \cref{thm:ciq_convergence}, we first prove the following lemma about msMINRES convergence.

\begin{lemma}
  Let $c^{(1)}_J$, $\ldots$, $c^{(Q)}$ be the outputs after $J$ iterations of msMINRES with input $\bK$, $\bb$, and shifts $t_1$, $\ldots$, $t_Q$.
  The residuals of the solves are bounded by
  %
  \begin{align*}
    \bigl\Vert (\bK - t_q \bI) \bc_J^{(q)} - \bb \bigr\Vert_2
    &\leq 2 \left( \frac{
      \sqrt{\kappa(\bK)} - 1
    }{
      \sqrt{\kappa(\bK)} + 1
    }\right)^J
    \Vert \bb \Vert_2,
    q \in [1, Q],
	\end{align*}
  %
  where $\kappa(\bK)$ is the condition number of $\bK$.
  \label{lemma:minres}
\end{lemma}
%
\begin{proof}
  The convergence proof uses a polynomial bound, which is the standard approach for Krylov algorithms.\footnote{
    \citet{paige1975solution} do not provide a detailed convergence analysis of MINRES.
    However, proving the following result is a straightfoward analog of the famous conjugate gradients convergence analysis.
  }
  See \citep[e.g.][]{shewchuk1994introduction,trefethen1997numerical,saad2003iterative} for an analogous proof for the conjugate gradients method.

	At iteration $J$, the msMINRES algorithm produces:
  %
	\begin{align}
    \bc^{(q)}_J
    = \argmin_{\bc^{(q)} \in \mathcal{K}_J(\bK, \bb)} \Bigl[
      \bigl\Vert (\bK - t_q \bI) \bc^{(q)} - \bb \bigr\Vert_2
    \Bigr],
    \quad
    q \in [1, Q],
    \label{eqn:minres_krylov}
	\end{align}
  %
  where we assume $\bc_0^{(q)} = \bzero$ for simplicity.
  Since the Krylov subspace $\mathcal{K}^{(j)} (\bK, \bb)$ contains powers of $\bK$ applied to the vector $\bb$, we can rewrite
  %
  $(\bK - t_q \bI) \bc^{(q)} - \bb$
  as a polynomial of $\bK$ applied to $\bb$:
  \[ (\bK - t_q \bI) \bc^{(q)} - \bb = \sum_{j=0}^J \alpha_j^{(q)} \bK^j \bb, \quad \alpha_j \in \reals \]
  for some coefficients $\alpha_0$, $\ldots$, $\alpha_J$.
  Therefore, \cref{eqn:minres_krylov} can be reframed as an optimal polynomial problem:
  %
	\begin{align*}
    %\min_{\bc^{(q)} \in \mathcal{K}_J(\bK, \bb)} \Bigl[
      %\bigl\Vert (\bK - t_q \bI) \bc^{(q)} - \bb \bigr\Vert_2
    %\Bigr]
    \bigl\Vert (\bK - t_q \bI) \bc_J^{(q)} - \bb \bigr\Vert_2
    &= \min_{P_q \in \mathcal{P}_J} \Bigl[
      \Vert P_q(\bK) \bb \Vert_2
    \Bigr],
    \quad
    q \in [1, Q]
    \\
    &\leq \min_{P_q \in \mathcal{P}_J} \Bigl[
      \Vert P_q(\bK) \Vert_2
    \Bigr] \Vert \bb \Vert_2
    \\
    &= \min_{P_q \in \mathcal{P}_J} \Bigl[
      \sigma^{(\max)}_{P_q(\bK)}
    \Bigr] \Vert \bb \Vert_2,
  \end{align*}
  where $\mathcal{P}_J$ is the class of $J$-degree polynomials such that $P(0) = 1$ for any $P \in \mathcal{P}_J$,
  and $\sigma^{(\max)}_{P_q(\bK)}$ is the maximum singular value of the matrix $P_q(\bK)$.
  Since $\bK$ is symmetric, we have that $\sigma^{(\max)}_{P_q(\bK)}$ must be equal to $P_q(\sigma^{(i)}_{\bK})$, where $\sigma^{(i)}_{\bK}$ is a singular value of $\bK$.
  Therefore,
  %
  \begin{align}
    \bigl\Vert (\bK - t_q \bI) \bc_J^{(q)} - \bb \bigr\Vert_2
    &\leq \min_{P_q \in \mathcal{P}_J} \Bigl[
      \max_{i}
      \left\vert P_q(\sigma^{(i)}_\bK) \right\vert
    \Bigr] \Vert \bb \Vert_2,
    \quad
    q \in [1, Q].
    \label{eqn:bound}
	\end{align}
  %
  We can replace the minimum in \cref{eqn:bound} with any $J^\text{th}$ degree polynomial $P(\cdot)$ that satisfies $P(0) = 1$.
  One natural choice is the Chebychev polynomial $T_J(\cdot)$ where the input is shifted/scaled so that $\vert T_j(\cdot) \vert \leq 1$ on the domain $\sigma^{(\min)}_{\bK} \leq (\cdot) \leq \sigma^{(\max)}_{\bK}$.
  For this choice of polynomial, we have that
  %
  \begin{equation}
    T_J( \gamma )
    \leq 2 \left( \frac{
      \sqrt{\kappa(\bK)} - 1
    }{
      \sqrt{\kappa(\bK)} + 1
    }\right)^J,
    \quad
    \gamma \in [\sigma^{(\min)}_{\bK}, \sigma^{(\max)}_{\bK}]
    \label{eqn:chebychev}
  \end{equation}
  (See \citep[e.g.][Sec. 9.2]{shewchuk1994introduction} or \citep[e.g.][Thm. 38.5]{trefethen1997numerical} for a detailed derivation.)
  Plugging \cref{eqn:chebychev} into \cref{eqn:bound} as an upper bound on $P_q(\cdot)$ completes the proof.
\end{proof}

\cref{lemma:minres} is a very loose bound, as it doesn't assume anything about the spectrum of $\bK$.
In practice, we find that smMINRES converges typically in $J \approx 100$ for kernel matrices, even when the conditioning is on the order of $\kappa(\bK) \approx 10^4$.
This convergence is faster with preconditioning.
With this lemma we are now able to prove our primary CIQ convergence result:
%
\newtheorem*{thm:ciq_convergence}{Theorem~\ref{thm:ciq_convergence} (Restated)}
\begin{thm:ciq_convergence}
  Let $\bK$ and $\bb$ be the inputs to \cref{alg:ciq}, producing the output $\bd_J \approx \bK^{1/2} \bb$ after $J$ iterations.
  The difference between $\bd_J$ and $\bK^{1/2} \bb$ is bounded by:
  %
  \begin{equation*}
    \left\Vert \bd_J - \bK^{\frac 1 2} \bb \right\Vert_2
    \leq
    \overbracket{
      \bigo{\exp\left( -\frac{2 Q \pi^2}{\log \kappa(\bK) + 3} \right)}
    }^{\text{Quadrature error}}
    +
    \overbracket{
      2 \sum_{q=1}^Q \left\vert w_q \right\vert
      \left( \frac{ \sqrt{\kappa(\bK)} - 1}{ \sqrt{\kappa(\bK)} + 1} \right)^J
      \left\Vert \bb \right\Vert_2
    }^{\text{msMINRES error}}
  \end{equation*}
  %
  where $Q$ is the number of quadrature points and $\kappa(\bK)$ is the condition number of $\bK$.
\end{thm:ciq_convergence}
%
\begin{proof}
  First we note that the CIQ solution $\bd_J$ can be written as $\sum_{i=1} w_q \bc^{(q)}_J$, where $\bc^{(q)}_J$ is the $q^\text{th}$ shifted solve $\approx (t_q \bI - \bK)^{-1} \bb$ from msMINRES.
  This theorem then simply applies the triangle inequality several times:
  %
  \begin{align*}
    \left\Vert \bd_J - \bK^{\frac 1 2} \bb \right\Vert_2
    &=
    \left\Vert \overbracket{\sum_{q=1}^Q w_q \bc^{(q)}_J - \left( \bK \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1} \right) \bb }^{\text{msMINRES error}} \right.
    \\
    &\phantom{=} \quad \left. + \underbracket{\left( \bK \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1} \right) \bb - \bK^{\frac 1 2} \bb}_{\text{Quadrature error}} \right\Vert_2
    \\
    &\leq \sum_{q=1}^Q \vert w_q \vert \left\Vert \bc^{(q)}_J - \bK \left( t_q \bI - \bK \right)^{-1} \bb \right\Vert_2
    \\
    &\phantom{=} \:\: + \left\Vert \bK \left( \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1} \right) \bb - \bK^{\frac 1 2} \bb \right\Vert_2
  \end{align*}
  %
  Bounding the quadrature error with \cref{lemma:hale} and bounding the shifted solve errors with \cref{lemma:minres} completes the proof.
\end{proof}
