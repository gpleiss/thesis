\chapter{Details on the Cauchy Integral Quadrature}
\label{app:quadrature}

\section{Selecting Quadrature Locations and Weights}
Here we briefly describe the quadrature formula derived by \citet{hale2008computing} for use with Cauchy's integral formula.
We refer the reader to the original publication for more details.

Assume that $\bK$ is a positive semi-definite matrix, and thus has real non-negative eigenvalues.
Our goal is to approximate Cauchy's integral formula with a quadrature estimate:
%
\begin{align}
	f(\bK)
  &= \frac{1}{2 \pi i} \oint_\Gamma f(t) \left( t \bI - \bK \right)^{-1} \intd t
  \label{eqn:contour_integral_2}
  \\
  &\approx
  \frac{1}{2 \pi i} \sum_{i=1}^Q w'_q f(t_q) \left( t_q \bI - \bK \right)^{-1},
  \label{eqn:contour_integral_quad_2}
\end{align}
%
where $f(\cdot)$ is an arbitrary matrix function, $w_q'$ and $t_q$ are quadrature weights and locations respectively.
Note that \cref{eqn:contour_integral_2} holds true for any contour $\Gamma$ in the complex plane that encompasses the spectrum of $\bK$.

\paragraph{A na\"ive approach with uniformly-spaced quadrature.}
For now, assume that $\lambda_\text{min}$ and $\lambda_\text{max}$---the minimum and maximum eigenvalues of $\bK$---are known.
(We will later address how they can be efficiently estimated.)
A na\"ive first approach to \cref{eqn:contour_integral_quad_2} is to uniformly place the quadrature locations in a circle that surrounds the eigenvalues:
%
\[
  t_q = \lambda_\text{min} + \frac 1 2 \left( \lambda_\text{max} - \lambda_\text{min} \right) \left( 1 + e^{2 i \pi \left( q / Q \right)} \right),
  \quad
  w'_q = \frac 1 Q.
\]
%
This corresponds to a standard trapezoidal quadrature rule.
However, \citet{hale2008computing} demonstrate that the convergence of this quadrature rule depends linearly on the condition number $\kappa(\bK) = \lambda_\text{max} / \lambda_\text{min}$.
As many kernel matrices tend to be approximately low-rank and therefore ill-conditioned, this simple quadrature rule would require a large $Q$ for numerical accuracy.

\paragraph{Improving convergence with a change of variables.}
Rather than uniformly spacing the quadrature points, it makes more sense to place more quadrature points near $\lambda_\text{min}$ and fewer near $\lambda_\text{max}$.
Intuitively, shifts near $\lambda_\text{max}$ will closely approximate a constant scaling matrix and so differences between these shifts will be minimal.
To accomplish this, \cite{hale2008computing} suggest performing a change of variables that projects \cref{eqn:contour_integral_2} onto an annulus.
Uniformly spaced quadrature points inside the annulus will cluster near $\lambda_\text{min}$ when projected back into the complex plane.
Importantly, this change of variables has a simple analytic formula involving Jacobi elliptic functions.
(See \citep[][Sec. 2]{hale2008computing} for details.)

\subsection{A Specific Quadrature Formula for $f(\bK) = \bK^{-1/2}$}
In the special case of $f(\bK) = \bK^{-1/2}$, we can utilize an additional change of variables for an even more efficient quadrature formulation \citep[][Sec. 4]{hale2008computing}.
Setting $s = t^{1/2}$, we have
%
\begin{align}
	\bK^{-\frac 1 2}
  &= \frac{1}{\pi i} \oint_{\Gamma_u} \left( s^2 \bI - \bK \right)^{-1} \intd s.
  \nonumber
  \\
  &\approx
  \frac{1}{2 \pi i} \sum_{i=1}^Q w_q \left( s_q^2 \bI - \bK \right)^{-1},
  \label{eqn:contour_integral_quad_3}
\end{align}
%
Since the integrand is symmetric with respect to the real axis, we only need to consider a contour of $s$ that exclusively lies on the imaginary axis.
Consequentially, all the $t_q$ quadrature locations (back in the original space) will be real-valued.
Combining this with the annulus change of variables results in the following quadrature weights/locations for \cref{eqn:contour_integral_quad_3}:
%
\begin{equation}
  \begin{split}
    t_q = s_q^2
    &= \lambda_\text{min}^{-1} \Bigl( \text{sn}_k(u_q) \Bigr)^2,
    \\
    w_q
    &= -\frac{ 2 \lambda_\text{min}^{-1/2}  }{ \pi Q }
    \: \left[
    \mathcal{K}'( k )
    \: \text{cn}_k \left( u_q \right)
    \: \text{dn}_k \left( u_q \right)
    \right],
  \end{split}
  \label{eqn:quad_points_and_locations}
\end{equation}
%
where we adopt the following notation:
\begin{itemize}
  \item $k = \lambda_\text{min} / \lambda_\text{max} = \kappa(\bA^{-1})$;
  \item $\mathcal{K}'(k)$ is the complete elliptic integral of the first kind with respect to the complimentary elliptic modulus $\sqrt{1 - k^2}$;
  \item $u_q = \frac{i}{Q}(q - \frac 1 2) \mathcal{K}'(k)$; and
  \item $\text{sn}_k(\cdot)$, $\text{cn}_k(\cdot)$, and $\text{dn}_k(\cdot)$ are the Jacobi elliptic functions with respect to elliptic modulus $k$.
\end{itemize}
Using these values of $w_q$ and $t_q$ leads to the $\log(\lambda_\text{max} / \lambda_\text{min})$ convergence in \cref{lemma:hale}.

\subsection{Estimating the Minimum and Maximum Eigenvalues}
The equations for the quadrature weights/locations requires on knowing the extreme eigenvalues $\lambda_\text{max}$ and $\lambda_\text{min}$.
Using the Lanczos algorithm \cite{lanczos1950iteration}---which is a Krylov subspace method---we can obtain accurate estimates of these extreme eigenvalues through $\approx 10$ matrix-vector multiplies with $\bK$.

\paragraph{The Lanczos algorithm}
is a method for computing a partial tridiagonalization of a symmetric matrix $\bA$.
Given an initial vector $\bb$, the algorithm iteratively factorizes $\bA$ as:
\[
  \bA \bQ_J = \bQ_J \bT_J + \br_J \be_J^\top
\]
where $\be_J$ is a unit vector, and
%
\begin{itemize}
  \item $\bQ_J \in \reals^{N \times J}$ is an orthonormal basis of the $J^\text{th}$ Krylov subspace $\mathcal{K}(\bA, \bb)$,
	\item $\bT_J \in \reals^{J \times J}$ is a symmetric tridiagonal matrix, and
	\item $\br_J \in \reals^J$ is a residual term.
\end{itemize}
%
At a high level, the Lanczos iterations first form the $J^\text{th}$ Krylov subspace and then perform Gram Schmidt orthogonalization:
\[
  \text{span} \{ \bq^{(1)}, \:\: \ldots, \:\: \bq^{(J)} \} = \mathcal{K}(\bA, \bb) = \text{span} \{ \bb, \:\: \bA\bb, \:\: \bA^2\bb, \:\: \ldots, \:\: \bA^{J-1} \bb \}.
\]
The orthogonal basis vectors are collected into $\bQ$ and the orthogonalization coefficients are collected into $\bT$.
Due to the symmetry of $\bA$, each vector $\bq^{(j)}$ only has to be orthogonalized against the two previous basis vectors $\bq^{(j-1)}$, $\bq^{(j-2)}$---which results in the tridiagonal structure of $\bT$.

\paragraph{Estimating Extreme Eigenvalues from Lanczos.}
To estimate $\lambda_\text{min}$ and $\lambda_\text{max}$ from Lanczos, we perform an eigendecomposition of $\bT_J$.
If $J$ is small (i.e. $J \approx 10$) then this eigendecomposition will require minimal computational resources.
A well-known convergence result of the Lanczos algorithm is that the extreme eigenvalues of $\bT_J$ tend to converge rapidly to $\lambda_\text{min}$ and $\lambda_\text{max}$ \cite[e.g.][]{saad2003iterative,golub2012matrix}.


\subsection{The Complete Quadrature Algorithm}
\cref{alg:quadrature} obtains the quadrature weights $w_q$ and locations $t_q$ corresponding to the approach of \citet{hale2008computing}.
The convergence of this quadrature is extremely rapid (as demonstrated by \cref{lemma:hale}), typically requiring no more than $Q \approx 15$ for high accuracy.
Computing these weights requires $\approx 10$ matrix-vector multiplies with $\bK$---corresponding to the Lanczos iterations---for a total time complexity of $\bigo{N}$.
All computations involving elliptic integrals can be readily computed using routines available in e.g. the SciPy library.

\input algorithms/quadrature

\section{Convergence Analysis of CIQ}

To prove the convergence result in \cref{thm:ciq_convergence}, we first prove the following lemma about msMINRES convergence.

\begin{lemma}
  Let $c^{(1)}_J$, $\ldots$, $c^{(Q)}$ be the outputs after $J$ iterations of msMINRES with input $\bA$, $\bb$, and shifts $t_1$, $\ldots$, $t_Q$.
  The residuals of the solves are bounded by
  %
  \begin{align*}
    \bigl\Vert (\bA - t_q \bI) \bc_J^{(q)} - \bb \bigr\Vert_2
    &\leq 2 \left( \frac{
      \sqrt{\kappa(\bA)} - 1
    }{
      \sqrt{\kappa(\bA)} + 1
    }\right)^J
    \Vert \bb \Vert_2,
    q \in [1, Q],
	\end{align*}
  %
  where $\kappa(\bA)$ is the condition number of $\bA$.
  \label{lemma:minres}
\end{lemma}
%
\begin{proof}
  The convergence proof uses a polynomial bound, which is the standard approach for Krylov algorithms.\footnote{
    \citet{paige1975solution} do not provide a detailed convergence analysis of MINRES.
    However, proving the following result is a straightfoward analog of the famous conjugate gradients convergence analysis.
  }
  See \citep[e.g.][]{shewchuk1994introduction,trefethen1997numerical,saad2003iterative} for an analogous proof for the conjugate gradients method.

	At iteration $J$, the msMINRES algorithm produces:
  %
	\begin{align}
    \bc^{(q)}_J
    = \argmin_{\bc^{(q)} \in \mathcal{K}_J(\bA, \bb)} \Bigl[
      \bigl\Vert (\bA - t_q \bI) \bc^{(q)} - \bb \bigr\Vert_2
    \Bigr],
    \quad
    q \in [1, Q],
    \label{eqn:minres_krylov}
	\end{align}
  %
  where we assume $\bc_0^{(q)} = \bzero$ for simplicity.
  Since the Krylov subspace $\mathcal{K}^{(j)} (\bA, \bb)$ contains powers of $\bA$ applied to the vector $\bb$, we can rewrite
  %
  $(\bA - t_q \bI) \bc^{(q)} - \bb$
  as a polynomial of $\bA$ applied to $\bb$:
  \[ (\bA - t_q \bI) \bc^{(q)} - \bb = \sum_{j=0}^J \alpha_j^{(q)} \bA^j \bb, \quad \alpha_j \in \reals \]
  for some coefficients $\alpha_0$, $\ldots$, $\alpha_J$.
  Therefore, \cref{eqn:minres_krylov} can be reframed as an optimal polynomial problem:
  %
	\begin{align*}
    %\min_{\bc^{(q)} \in \mathcal{K}_J(\bA, \bb)} \Bigl[
      %\bigl\Vert (\bA - t_q \bI) \bc^{(q)} - \bb \bigr\Vert_2
    %\Bigr]
    \bigl\Vert (\bA - t_q \bI) \bc_J^{(q)} - \bb \bigr\Vert_2
    &= \min_{P_q \in \mathcal{P}_J} \Bigl[
      \Vert P_q(\bA) \bb \Vert_2
    \Bigr],
    \quad
    q \in [1, Q]
    \\
    &\leq \min_{P_q \in \mathcal{P}_J} \Bigl[
      \Vert P_q(\bA) \Vert_2
    \Bigr] \Vert \bb \Vert_2
    \\
    &= \min_{P_q \in \mathcal{P}_J} \Bigl[
      \sigma^{(\max)}_{P_q(\bA)}
    \Bigr] \Vert \bb \Vert_2,
  \end{align*}
  where $\mathcal{P}_J$ is the class of $J$-degree polynomials such that $P(0) = 1$ for any $P \in \mathcal{P}_J$,
  and $\sigma^{(\max)}_{P_q(\bA)}$ is the maximum singular value of the matrix $P_q(\bA)$.
  Since $\bA$ is symmetric, we have that $\sigma^{(\max)}_{P_q(\bA)}$ must be equal to $P_q(\sigma^{(i)}_{\bA})$, where $\sigma^{(i)}_{\bA}$ is a singular value of $\bA$.
  Therefore,
  %
  \begin{align}
    \bigl\Vert (\bA - t_q \bI) \bc_J^{(q)} - \bb \bigr\Vert_2
    &\leq \min_{P_q \in \mathcal{P}_J} \Bigl[
      \max_{i}
      \left\vert P_q(\sigma^{(i)}_\bA) \right\vert
    \Bigr] \Vert \bb \Vert_2,
    \quad
    q \in [1, Q].
    \label{eqn:bound}
	\end{align}
  %
  We can replace the minimum in \cref{eqn:bound} with any $J^\text{th}$ degree polynomial $P(\cdot)$ that satisfies $P(0) = 1$.
  One natural choice is the Chebychev polynomial $T_J(\cdot)$ where the input is shifted/scaled so that $\vert T_j(\cdot) \vert \leq 1$ on the domain $\sigma^{(\min)}_{\bA} \leq (\cdot) \leq \sigma^{(\max)}_{\bA}$.
  For this choice of polynomial, we have that
  %
  \begin{equation}
    T_J( \gamma )
    \leq 2 \left( \frac{
      \sqrt{\kappa(\bA)} - 1
    }{
      \sqrt{\kappa(\bA)} + 1
    }\right)^J,
    \quad
    \gamma \in [\sigma^{(\min)}_{\bA}, \sigma^{(\max)}_{\bA}]
    \label{eqn:chebychev}
  \end{equation}
  (See \citep[e.g.][Sec. 9.2]{shewchuk1994introduction} or \citep[e.g.][Thm. 38.5]{trefethen1997numerical} for a detailed derivation.)
  Plugging \cref{eqn:chebychev} into \cref{eqn:bound} as an upper bound on $P_q(\cdot)$ completes the proof.
\end{proof}

\cref{lemma:minres} is a very loose bound, as it doesn't assume anything about the spectrum of $\bA$.
In practice, we find that smMINRES converges typically in $J \approx 100$ for kernel matrices, even when the conditioning is on the order of $\kappa(\bA) \approx 10^4$.
This convergence is faster with preconditioning.
With this lemma we are now able to prove our primary CIQ convergence result:
%
\newtheorem*{thm:ciq_convergence}{Theorem~\ref{thm:ciq_convergence} (Restated)}
\begin{thm:ciq_convergence}
  Let $\bK$ and $\bb$ be the inputs to \cref{alg:ciq}, producing the output $\bd_J \approx \bK^{1/2} \bb$ after $J$ iterations.
  The difference between $\bd_J$ and $\bK^{1/2} \bb$ is bounded by:
  %
  \begin{equation*}
    \left\Vert \bd_J - \bK^{\frac 1 2} \bb \right\Vert_2
    \leq
    \overbracket{
      \bigo{\exp\left( -\frac{2 Q \pi^2}{\log \kappa(\bK) + 3} \right)}
    }^{\text{Quadrature error}}
    +
    \overbracket{
      2 \sum_{q=1}^Q \left\vert w_q \right\vert
      \left( \frac{ \sqrt{\kappa(\bK)} - 1}{ \sqrt{\kappa(\bK)} + 1} \right)^J
      \left\Vert \bb \right\Vert_2
    }^{\text{msMINRES error}}
  \end{equation*}
  %
  where $Q$ is the number of quadrature points and $\kappa(\bK)$ is the condition number of $\bK$.
\end{thm:ciq_convergence}
%
\begin{proof}
  First we note that the CIQ solution $\bd_J$ can be written as $\sum_{i=1} w_q \bc^{(q)}_J$, where $\bc^{(q)}_J$ is the $q^\text{th}$ shifted solve $\approx (t_q \bI - \bK)^{-1} \bb$ from msMINRES.
  This theorem then simply applies the triangle inequality several times:
  %
  \begin{align*}
    \left\Vert \bd_J - \bK^{\frac 1 2} \bb \right\Vert_2
    &=
    \left\Vert \overbracket{\sum_{q=1}^Q w_q \bc^{(q)}_J - \left( \bK \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1} \right) \bb }^{\text{msMINRES error}} \right.
    \\
    &\phantom{=} \quad \left. + \underbracket{\left( \bK \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1} \right) \bb - \bK^{\frac 1 2} \bb}_{\text{Quadrature error}} \right\Vert_2
    \\
    &\leq \sum_{q=1}^Q \vert w_q \vert \left\Vert \bc^{(q)}_J - \bK \left( t_q \bI - \bK \right)^{-1} \bb \right\Vert_2
    \\
    &\phantom{=} \:\: + \left\Vert \bK \left( \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1} \right) \bb - \bK^{\frac 1 2} \bb \right\Vert_2
  \end{align*}
  %
  Bounding the quadrature error with \cref{lemma:hale} and bounding the shifted solve errors with \cref{lemma:minres} completes the proof.
\end{proof}
