\chapter{Details on the Cauchy Integral Quadrature}
\label{app:quadrature}

\section{Selecting Quadrature Locations and Weights}

\section{Convergence Analysis of CIQ}

To prove the convergence result in \cref{thm:ciq_convergence}, we first prove the following lemma about msMINRES convergence.

\begin{lemma}
  Let $c^{(1)}_J$, $\ldots$, $c^{(Q)}$ be the outputs after $J$ iterations of msMINRES with input $\bA$, $\bb$, and shifts $t_1$, $\ldots$, $t_Q$.
  The residuals of the solves are bounded by
  %
  \begin{align*}
    \bigl\Vert (\bA - t_q \bI) \bc_J^{(q)} - \bb \bigr\Vert_2
    &\leq 2 \left( \frac{
      \sqrt{\kappa(\bA)} - 1
    }{
      \sqrt{\kappa(\bA)} + 1
    }\right)^J
    \Vert \bb \Vert_2,
    q \in [1, Q],
	\end{align*}
  %
  where $\kappa(\bA)$ is the condition number of $\bA$.
  \label{lemma:minres}
\end{lemma}
%
\begin{proof}
  The convergence proof uses a polynomial bound, which is the standard approach for Krylov algorithms.\footnote{
    \citet{paige1975solution} do not provide a detailed convergence analysis of MINRES.
    However, proving the following result is a straightfoward analog of the famous conjugate gradients convergence analysis.
  }
  See \citep[e.g.][]{shewchuk1994introduction,trefethen1997numerical,saad2003iterative} for an analogous proof for the conjugate gradients method.

	At iteration $J$, the msMINRES algorithm produces:
  %
	\begin{align}
    \bc^{(q)}_J
    = \argmin_{\bc^{(q)} \in \mathcal{K}_J(\bA, \bb)} \Bigl[
      \bigl\Vert (\bA - t_q \bI) \bc^{(q)} - \bb \bigr\Vert_2
    \Bigr],
    \quad
    q \in [1, Q],
    \label{eqn:minres_krylov}
	\end{align}
  %
  where we assume $\bc_0^{(q)} = \bzero$ for simplicity.
  Since the Krylov subspace $\mathcal{K}^{(j)} (\bA, \bb)$ contains powers of $\bA$ applied to the vector $\bb$, we can rewrite
  %
  $(\bA - t_q \bI) \bc^{(q)} - \bb$
  as a polynomial of $\bA$ applied to $\bb$:
  \[ (\bA - t_q \bI) \bc^{(q)} - \bb = \sum_{j=0}^J \alpha_j^{(q)} \bA^j \bb, \quad \alpha_j \in \reals \]
  for some coefficients $\alpha_0$, $\ldots$, $\alpha_J$.
  Therefore, \cref{eqn:minres_krylov} can be reframed as an optimal polynomial problem:
  %
	\begin{align*}
    %\min_{\bc^{(q)} \in \mathcal{K}_J(\bA, \bb)} \Bigl[
      %\bigl\Vert (\bA - t_q \bI) \bc^{(q)} - \bb \bigr\Vert_2
    %\Bigr]
    \bigl\Vert (\bA - t_q \bI) \bc_J^{(q)} - \bb \bigr\Vert_2
    &= \min_{P_q \in \mathcal{P}_J} \Bigl[
      \Vert P_q(\bA) \bb \Vert_2
    \Bigr],
    \quad
    q \in [1, Q]
    \\
    &\leq \min_{P_q \in \mathcal{P}_J} \Bigl[
      \Vert P_q(\bA) \Vert_2
    \Bigr] \Vert \bb \Vert_2
    \\
    &= \min_{P_q \in \mathcal{P}_J} \Bigl[
      \sigma^{(\max)}_{P_q(\bA)}
    \Bigr] \Vert \bb \Vert_2,
  \end{align*}
  where $\mathcal{P}_J$ is the class of $J$-degree polynomials such that $P(0) = 1$ for any $P \in \mathcal{P}_J$,
  and $\sigma^{(\max)}_{P_q(\bA)}$ is the maximum singular value of the matrix $P_q(\bA)$.
  Since $\bA$ is symmetric, we have that $\sigma^{(\max)}_{P_q(\bA)}$ must be equal to $P_q(\sigma^{(i)}_{\bA})$, where $\sigma^{(i)}_{\bA}$ is a singular value of $\bA$.
  Therefore,
  %
  \begin{align}
    \bigl\Vert (\bA - t_q \bI) \bc_J^{(q)} - \bb \bigr\Vert_2
    &\leq \min_{P_q \in \mathcal{P}_J} \Bigl[
      \max_{i}
      \left\vert P_q(\sigma^{(i)}_\bA) \right\vert
    \Bigr] \Vert \bb \Vert_2,
    \quad
    q \in [1, Q].
    \label{eqn:bound}
	\end{align}
  %
  We can replace the minimum in \cref{eqn:bound} with any $J^\text{th}$ degree polynomial $P(\cdot)$ that satisfies $P(0) = 1$.
  One natural choice is the Chebychev polynomial $T_J(\cdot)$ where the input is shifted/scaled so that $\vert T_j(\cdot) \vert \leq 1$ on the domain $\sigma^{(\min)}_{\bA} \leq (\cdot) \leq \sigma^{(\max)}_{\bA}$.
  For this choice of polynomial, we have that
  %
  \begin{equation}
    T_J( \gamma )
    \leq 2 \left( \frac{
      \sqrt{\kappa(\bA)} - 1
    }{
      \sqrt{\kappa(\bA)} + 1
    }\right)^J,
    \quad
    \gamma \in [\sigma^{(\min)}_{\bA}, \sigma^{(\max)}_{\bA}]
    \label{eqn:chebychev}
  \end{equation}
  (See \citep[e.g.][Sec. 9.2]{shewchuk1994introduction} or \citep[e.g.][Thm. 38.5]{trefethen1997numerical} for a detailed derivation.)
  Plugging \cref{eqn:chebychev} into \cref{eqn:bound} as an upper bound on $P_q(\cdot)$ completes the proof.
\end{proof}

\cref{lemma:minres} is a very loose bound, as it doesn't assume anything about the spectrum of $\bA$.
In practice, we find that smMINRES converges typically in $J \approx 100$ for kernel matrices, even when the conditioning is on the order of $\kappa(\bA) \approx 10^4$.
This convergence is faster with preconditioning.
With this lemma we are now able to prove our primary CIQ convergence result:
%
\newtheorem*{thm:ciq_convergence}{Theorem~\ref{thm:ciq_convergence} (Restated)}
\begin{thm:ciq_convergence}
  Let $\bK$ and $\bb$ be the inputs to \cref{alg:ciq}, producing the output $\bd_J \approx \bK^{1/2} \bb$ after $J$ iterations.
  The difference between $\bd_J$ and $\bK^{1/2} \bb$ is bounded by:
  %
  \begin{equation*}
    \left\Vert \bd_J - \bK^{\frac 1 2} \bb \right\Vert_2
    \leq
    \overbracket{
      \bigo{\exp\left( -\frac{2 Q \pi^2}{\log \kappa(\bK) + 3} \right)}
    }^{\text{Quadrature error}}
    +
    \overbracket{
      2 \left\vert \sum_{q=1}^Q w_q \right\vert
      \left( \frac{ \sqrt{\kappa(\bK)} - 1}{ \sqrt{\kappa(\bK)} + 1} \right)^J
      \left\Vert \bb \right\Vert_2
    }^{\text{msMINRES error}}
  \end{equation*}
  %
  where $Q$ is the number of quadrature points and $\kappa(\bK)$ is the condition number of $\bK$.
\end{thm:ciq_convergence}
%
\begin{proof}
  First we note that the CIQ solution $\bd_J$ can be written as $\sum_{i=1} w_q \bc^{(q)}_J$, where $\bc^{(q)}_J$ is the $q^\text{th}$ shifted solve $\approx (t_q \bI - \bK)^{-1} \bb$ from msMINRES.
  This theorem then simply applies the triangle inequality several times:
  %
  \begin{align*}
    \left\Vert \bd_J - \bK^{\frac 1 2} \bb \right\Vert_2
    &=
    \left\Vert \overbracket{\sum_{q=1}^Q w_q \bc^{(q)}_J - \left( \bK \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1} \right) \bb }^{\text{msMINRES error}} \right.
    \\
    &\phantom{=} \quad \left. + \underbracket{\left( \bK \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1} \right) \bb - \bK^{\frac 1 2} \bb}_{\text{Quadrature error}} \right\Vert_2
    \\
    &\leq \sum_{q=1}^Q w_q \left\Vert \bc^{(q)}_J - \bK \left( t_q \bI - \bK \right)^{-1} \bb \right\Vert_2
    \\
    &\phantom{=} \:\: + \left\Vert \bK \left( \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1} \right) \bb - \bK^{\frac 1 2} \bb \right\Vert_2
  \end{align*}
  %
  Bounding the quadrature error with \cref{lemma:hale} and bounding the shifted solve errors with \cref{lemma:minres} completes the proof.
\end{proof}
