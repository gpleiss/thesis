%!TEX root=../main.tex
\section{Discussion}

Historically for Gaussian processes, ``a large dataset is one that contains over a few thousand data points'' \citep{hensman2013gaussian}.
In this chapter, we have extended exact GPs far beyond what has previously been thought possible---applying GPs to datasets with over a million training examples.
In our experiments, we find that exact GPs perform significantly better than approximate methods on large datasets, while requiring fewer design choices.

\paragraph{Is CG still exact?}
In the GP literature, \emph{exact} GP training and inference typically refers to Cholesky-based inference with exact kernels \citep{rasmussen2006gaussian}.
A natural question to ask is whether we can consider our approach ``exact'' in light of the fact that mBCG perform solves only up to a pre-specified error tolerance.
However, unlike scalable GP approximations, the difference between a mBCG-based model and a theoretical model with ``perfect'' solves is precisely controlled by this error tolerance.
We therefore consider mBCG ``exact'' in the context of mathematical optimization---namely that it computes solutions up to arbitrary numerical precision.
In fact, mBCG-based methods can often be more precise than Cholesky based approaches in floating-point arithmetic due to fewer round-off errors, as demonstrated in \cref{chapter:bbmm} (\cref{fig:cg_error}).

\paragraph{When to approximate?}
There are many scalable GP approximations with varying statistical properties, advantages, and application regimes.
We compare against the SVGP and SGPR methods due to their popularity and general applicability.
There may be some regimes where other approximate methods outperform these two approaches.
Our objective in this chapter is not to perform an exhaustive study of approximate methods, but rather to highlight that such comparisons are now possible.

Indeed, there are cases where approximate GP methods might still be preferable, especially when computational resources are limited.
In certain regimes, such as low dimensional spaces, approximate methods like KISS-GP can achieve high degrees of accuracy in less time than exact GPs \cite{wilson2015kernel}.
Additionally, GP inference with non-conjugate likelihoods necessitates approximate Bayesian inference techniques (see \cref{chapter:ciq}).

Nevertheless, with efficient utilization of modern hardware, exact Gaussian processes are now an appealing option on substantially larger datasets than previously thought possible.
We expect exact GPs to become even more scalable and accessible with continued advances in hardware design.
