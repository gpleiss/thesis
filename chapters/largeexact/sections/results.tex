%!TEX root=../main.tex
\section{Results}
\label{sec:largeexact_results}

We compare the performance of exact Gaussian processes against widely-used scalable GP approximation methods on a range of large-scale datasets from the UCI dataset repository \cite{asuncion2007uci}.
Although \citet{nguyen2019exact} show results for exact GPs on $N \leq 120,\!000$, these results are the first-ever comparison of exact verses approximate GPs on $N\gg 10^5$.
Our experiments demonstrate that exact GPs:
\begin{enumerate*}
  \item outperform popular approximate GPs methods on nearly all benchmarking datasets in our study;
  \item compute thousands of test-point predictions in milliseconds, even when $N > 10^6$;
  \item utilize all available data when making predictions, even when $N > 10^5$; and
  \item achieve linear training speedups on large datasets when using multiple GPUs.
\end{enumerate*}

We compare exact GPs against two scalable GP approximations: Sparse Gaussian Process Regression (SGPR) \cite{titsias2009variational}, and Stochastic Variational Gaussian Processes (SVGP) \cite{hensman2013gaussian}.
We choose these methods due to their popularity and general applicability, enabling a comparison over a wide range of datasets.
We use $M = 512$ for SGPR and $M = 1,\!024$ for SVGP, which are common values used for these methods \cite{matthews2017gpflow}.
We later experiment with varying the number of inducing points.

\paragraph{Experiment details.}
Each dataset is randomly split into $75\%$ training, $10\%$ validation, and $15\%$ testing sets.
The data is scaled to be mean $0$ and standard deviation $1$ as measured by the training set.
We use a constant prior mean and a Mat\'ern 5/2 kernel with independent lengthscales for each dimension.
We train the learnable parameters of exact GPs and SGPR with $100$ iterations of Adam with a learning rate of $0.1$.
For SVGP, we perform $100$ epochs of Adam with a minibatch size of $1,\!024$ and a learning rate of $0.01$, which we find to perform better than $0.1$.
All exact and SGPR models use a rank-$100$ partial pivoted-Cholesky preconditioner.
During training the mBCG convergence tolerance is set to  $\Vert \trainK \bc - \by \Vert_2 / \Vert \by \Vert_2 = 1$; for predictions this tolerance is set to $0.001$.
We use a rank-$200$ LOVE approximation of $\trainK^{-1}$ to compute predictive variances.
On two of the datasets (HouseElectric and Buzz) we constrain the learned noise to be at least $0.1$ to regularize the poorly conditioned kernel matrices.

All training/evaluation experiments use a single NVIDIA GTX 2080-TI GPU unless otherwise stated.
We use the KeOps library \cite{charlier2020kernel} in conjunction with our GPyTorch BBMM/LOVE implementations to perform the partitioned kernel MVMs.
%We perform all training on one machine with 8 NVIDIA Tesla V100-SXM2-32GB-LS GPUs.

\begin{table}[!tb]
  \caption[Performance comparison between exact GPs and scalable approximations on large UCI datasets.]{
    Performance comparison between exact GPs and scalable approximations on large UCI datasets using ARD Mat\'ern kernels.
    All trials on $N < 100,\!000$ datasets are averaged over 3 trials; $\pm$ corresponds to 1 standard deviation.
    (We are unable to scale SGPR to HouseElectric due to its memory requirements when $m=512$.)
    {\bf Top:} test set root mean square error (RMSE).
    {\bf Bottom:} test set negative log likelihood (NLL).
  }
  \label{tab:large_exact_gp_results}
  \centering
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_results.tex}
  }
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_results_nll.tex}
  }
  \vspace{1em}
\end{table}

\paragraph{Accuracy.}
\cref{tab:large_exact_gp_results} displays the test set RMSEs and negative log likelihoods (NLLs) of exact GPs and their approximate counterparts.
We find that exact GPs achieve lower error than approximate methods on nearly every dataset.
Notably, on certain datasets like 3droad, exact GPs achieve a half or even a quarter of the error of some approximate methods.

Moreover, we also see that performance of the scalable GP methods is dataset dependent.
Neither SVGP nor SGPR consistently outperforms the other.
Interestingly, dataset size/dimensionality do not seem to influence the relative performance of approximate methods.
For example, though Protein and Kin40K are similar in size and have similar dimensionality, the approximate methods perform worse on Kin40K (relative to the RMSE of exact GPs).

\begin{table*}[t!]
  \vspace{1em}
  \caption[Wall-clock time comparison between exact GPs and approximate GPs on large UCI datasets.]{
    Wall-clock time comparison between exact GPs and approximate GPs on large UCI datasets.
    Models are trained and evaluated on a single NVIDIA GTX 2080-TI GPU.
    All trials on $N < 100,\!000$ datasets are averaged over 3 trials; $\pm$ corresponds to 1 standard deviation.
    {\bf Top:} training time for exact GPs and scalable approximations.
    {\bf Bottom:} prediction time for exact GPs.
    Precomputation refers to computing the LOVE cache.
    Prediction refers to computing the predictive distributions for $1,\!000$ test points.
  }
  \label{tab:large_exact_gp_timings}
  \centering
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_timings.tex}
  }
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_timings_pred.tex}
  }

  \vspace{2em}
\end{table*}


\paragraph{Training time.}
\cref{tab:large_exact_gp_timings} (top) displays the training time for exact and approximate GPs.
On datasets with $N \leq 100,\!000$ data points, exact GPs can usually be trained in minutes.
Datasets with $N \geq 100,\!000$ can be trained in hours.
While approximate GPs tend to be faster, we do note that the training times of exact GPs are within an order of magnitude of their approximate counterparts.
It is quite remarkable that exact GPs, which require $\bigo{N^2}$ computation with BBMM, are not much slower than $\bigo{N}$ approximate methods.
We hypothesize that exact GPs benefit more from GPU acceleration, as exact kernel matrices afford more MVM parallelism than approximate kernel matrices.

\paragraph{Prediction time.}
Although exact GPs take longer to train, we find that their speed is comparable to approximate methods at test time.
\cref{tab:large_exact_gp_timings} (bottom) displays the time to compute $1,\!000$ predictive means and variances.
After the LOVE precomputation, exact GPs make predictions in \emph{milliseconds}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.70\linewidth]{figures/gpu_speedup.pdf}
  \caption[Speed of BBMM training using multi-GPU computation.]{
    Speed of BBMM training using multi-GPU computation.
    On large datasets, exact GPs with BBMM achieve a near linear speedup with more GPUs.
    (Speedups are measured on NVIDIA Tesla V100-SXM2-32GB-LS GPUs using models with non-ARD Mat\'ern 3/2 kernels.)
  }
  \label{fig:gpu_speedup}
\end{figure}

\paragraph{Training acceleration with multiple GPUs.}
As discussed in \cref{sec:largeexact_method}, the MVMs in BBMM and LOVE can be distributed across multiple devices.
\cref{fig:gpu_speedup} plots the speedup as more GPUs are used for training on the KEGGU, 3DRoad, Song, and Buzz datasets.
(Speedups are measured on NVIDIA Tesla V100-SXM2-32GB-LS GPUs using models with non-ARD Mat\'ern 3/2 kernels.)
Each of these datasets achieve a nearly linear speedup up to 4 GPUs.
The speedup is more pronounced for the two large datasets (3DRoad and Song).

