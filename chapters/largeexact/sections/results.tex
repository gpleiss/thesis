%!TEX root=../main.tex
\section{Results}
\label{sec:largeexact_results}

We compare exact GPs against widely-used approximate methods on large-scale datasets from the UCI dataset repository \cite{asuncion2007uci}.
Although \citet{nguyen2019exact} show results for exact GPs on $N \leq 120,\!000$, these results are the first-ever comparison of exact verses approximate GPs on $N\gg 10^5$.
Our experiments demonstrate that exact GPs:
\begin{enumerate*}
  \item outperform popular approximate GPs methods on many benchmarking datasets;
  \item compute thousands of test-point predictions in milliseconds, even when $N > 10^6$;
  \item utilize all available data when making predictions; and
  \item achieve linear training speedups when using multiple GPUs.
\end{enumerate*}

We compare exact GPs against two scalable GP approximations: Sparse Gaussian Process Regression (SGPR) \cite{titsias2009variational}, and Stochastic Variational Gaussian Processes (SVGP) \cite{hensman2013gaussian}.
These methods are widely popular and general applicable, enabling a comparison over a wide range of datasets.
Unless otherwise stated, we use $M = 512$ for SGPR and $M = 1,\!024$ for SVGP, which are common values for these methods \cite{matthews2017gpflow}.

\paragraph{Experiment details.}
Each dataset is randomly split into $64\%$ training, $16\%$ validation, and $20\%$ testing sets.
Data are scaled to be mean $0$ and standard deviation $1$ as measured by the training set.
We use a constant prior mean and a Mat\'ern 3/2 kernel with a shared lengthscale for each dimension.

For exact GPs: we pre-train the model's hyperparameters using a subset of $10,\!000$ randomly selected training points.
The sub-sampled model is optimized with 10 steps of L-BFGS \citep{liu1989lbfgs} and 10 steps of Adam \citep{kingma2014adam} with step sizes of $0.1$.
After pre-training, we run 3 additional iterations of Adam on the full dataset.
For SGPR models: we optimize hyperparameters with $100$ iterations of Adam, learning rate of $0.1$.
For SVGP models: we jointly optimize the variational parameters and hyperparameters with Adam---using a learning rate of $0.01$ and a minibatch size of $1,\!024$---for $100$ epochs.

Exact GPs and SGPR models use BBMM for training with a rank-$100$ partial pivoted-Cholesky preconditioner.
During training, the mBCG convergence tolerance is set to  $\Vert \trainK \bc - \by \Vert_2 / \Vert \by \Vert_2 = 1$.
For predictions, the mBCG tolerance is set to $0.001$.
We use a rank-$100$ LOVE approximation of $\trainK^{-1}$ to compute predictive variances.
On the HouseElectric dataset, the likelihood's observational noise is constrained to be $\geq 0.1$ in order to regularize the poorly conditioned kernel matrix.
We use the KeOps library \cite{charlier2020kernel} in conjunction with our GPyTorch BBMM/LOVE implementations to perform partitioned kernel MVMs.
%We perform all training on one machine with 8 NVIDIA Tesla V100-SXM2-32GB-LS GPUs.

\begin{table}[!tb]
  \caption[Performance of exact GPs and scalable approximations on large UCI datasets.]{
    Performance of exact GPs and scalable approximations on large UCI datasets (shared-lengthscale Mat\'ern 3/2 kernels).
    All results are averaged over 3 trials; $\pm$ corresponds to 1 standard deviation.
    (We are unable to scale SGPR to HouseElectric due to its memory requirements when $M=512$.)
    {\bf Top:} test set root mean square error (RMSE).
    {\bf Bottom:} test set negative log likelihood (NLL).
  }
  \label{tab:large_exact_gp_results}
  \centering
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_results.tex}
  }
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_results_nll.tex}
  }
  \vspace{1em}
\end{table}

\paragraph{Accuracy.}
\cref{tab:large_exact_gp_results} displays the test set RMSEs and negative log likelihoods (NLLs) of exact GPs and their approximate counterparts.
We find that exact GPs achieve lower error than approximate methods on nearly every dataset.
Notably, on certain datasets like 3Droad, exact GPs achieve a half or even a quarter of the error of some approximate methods.

Moreover, we also see that performance of the scalable GP methods is dataset dependent.
Neither SVGP nor SGPR consistently outperforms the other.
Interestingly, dataset size/dimensionality do not seem to influence the relative performance of approximate methods.
For example, though Protein and Kin40K are similar in size and have similar dimensionality, the approximate methods perform worse on Kin40K (relative to the RMSE of exact GPs).

\gp{STOPPED HERE.}

\begin{table*}[t!]
  \vspace{1em}
  \caption[Wall-clock time of exact GPs verse approximate GPs.]{
    Wall-clock time comparison of exact GPs verses approximate GPs on large UCI datasets.
    Models are trained and evaluated on a single NVIDIA GTX 2080-TI GPU.
    (Asterisks (*) indicate measurements made using 8 V100 GPUs without KeOps.)
    {\bf Top:} training time for exact GPs and scalable approximations.
    {\bf Bottom:} precomputation and prediction times for exact GPs.
    ``Precomputation'' refers to the LOVE cache computation.
    ``Prediction'' refers to predictive distribution computations for $1,\!000$ test points.
  }
  \label{tab:large_exact_gp_timings}
  \centering
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_timings.tex}
  }
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_timings_pred.tex}
  }

  \vspace{2em}
\end{table*}


\paragraph{Training time.}
\cref{tab:large_exact_gp_timings} (top) displays the training time for exact and approximate GPs.
With the pre-training strategy, exact GPs on datasets with $N \leq 50,\!000$ can usually be trained in seconds.
Datasets with $N \geq 50,\!000$ can be trained in minutes, while datasets with $N \geq 500,\!000$ require hours.
Remarkably, exact GP can often be trained in \emph{less time than their approximate counterparts}, despite having a larger asymptotic complexity.
Approximate GPs tend to require more optimization iterations to properly place inducing points.
We also hypothesize that exact GPs benefit more from GPU acceleration, as dense kernel matrices afford more MVM parallelism than their low-rank approximations.

\paragraph{Prediction time.}
At test time, we find that the speed of exact GPs is comparable to approximate methods at test time.
\cref{tab:large_exact_gp_timings} (bottom) displays the time to compute $1,\!000$ predictive means and variances.
After the LOVE precomputation, exact GPs make predictions in \emph{milliseconds}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.70\linewidth]{figures/gpu_speedup.pdf}
  \caption[Speed of BBMM training using multi-GPU computation.]{
    Speed of BBMM training using multi-GPU computation.
    On large datasets, exact GPs with BBMM achieve a near linear speedup with more GPUs.
    (Speedups are measured on NVIDIA Tesla V100-SXM2-32GB-LS GPUs.)
  }
  \label{fig:gpu_speedup}
\end{figure}

\paragraph{Training acceleration with multiple GPUs.}
As discussed in \cref{sec:largeexact_method}, MVMs for training and predictions can be distributed across multiple devices.
\cref{fig:gpu_speedup} plots the speedup as more GPUs are used for training on the KEGGU, 3DRoad, Song, and Buzz datasets.
(Speedups are measured on NVIDIA Tesla V100-SXM2-32GB-LS GPUs without using the KeOps library.)
Each of these datasets achieve a nearly linear speedup up to 4 GPUs.
The speedup is more pronounced for the two large datasets (3DRoad and Song).

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/initialization.pdf}
  \caption[
    The effect of pre-training initialization on exact Gaussian process accuracy and timing.
  ]{
    The effect of pre-training initialization on exact Gaussian process accuracy and timing.
  }
  \label{fig:initialization}
\end{figure*}

\paragraph{Initialization.}
In \cref{fig:initialization} we compare GP models with and without our pre-training initialization scheme.
The GPs with initialization pre-train on a $N=10,\!000$ subset of the training data, before running a final $3$ iterations of Adam on the full dataset.
The GPs without initialization are trained on the full dataset for 100 iterations of Adam.
On all datasets, the models initialized with pre-trained hyperparameters achieve a comparable test set RMSE.
However, these models require an \emph{order of magnitude} less training time than the un-initialized models.
