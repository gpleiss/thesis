%!TEX root=../main.tex
\section{Results}
\label{sec:largeexact_results}

We compare the performance of exact Gaussian processes against widely-used scalable GP approximation methods on a range of large-scale datasets from the UCI dataset repository \cite{asuncion2007uci}.
Our experiments demonstrate that exact GPs:
\begin{enumerate*}
  \item outperform popular approximate GPs methods on nearly all benchmarking datasets in our study;
  \item compute thousands of test-point predictions in less than a second, even when $N > 10^6$;
  \item utilize all available data when making predictions, even when $N > 10^5$; and
  \item achieve linear training speedups on large datasets by adding additional GPU devices.
\end{enumerate*}

\paragraph{Baselines.}
We compare against two scalable GP approximations: Sparse Gaussian Process Regression (SGPR) \cite{titsias2009variational}, and Stochastic Variational Gaussian Processes (SVGP) \cite{hensman2013gaussian}.
We choose these methods due to their popularity and general applicability, enabling a comparison over a wide range of datasets.
We use $M = 512$ for SGPR and $M = 1,\!024$ for SVGP, which are common values used for these methods \cite{matthews2017gpflow}.
We later experiment with varying the number of inducing points.

\paragraph{Experiment details.}
Each dataset is randomly split into $75\%$ training, $10\%$ validating, and $15\%$ testing sets.
The data is whitened to be mean $0$ and standard deviation $1$ as measured by the training set.
We use a constant prior mean and a Mat\'ern 5/2 kernel with independent lengthscales for each dimension.

We train the learnable parameters of exact GPs and SGPR with $100$ iterations of Adam with a learning rate of $0.1$.
For SVGP, we perform $100$ epochs of Adam with a minibatch size of $1,\!024$ and a learning rate of $0.01$, which we find to perform better than $0.1$.
All exact and SGPR models use a rank-$100$ partial pivoted-Cholesky preconditioner and run PCG with a tolerance of $\Vert \trainK \bc - \by \Vert_2 / \Vert \by \Vert_2 = 1$ during training.
We constrain the learned noise to be at least $0.1$ to regularize the poorly conditioned kernel matrix for the houseelectric and Buzz datasets.

All training/evaluation experiments use a single NVIDIA GTX 2080-TI GPU unless otherwise stated.
We use the KeOps library \cite{charlier2020kernel} in conjunction with our GPyTorch BBMM/LOVE implementations to perform the partitioned kernel MVMs.
%We perform all training on one machine with 8 NVIDIA Tesla V100-SXM2-32GB-LS GPUs.

\begin{table}[!tb]
  \caption[Performance comparison between exact GPs and scalable approximations on large UCI datasets.]{
    Performance comparison between exact GPs and scalable approximations on large UCI datasets using ARD Mat\'ern kernels.
    All trials on $N < 100,\!000$ datasets are averaged over 3 trials; $\pm$ corresponds to 1 standard deviation.
    (We are unable to scale SGPR to HouseElectric due to its memory requirements when $m=512$.)
    {\bf Top:} test set root mean square error (RMSE).
    {\bf Bottom:} test set negative log likelihood (NLL).
  }
  \label{tab:large_exact_gp_results}
  \centering
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_results.tex}
  }
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_results_nll.tex}
  }
  \vspace{1em}
\end{table}

\paragraph{Accuracy.}
\cref{tab:large_exact_gp_results} displays the accuracy and negative log likelihoods of exact GPs and approximate methods on several large-scale datasets.
We find that exact GPs achieve lower error than approximate methods on nearly every dataset.
Notably, on certain datasets like 3droad, exact GPs achieve a half or even a quarter of the error of some approximate methods.
Although \citet{nguyen2019exact} show results for exact GPs on $N \leq 120,\!000$, this is the first set of results comparing exact GPs to approximate GPs on $N\gg 10^5$.

Moreover, we also see that the choice of approximate method affects performance; neither approximate method consistently outperforms the other.
Interestingly, neither dataset size nor dimensionality seems to influence the relative performance of the approximate methods.
For example, though Protein and Kin40K are similar in size and have almost the same dimensionality, the approximate methods perform worse on Kin40K (relative to the RMSE of exact GPs).

\begin{table*}[t!]
  \vspace{1em}
  \caption[Wall-clock time comparison between exact GPs and approximate GPs on large UCI datasets.]{
    Wall-clock time comparison between exact GPs and approximate GPs on large UCI datasets.
    Models are trained and evaluated on a single NVIDIA GTX 2080-TI GPU.
    All trials on $N < 100,\!000$ datasets are averaged over 3 trials; $\pm$ corresponds to 1 standard deviation.
    {\bf Top:} training time for exact GPs and scalable approximations.
    {\bf Bottom:} prediction time for exact GPs.
    Precomputation refers to computing the LOVE cache.
    Prediction refers to computing the predictive distributions for $1,\!000$ test points.
  }
  \label{tab:large_exact_gp_timings}
  \centering
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_timings.tex}
  }
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_timings_pred.tex}
  }

  \vspace{2em}
\end{table*}


\paragraph{Training time.}
\cref{tab:large_exact_gp_timings} displays the training time for exact and approximate GPs.
Datasets with $N \leq 100,\!000$ data points, exact GPs can be trained in minutes.
Datasets with $N \geq 100,\!000$ can be trained in hours.

While approximate methods tend to be faster than exact GPs, we do note that exact GPs tend to require the same order of magnitude of training time.
It is quite remarkable that exact GPs, which require $\bigo{N^2}$ computation with BBMM, are not much slower than approximate methods that are linear in $N$.
We hypothesize the GPU acceleration benefits exact GPs more than approximate GPs as exact kernel matrices can afford more parallelism than approximate methods.

\paragraph{Prediction time.}
Although exact GPs take longer to train, we find that their speed is comparable to approximate methods at test time.
\cref{tab:large_exact_gp_timings} displays the time to compute $1,\!000$ predictive means and variances at test time before and after LOVE precomputation.
All predictions are made on one NVIDIA RTX 2080 Ti GPU.
We see exact GPs take \emph{milliseconds} across all dataset sizes used.

\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{figures/gpu_speedup.pdf}
  \caption[Speed of BBMM training using multi-GPU computation.]{
    Speed of BBMM training using multi-GPU computation.
    On large datasets, exact GPs with BBMM achieve a near linear speedup with more GPU resources.
  }
  \label{fig:gpu_speedup}
\end{figure*}

\paragraph{Training acceleration with multiple GPUs.}
As discussed in \cref{sec:largeexact_method}, the MVMs in BBMM and LOVE can be distributed across multiple devices.
\cref{fig:gpu_speedup} plots the speedup as more GPUs are used for training on the KEGGU, 3DRoad, Song, and Buzz datasets.
Each of these datasets achieve a nearly linear speedup when adding up to 4 GPUs.
The speedup is more pronounced for the two large datasets (3DRoad and Song).


\subsection{Ablation Studies}
\begin{figure}[!t]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/inducing_points.pdf}
  \caption{
    Error of SVGP and SGPR methods as a function of the number of inducing points
    ($m$).  Both methods scale cubically with $m$.
    We were unable to run SGPR with more than $1,\!024$ inducing points on a single GPU.
    Exact GPs have lower error than both methods.
  }
  \label{fig:num_inducing_points}
\end{figure}
With our method, we can better understand how exact GPs and approximate GPs scale to datasets with $n\gg 10^4$.
Here, we demonstrate how the amount of data affects exact GP performance, and
how the number of inducing points affects the performance of approximate GPs.
\begin{figure*}[t!]
  \centering
  \includegraphics[width=\linewidth]{figures/subsampling.pdf}
  \caption{
    Test root-mean-square error (RMSE) as a function of subsampled dataset size (lower is better).
    Subsampled exact GPs outperform approximate GPs even with a quarter of the training set.
    Exact GP error continues to decrease as data is added until the full dataset is used.
  }
  \label{fig:subsampling_results}
\end{figure*}

\paragraph{Do GPs need the entire dataset?}
As a non-parametric model, Gaussian processes naturally adapt to the amount of training
data available \citep{wilson2014thesis}. \cref{fig:subsampling_results} shows an increase in accuracy as we increase the amount
of training data on the KEGGU, 3DRoad, and Song datasets. For each dataset, we
subsample a fraction of the training data and plot the resulting root-mean-square
error on the test-set as a function of subsampled training set size. We use the
same 1/3 holdout of the full dataset to test in each case.
As expected, the test RMSE decreases monotonically as we increase the subsample size.
\cref{fig:subsampling_results} also shows the
performance of exact GP, SGPR, and SVGP trained on the entire training set.
Strikingly, in all three cases, \textit{an exact GP with less than a quarter of
the training data outperformed approximate GPs trained on the entire
training set}. Furthermore, test error continues to decrease with each addition of training data.

\paragraph{Would more inducing points help?}
The results in \cref{tab:large_exact_gp_results} naturally raise the question: ``can approximate models with more inducing
points recover the performance of exact methods?''
We plot test RMSE on two datasets, Bike and Protein, as a function of the number of inducing points in \cref{fig:num_inducing_points}.
The test RMSE of both inducing point methods saturates on both datasets well above
the test RMSE of an exact GP.
Furthermore, we note that using $m$ inducing points introduces a $m \times m$
matrix and a $\bigo{nm^2 + m^3}$ time complexity
\cite{hensman2013gaussian,hensman2015scalable} which makes it
difficult to train SGPR with $m \gg 1024$ inducing points on
one GPU. It is possible to combine kernel partitioning with inducing-point methods to utilize even larger values of $m$.
However, as \cref{fig:num_inducing_points} and \cref{tab:large_exact_gp_results} show, it may be preferable
to use the extra computational resources to train an exact GP on more data rather than
to train an approximate GP with more inducing points.
%
