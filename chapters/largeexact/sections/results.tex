%!TEX root=../main.tex
\section{Results}
\label{sec:largeexact_results}

We compare exact GPs against widely-used approximate methods on large-scale datasets from the UCI dataset repository \cite{asuncion2007uci}.
Although \citet{nguyen2019exact} show results for exact GPs on $N \leq 120,\!000$, these results are the first-ever comparison of exact verses approximate GPs on $N\gg 10^5$.
Our experiments demonstrate that exact GPs:
\begin{enumerate*}
  \item outperform popular approximate GPs methods on many benchmarking datasets;
  \item compute thousands of test-point predictions in milliseconds, even when $N > 10^6$;
  \item utilize all available data when making predictions; and
  \item achieve linear training speedups when using multiple GPUs.
\end{enumerate*}

We compare exact GPs against two scalable GP approximations: Sparse Gaussian Process Regression (SGPR) \cite{titsias2009variational}, and Stochastic Variational Gaussian Processes (SVGP) \cite{hensman2013gaussian}.
These methods are widely popular and general applicable, enabling a comparison over a wide range of datasets.
Unless otherwise stated, we use $M = 512$ for SGPR and $M = 1,\!024$ for SVGP, which are common values for these methods \cite{matthews2017gpflow}.

\paragraph{Experiment details.}
Each dataset is randomly split into $64\%$ training, $16\%$ validation, and $20\%$ testing sets.
Data are scaled to be mean $0$ and standard deviation $1$ as measured by the training set.
We use a constant prior mean and a Mat\'ern 3/2 kernel with a shared lengthscale for each dimension.

For exact GPs: we pre-train the model's hyperparameters using a subset of $10,\!000$ randomly selected training points.
The sub-sampled model is optimized with 10 steps of L-BFGS \citep{liu1989lbfgs} and 10 steps of Adam \citep{kingma2014adam} with step sizes of $0.1$.
After pre-training, we run 3 additional iterations of Adam on the full dataset.
For SGPR models: we optimize hyperparameters with $100$ iterations of Adam, learning rate of $0.1$.
For SVGP models: we jointly optimize the variational parameters and hyperparameters with Adam---using a learning rate of $0.01$ and a minibatch size of $1,\!024$---for $100$ epochs.

Exact GPs and SGPR models use BBMM for training with a rank-$100$ partial pivoted-Cholesky preconditioner.
During training, the mBCG convergence tolerance is set to  $\Vert \trainK \bc - \by \Vert_2 / \Vert \by \Vert_2 = 1$.
For predictions, the mBCG tolerance is set to $0.001$.
We use a rank-$100$ LOVE approximation of $\trainK^{-1}$ to compute predictive variances.
On the HouseElectric dataset, the likelihood's observational noise is constrained to be $\geq 0.1$ in order to regularize the poorly conditioned kernel matrix.
We use the KeOps library \cite{charlier2020kernel} in conjunction with our GPyTorch BBMM/LOVE implementations to perform partitioned kernel MVMs.
%We perform all training on one machine with 8 NVIDIA Tesla V100-SXM2-32GB-LS GPUs.

\begin{table}[!tb]
  \caption[Performance of exact GPs and scalable approximations on large UCI datasets.]{
    Performance of exact GPs and scalable approximations on large UCI datasets (shared-lengthscale Mat\'ern 3/2 kernels).
    All results are averaged over 3 trials; $\pm$ corresponds to 1 standard deviation.
    (We are unable to scale SGPR to HouseElectric due to its memory requirements when $M=512$.)
    {\bf Top:} test set root mean square error (RMSE).
    {\bf Bottom:} test set negative log likelihood (NLL).
  }
  \label{tab:large_exact_gp_results}
  \centering
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_results.tex}
  }
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_results_nll.tex}
  }
  \vspace{1em}
\end{table}

\paragraph{Accuracy.}
\cref{tab:large_exact_gp_results} displays the test set RMSEs and negative log likelihoods (NLLs) of exact GPs and their approximate counterparts.
We find that exact GPs achieve lower error than approximate methods on nearly every dataset.
Notably, on certain datasets like 3Droad, exact GPs achieve a half or even a quarter of the error of some approximate methods.

Moreover, we also see that performance of the scalable GP methods is dataset dependent.
Neither SVGP nor SGPR consistently outperforms the other.
Interestingly, dataset size/dimensionality do not seem to influence the relative performance of approximate methods.
For example, though Protein and Kin40K are similar in size and have similar dimensionality, the approximate methods perform worse on Kin40K (relative to the RMSE of exact GPs).

\gp{STOPPED HERE.}

\begin{table*}[t!]
  \vspace{1em}
  \caption[Wall-clock time comparison between exact GPs and approximate GPs on large UCI datasets.]{
    Wall-clock time comparison between exact GPs and approximate GPs on large UCI datasets.
    Models are trained and evaluated on a single NVIDIA GTX 2080-TI GPU.
    All trials on $N < 100,\!000$ datasets are averaged over 3 trials; $\pm$ corresponds to 1 standard deviation.
    {\bf Top:} training time for exact GPs and scalable approximations.
    {\bf Bottom:} prediction time for exact GPs.
    Precomputation refers to computing the LOVE cache.
    Prediction refers to computing the predictive distributions for $1,\!000$ test points.
  }
  \label{tab:large_exact_gp_timings}
  \centering
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_timings.tex}
  }
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_timings_pred.tex}
  }

  \vspace{2em}
\end{table*}


\paragraph{Training time.}
\cref{tab:large_exact_gp_timings} (top) displays the training time for exact and approximate GPs.
On datasets with $N \leq 100,\!000$ data points, exact GPs can usually be trained in minutes.
Datasets with $N \geq 100,\!000$ can be trained in hours.
While approximate GPs tend to be faster, we do note that the training times of exact GPs are within an order of magnitude of their approximate counterparts.
It is quite remarkable that exact GPs, which require $\bigo{N^2}$ computation with BBMM, are not much slower than $\bigo{N}$ approximate methods.
We hypothesize that exact GPs benefit more from GPU acceleration, as exact kernel matrices afford more MVM parallelism than approximate kernel matrices.

\paragraph{Prediction time.}
Although exact GPs take longer to train, we find that their speed is comparable to approximate methods at test time.
\cref{tab:large_exact_gp_timings} (bottom) displays the time to compute $1,\!000$ predictive means and variances.
After the LOVE precomputation, exact GPs make predictions in \emph{milliseconds}.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.70\linewidth]{figures/gpu_speedup.pdf}
  \caption[Speed of BBMM training using multi-GPU computation.]{
    Speed of BBMM training using multi-GPU computation.
    On large datasets, exact GPs with BBMM achieve a near linear speedup with more GPUs.
    (Speedups are measured on NVIDIA Tesla V100-SXM2-32GB-LS GPUs.)
  }
  \label{fig:gpu_speedup}
\end{figure}

\paragraph{Training acceleration with multiple GPUs.}
As discussed in \cref{sec:largeexact_method}, the MVMs in BBMM and LOVE can be distributed across multiple devices.
\cref{fig:gpu_speedup} plots the speedup as more GPUs are used for training on the KEGGU, 3DRoad, Song, and Buzz datasets.
(Speedups are measured on NVIDIA Tesla V100-SXM2-32GB-LS GPUs using models with non-ARD Mat\'ern 3/2 kernels.)
Each of these datasets achieve a nearly linear speedup up to 4 GPUs.
The speedup is more pronounced for the two large datasets (3DRoad and Song).

\begin{figure*}[t!]
  \centering
  \includegraphics[width=0.75\linewidth]{figures/initialization.pdf}
  \caption[
    The effect of pre-training on exact Gaussian process accuracy and timing.
  ]{
    The effect of pre-training on exact Gaussian process accuracy and timing.

  }
  \label{fig:initialization}
\end{figure*}

\paragraph{Initialization.}
\autoref{fig:initialization-test} shows
that this initialization plus fine-tuning procedure achieves comparable test
performance to running Adam for the full 100 iterations without pretraining.
We do not pretrain the SGPR and SVGP models because we found that they
required a significant number of fine-tuning steps after pretraining due to
their increased number of model parameters. We show additional training
statistics for exact GPs trained with 100 steps of Adam in the appendix.
