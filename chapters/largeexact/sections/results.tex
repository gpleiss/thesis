%!TEX root=../main.tex
\section{Results}
\label{sec:largeexact_results}

We compare the performance of exact Gaussian processes against widely-used scalable GP approximation methods on a range of large-scale datasets from the UCI dataset repository \cite{asuncion2007uci}.
Our experiments demonstrate that exact GPs:
\begin{enumerate*}
  \item outperform popular approximate GPs methods on nearly all benchmarking datasets in our study;
  \item compute thousands of test-point predictions in less than a second, even when $N > 10^6$;
  \item utilize all available data when making predictions, even when $N > 10^5$; and
  \item achieve linear training speedups on large datasets by adding additional GPU devices.
\end{enumerate*}

\paragraph{Baselines.}
We compare against two scalable GP approximations: Sparse Gaussian Process Regression (SGPR) \cite{titsias2009variational}, and Stochastic Variational Gaussian Processes (SVGP) \cite{hensman2013gaussian}.
We choose these methods due to their popularity and general applicability, enabling a comparison over a wide range of datasets.
We use $M = 512$ for SGPR and $M = 1,\!024$ for SVGP, which are common values used for these methods \cite{matthews2017gpflow}.
We later experiment with varying the number of inducing points.

\paragraph{Experiment details.}
Each dataset is randomly split into $75\%$ training, $10\%$ validating, and $15\%$ testing sets.
The data is whitened to be mean $0$ and standard deviation $1$ as measured by the training set.
We use a constant prior mean and a Mat\'ern 5/2 kernel with independent lengthscales for each dimension.

We train the learnable parameters of exact GPs and SGPR with $100$ iterations of Adam with a learning rate of $0.1$.
For SVGP, we perform $100$ epochs of Adam with a minibatch size of $1,\!024$ and a learning rate of $0.01$, which we find to perform better than $0.1$.
All exact and SGPR models use a rank-$100$ partial pivoted-Cholesky preconditioner and run PCG with a tolerance of $\Vert \trainK \bc - \by \Vert_2 / \Vert \by \Vert_2 = 1$ during training.
We constrain the learned noise to be at least $0.1$ to regularize the poorly conditioned kernel matrix for the houseelectric and Buzz datasets.

All training/evaluation experiments use a single NVIDIA GTX 2080-TI GPU unless otherwise stated.
We use the KeOps library \cite{charlier2020kernel} in conjunction with our GPyTorch BBMM/LOVE implementations to perform the partitioned kernel MVMs.
%We perform all training on one machine with 8 NVIDIA Tesla V100-SXM2-32GB-LS GPUs.

\begin{table}[!tb]
  \caption[Performance comparison between exact GPs and scalable approximations on large UCI datasets.]{
    Performance comparison between exact GPs and scalable approximations on large UCI datasets using ARD Mat\'ern kernels.
    All trials on $N < 100,\!000$ datasets are averaged over 3 trials; $\pm$ corresponds to 1 standard deviation.
    (We are unable to scale SGPR to HouseElectric due to its memory requirements when $m=512$.)
    {\bf Top:} test set root mean square error (RMSE).
    {\bf Bottom:} test set negative log likelihood (NLL).
  }
  \label{tab:large_exact_gp_results}
  \centering
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_results.tex}
  }
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_results_nll.tex}
  }
  \vspace{1em}
\end{table}

\paragraph{Accuracy.}
\cref{tab:large_exact_gp_results} displays the accuracy and negative log likelihoods of exact GPs and approximate methods on several large-scale datasets.
We find that exact GPs achieve lower error than approximate methods on nearly every dataset.
Notably, on certain datasets like 3droad, exact GPs achieve a half or even a quarter of the error of some approximate methods.
Although \citet{nguyen2019exact} show results for exact GPs on $N \leq 120,\!000$, this is the first set of results comparing exact GPs to approximate GPs on $N\gg 10^5$.

Moreover, we also see that performance of the scalable GP methods is dataset dependent.
Neither SVGP nor SGPR consistently outperforms the other.
Interestingly, dataset size/dimensionality do not seem to influence the relative performance of approximate methods.
For example, though Protein and Kin40K are similar in size and have similar dimensionality, the approximate methods perform worse on Kin40K (relative to the RMSE of exact GPs).

\begin{table*}[t!]
  \vspace{1em}
  \caption[Wall-clock time comparison between exact GPs and approximate GPs on large UCI datasets.]{
    Wall-clock time comparison between exact GPs and approximate GPs on large UCI datasets.
    Models are trained and evaluated on a single NVIDIA GTX 2080-TI GPU.
    All trials on $N < 100,\!000$ datasets are averaged over 3 trials; $\pm$ corresponds to 1 standard deviation.
    {\bf Top:} training time for exact GPs and scalable approximations.
    {\bf Bottom:} prediction time for exact GPs.
    Precomputation refers to computing the LOVE cache.
    Prediction refers to computing the predictive distributions for $1,\!000$ test points.
  }
  \label{tab:large_exact_gp_timings}
  \centering
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_timings.tex}
  }
  \vspace{1em}

  \resizebox{\textwidth}{!}{%
    \input{tables/large_exact_gp_timings_pred.tex}
  }

  \vspace{2em}
\end{table*}


\paragraph{Training time.}
\cref{tab:large_exact_gp_timings} displays the training time for exact and approximate GPs.
Datasets with $N \leq 100,\!000$ data points, exact GPs can be trained in minutes.
Datasets with $N \geq 100,\!000$ can be trained in hours.
While approximate GPs tend to be faster, we do note that the training time of exact GPs is within an order of magnitude of their approximate counterparts.
It is quite remarkable that exact GPs, which require $\bigo{N^2}$ computation with BBMM, are not much slower than $\bigo{N}$ approximate methods.
We hypothesize that exact GPs benefit more from GPU acceleration, as exact kernel matrices afford more MVM parallelism than approximate kernel matrices.

\paragraph{Prediction time.}
Although exact GPs take longer to train, we find that their speed is comparable to approximate methods at test time.
\cref{tab:large_exact_gp_timings} displays the time to compute $1,\!000$ predictive means and variances at test time before and after LOVE precomputation.
All predictions are made on one NVIDIA RTX 2080 Ti GPU.
We see exact GPs take \emph{milliseconds} across all dataset sizes used.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.70\linewidth]{figures/gpu_speedup.pdf}
  \caption[Speed of BBMM training using multi-GPU computation.]{
    Speed of BBMM training using multi-GPU computation.
    On large datasets, exact GPs with BBMM achieve a near linear speedup with more GPUs.
    (Speedups are measured on NVIDIA Tesla V100-SXM2-32GB-LS GPUs using models with non-ARD Mat\'ern 3/2 kernels.)
  }
  \label{fig:gpu_speedup}
\end{figure}

\paragraph{Training acceleration with multiple GPUs.}
As discussed in \cref{sec:largeexact_method}, the MVMs in BBMM and LOVE can be distributed across multiple devices.
\cref{fig:gpu_speedup} plots the speedup as more GPUs are used for training on the KEGGU, 3DRoad, Song, and Buzz datasets.
(Speedups are measured on NVIDIA Tesla V100-SXM2-32GB-LS GPUs using models with non-ARD Mat\'ern 3/2 kernels.)
Each of these datasets achieve a nearly linear speedup when adding up to 4 GPUs.
The speedup is more pronounced for the two large datasets (3DRoad and Song).


\subsection{Ablation Studies}
With BBMM and LOVE, we can better understand how exact GPs scale to datasets with $N\gg 10^4$ compared to approximate GPs.
Here, we demonstrate how the amount of data affects exact GP performance, and how the number of inducing points affects the performance of approximate GPs.
All experimental results in this section use GPs with non-ARD Mat\'ern 3/2 kernels.

\begin{figure}[t!]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/subsampling.pdf}
  \caption{
    Test root-mean-square error (RMSE) as a function of subsampled dataset size (lower is better).
    Subsampled exact GPs outperform approximate GPs even with a quarter of the training set.
    Exact GP error continues to decrease as data is added until the full dataset is used.
  }
  \label{fig:subsampling_results}
\end{figure}

\paragraph{Do GPs need the entire dataset?}
As a non-parametric model, Gaussian processes naturally adapt to the amount of training data available.
\cref{fig:subsampling_results} shows an increase in accuracy as we increase the amount of training data on the KEGGU, 3DRoad, and Song datasets.
For each dataset, we subsample a fraction of the training data and plot the resulting test set RMSE as a function of training set size.
As expected, the error decreases monotonically as we increase the subsample size.
\cref{fig:subsampling_results} also shows the performance of exact GPs, SGPR, and SVGP trained on the entire dataset.
Strikingly, in all three cases, \textit{an exact GP with less than a quarter of the training data outperforms approximate GPs trained on the entire training set}.
Test error continues to decrease with the addition of training data.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.7\linewidth]{figures/inducing_points.pdf}
  \caption{
    Error of SVGP and SGPR methods as a function of the number of inducing points
    ($m$).  Both methods scale cubically with $m$.
    We were unable to run SGPR with more than $1,\!024$ inducing points on a single GPU.
    Exact GPs have lower error than both methods.
  }
  \label{fig:num_inducing_points}
\end{figure}

\paragraph{Would more inducing points help?}
The results in \cref{tab:large_exact_gp_results} naturally raise the question: ``can approximate models with more inducing points recover the performance of exact methods?''
In \cref{fig:num_inducing_points}, we plot test set RMSE on two datasets, Bike and Protein, as a function of the number of inducing points.
The test RMSE of SGPR and SVGP saturates on both datasets well above the test RMSE of an exact GP.
We note that using $M$ inducing points introduces a $M \times M$ matrix and a $\bigo{NM^2 + M^3}$ time complexity \cite{hensman2013gaussian,hensman2015scalable} which makes it difficult to train SGPR with $M \gg 1024$ inducing points.
It is possible to combine partitioned kernel MVMs with inducing-point methods to utilize even larger values of $M$.
However, as \cref{fig:num_inducing_points} and \cref{tab:large_exact_gp_results} show, it may be preferable to use the extra computational resources to train an exact GP on more data rather than to train an approximate GP with more inducing points.
