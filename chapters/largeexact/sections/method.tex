%!TEX root=../main.tex
\section{Method}
\label{sec:method}
To perform exact Gaussian process inference on large datasets, we must overcome
the time and space requirements of solving linear systems.
Most GP implementations use the Cholesky decomposition to
solve linear systems required for inference \cite{rasmussen2006gaussian}.
The $\bigo{N^3}$ time complexity of the decomposition makes it difficult to perform exact GP inference on datasets with $N > 10^4$ data points without distributed computing and its associated communication overhead.
In addition to this limitation, the Cholesky decomposition requires
$\bigo{N^2}$ memory to store the lower-triangular factor $\bL$ in addition to the kernel matrix itself.
At $N=500,\!000$, the decomposition requires a full terabyte of memory and a prohibitively large
amount of computational resources.
Concurrent work by \citet{nguyen2019exact} was limited to exact GPs with $N \leq 120,\!000$ due to these drawbacks.

To address the above challenges, we build on \citet{gardner2018gpytorch} and use preconditioned conjugate gradients (PCG) to solve
linear systems. We overcome the memory limitations by partitioning the kernel
matrix to perform all matrix-vector multiplications (MVMs) without ever forming the kernel matrix explicitly,
reducing the memory requirement to $\bigo{N}$. In addition, we parallelize partitioned MVMs across multiple GPUs to
further accelerate the computations, making training possible and timely even for datasets with $N > 10^6$.

\paragraph{$\bigo{N}$ memory MVM-based inference.}
The primary input to the modified PCG algorithm of \citet{gardner2018gpytorch} is {\tt mvm\_$\trainK$}, a
black-box function that performs MVMs using the kernel matrix $\trainK$.

Besides the storage cost associated with {\tt mvm\_$\trainK$}, each iteration
of PCG updates four vectors: $\bu$ (the current solution), $\br$ (the current error),
$\bp$ (the ``search'' direction for the next solution), and $\bz$ (a
preconditioned error term). Storing these vectors requires exactly $4N$ space.
The quadratic space cost associated with PCG-based GP inference only comes from computing {\tt mvm\_$\trainK$}.

Typically in the full GP setting, {\tt mvm\_$\widehat \bK_{\bX \! \bX}$} is implemented by first
computing the full $n \times n$ kernel matrix $\widehat \bK_{\bX \! \bX}$, then
computing the matrix-vector product with the full matrix. However, this would
have the same $\bigo{n^2}$ memory requirement as Cholesky-based GP inference.
Although forming $\widehat \bK_{\bX\!\bX}$ requires $\bigo{n^2}$ memory, the result of the MVM $\widehat \bK_{\bX\!\bX}
\bv$ requires only $\bigo{n}$ memory. Therefore, we reduce the memory requirement to $\bigo{n}$
by computing $\widehat \bK_{\bX\!\bX} \bv$ in separate constant-sized pieces.

\paragraph{Partitioned kernel MVMs.}
To compute $\widehat \bK_{\bX\!\bX} \bv$ in pieces, we partition
the kernel matrix $\widehat \bK_{\bX\bX}$ such that \textit{we only store a constant number of rows
at any given time}. With the $4N$ memory requirement of
storing the PCG vectors, our approach requires only $\bigo{N}$ memory.

We first partition the data matrix with $N$ points in $D$ dimensions, $\bX \in
\reals^{N \times D}$, into $P$ partitions each of which contains roughly $N/P$ data points:
%
\[
  \bX = \begin{bmatrix}
    \bX^{(1)}; &
    \cdots &
    ; \bX^{(P)}
  \end{bmatrix}
\]
%
where we use ``$;$'' to denote row-wise concatenation.
For each $X^{(\ell)}$, we can compute $\widehat \bK_{\bX^{(\ell)}\!\bX}$, which is a
roughly $(N / P) \times N$ kernel matrix
between the partition $\bX^{(\ell)}$ and the full data $\bX$.
By partitioning the kernel matrix this way, we rewrite it as a concatenation of the $P$ partitions:
%
\[
  \widehat \bK_{\bX\!\bX} = \begin{bmatrix}
      \widehat \bK_{\bX^{(1)}\bX}; &
    \cdots &
    ; \widehat \bK_{\bX^{(P)}\bX}
  \end{bmatrix}.
\]
%
Computing each partition requires access to the full training set $X$, which we
assume fits in memory. However, each partition $\widehat \bK_{\bX^{(\ell)}\!\bX}$
contains only $1/P$ of the entries of the full kernel matrix.
Rewriting the matrix-vector product $\widehat \bK_{\bX\!\bX} \mathbf v$ in terms of these partitions,
%
\[
  \widehat \bK_{\bX\!\bX} \bv = \begin{bmatrix}
    \widehat \bK_{\bX^{(1)}\!\bX} \bv; &
    \cdots &
    ; \widehat \bK_{\bX^{(P)}\bX} \bv
  \end{bmatrix},
\]
%
we see that this matrix-vector product can be computed in smaller components by
separately computing each $\widehat \bK_{\bX^{(\ell)}\!\bX} \bv$ and concatenating the results. We discard each kernel
partition $\widehat \bK_{\bX^{(\ell)}\!\bX}$ once its MVM
has been computed.
This partitioning requires access to the training data $\bX$ and vector
$\bv$ already in memory and only allocates new memory to temporarily store
the output vector $\bz$ and a $(N / P) \times N$ kernel matrix partition $
\widehat \bK_{\bX^{(\ell)}\!\bX}$.
This algorithm allows us to reduce memory usage in exchange for sequential but easily parallelizable computations.
If $P=1$ then we have the na\"ive $\bigo{N^2}$ memory MVM procedure. As $P \to N$, PCG
will only require $\bigo{N}$ memory. In practice, we set a constant number of rows
per partition according to the amount of memory available
rather than number of partitions $P$.
By keeping a partition in memory only until its component of the MVM has been computed, we can train GPs with an $\bigo{N}$ memory requirement.

\paragraph{Distributed MVMs in Parallel.}
MVM-based inference can easily take advantage of multiple GPUs or distributed
computational resources because each MVM $\widehat \bK_{\bX^{(l)}\!\bX}\bv$ can be performed
on a different device. Thus we can compute multiple such MVMs in parallel to
attain wall-clock speedups proportional to the number of devices
available on large data sets where computation time exceeds the distributed
computing overhead. Although $\bigo{N}$ memory is achievable by setting $P = \bigo{N}$, in
practice one may prefer $\bigo{N^2/P}$ memory to more effectively accelerate
MVMs on parallel hardware with the necessary memory resources.

Additionally, we note that distributed parallel MVMs
require only $O(n)$ communication.
Each partitioned matrix multiplication only has
to supply each device with a new right-hand-side vector $\bv$. Finally, if $w$ devices are used, the output from each
device will be a vector of length $n / pw$. Thus only $\bigo{n}$ data are copied to or from the devices.
In contrast, distributing the Cholesky decomposition across multiple devices
would require $\bigo{n^2}$ communication \citep{gustavson2006three} \citep{nguyen2019exact}.

Distributed computations have been utilized for approximate GP inference through
mixture-of-experts GP models \citep{deisenroth2015distributed}. Concurrent with the Cholesky-based approach by \citet{nguyen2019exact},
our method is the first to parallelize \emph{exact} Gaussian process inference through distributed computing.

\paragraph{Predictions.} At inference time, we must compute the predictive mean and variance
given in \eqref{eq:pred_mean} and \eqref{eq:pred_var}. Although the predictive mean contains a linear solve $\widehat \bK_{\bX \! \bX}^{-1}
\by$, this solve depends only on the training data. The result of this solve can
be stored in a linear vector $\mathbf{a}$ and used for subsequent
predictions. Therefore, computing the predictive mean requires computing
$\widehat \bK_{\bx^{*} \! \bX} \mathbf{a}$. Because this equation involves no linear solves, it can be computed efficiently on a single GPU.

Similarly, a training data dependent cache can be computed for the predictive variance using the method of \citet{pleiss2018constant} with a satisfactorily tight convergence tolerance.
On exact GPs, this approach affords $\bigo{n}$ predictive variance computations,\footnote{
  We do not achieve constant-time predictions as described in \cite{pleiss2018constant}.
  Reducing the $\bigo{n}$ prediction time to $\bigo{1}$ requires using structure kernel interpolation \cite{wilson2015kernel} to approximate the kernel matrix.
} removing the need to perform a linear solve at test time.
In practice, we observe that both the predictive mean and variance can be computed in less than a second on a single GPU, even if the full model required days to train on multiple GPUs.
Because predictions are fast after these precomputations, we can afford to use
more stringent criteria for CG convergence for these one-time precomputations.

\paragraph{Preconditioning.}
To accelerate the convergence of CG, \citet{gardner2018gpytorch} introduced a preconditioner for $\trainK$ derived from its \emph{partial pivoted Cholesky} decomposition. Preconditioning works by modifying CG to solve the related linear system $\bP^{-1} \trainK \bv = \bP^{-1}\by$
instead of solving the original system $\trainK \bv = \by$. These
linear systems have the same solution $\bv^{*}$. However, the number of CG iterations required depends on the
eigenvalue distribution of $\bP^{-1}\trainK$ rather than that of $\trainK$.

Computing a rank $K$ pivoted Cholesky preconditioner requires only $K$ kernel matrix rows:
an already $\bigo{N}$ space dependence. While each iteration of CG requires computing each kernel matrix partition
from scratch, the preconditioner is computed once before any iterations of CG are performed. Therefore, it can be efficient to increase the size of the preconditioner to an extent if it reduces the number of CG iterations.
While in \citet{gardner2018gpytorch} the preconditioner size is typically limited to under 20 by default, in our use
case we found that preconditioners of up to size $k=100$ provide a noticeable improvement to wall-clock speed for large datasets.

\paragraph{PCG Convergence Criteria.}
Importantly, Conjugate gradients is \emph{not} an approximate method for performing linear solves. Rather, it is a method that consumes time to perform solves to a specified tolerance. If this tolerance is low, the solve is exact. Thus, it is analogous to using gradient descent to solve a convex optimization problem (and is in fact largely what is happening). This gives us the ability to investigate how the performance of exact GPs change with different degrees of convergence.

At test time, we find that an accurate solve $\widehat \bK_{\bX \! \bX}^{-1} \by$ (with tolerance $\epsilon \leq 0.01$) is critical for good predictive performance; we therefore find that GP predictions require exact solves. For hyperparameter training, however, we find that, interestingly, less strict convergence criteria suffice, and even a looser convergence criterion of up to $\epsilon = 1$ has little impact on final model performance. Given that predictions using our approach are highly efficient (see \autoref{tab:no_ard_timings}), it may be interesting to investigate alternative approximate methods for finding good hyperparameters, and then using the techniques in this paper for exact inference and predicitons.
