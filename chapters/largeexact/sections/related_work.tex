%!TEX root=../main.tex
\section{Related Work}

\paragraph{MVM-based GP inference.}
Conjugate gradients and related MVM-based algorithms \cite{gibbs1997efficient,ubaru2017fast,dong2017scalable} have been used in certain settings throughout the GP literature.
However, these methods have typically been used when the kernel matrix is structured and affords fast matrix-vector multiplications.
\citet{cunningham2008fast} note that CG reduces asymptotic complexity when the data lie on a regularly-spaced grid because the kernel matrix is structured and affords $\bigo{n \log n}$ MVMs.
This idea was extended to multi-dimensional grids by \citet{saatcci2012scalable}.
\citet{wilson2015kernel} introduce a general-purpose GP approximation specifically designed for CG.
They combine a structured inducing point matrix with sparse interpolation for approximate kernel matrices with nearly-linear MVMs.

More recently, there has been a push to use MVM-based methods on exact GPs.
\citet{cutajar2016preconditioning} use conjugate gradients to train exact GPs on datasets with up to $50,\!000$ points.
The authors investigate using off-the-shelf preconditioners and develop new ones based on inducing-point kernel approximations.

\paragraph{Approximate GP methods.}
There are several approximations to GP inference that require $\leq O(n^2)$ memory and scale to large datasets.
Perhaps the most common class of approaches are \emph{inducing point methods} \cite{quinonero2005unifying,snelson2006sparse}, which introduce a set of $m \ll n$ data points $\bZ$ to form a low-rank kernel approximation:
\[
  \bK_{\bX \! \bX} \approx \bK_{\bX \! \bZ} \bK_{\bZ \! \bZ}^{-1} \bK_{\bZ \! \bX}.
\]
Training and predictions with this approximation take $\bigo{n m^2}$ time and $\bigo{n m}$ space.
Here we highlight some notable variants of the basic approach, though it is by no means an exhaustive list -- see \cite{liu2018gaussian} for a more thorough review.
\emph{Sparse Gaussian process regression} (SGPR) \cite{titsias2009variational} selects the inducing points $\bZ$ through a regularized objective.
\emph{Structured kernel interpolation} (SKI) \cite{wilson2015kernel} and its variants \cite{gardner2018product} place the inducing points on a grid, in combination with sparse interpolation,
for $\mathcal{O}(n + g(m))$ computations and memory, where $g(m) \approx m$. \emph{Stochastic variational Gaussian processes} (SVGP) \cite{hensman2013gaussian} introduce a set of
variational parameters that can be optimized using minibatch training. Recent work has investigated how to scale up the number of inducing points using tensor decompositions \cite{evans2018scalable,izmailov2018scalable}.
