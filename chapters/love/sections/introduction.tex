%!TEX root=../main.tex
\section{Introduction}
Applying a Gaussian process model to previously unseen test data returns a \emph{predictive distribution} rather than a single point prediction.
This predictive distribution, which is formed in a non-parametric fashion, has several advantages.
In particular, the uncertainties expressed by the predictive distribution naturally vary with the size of the training dataset.
Consequentially, Gaussian process predictions are well-calibrated---even for data points that lie far away from previously-seen training data \cite{rasmussen2001occam,wilson2014thesis}.
This property is crucial for applications where incorrect predictions could have catastrophic consequences, such as in medicine \cite{schulam2017if} or large-scale robotics \cite{deisenroth2015gaussian}.

This non-parametric formulation unfortunately comes with a computational downside.
Computing predictions with a GP requires performing linear solves with the $N \times N$ training covariance matrix (see \cref{eqn:predictive_mean,eqn:predictive_var}).
BBMM and recent advances in \emph{inducing point methods} can be used to reduce some of these computational requirements.
For a test point $\bxtest$, the predictive mean is expressed as $\bk_{\bX\bxtest}^\top {\blue \ba}$, where $\blue \ba = \trainK^{-1} \by$ is a pre-computed solve dependent only on training data.
After computing $\ba$ in $\bigo{N^2}$ time using conjugate gradients, subsequent predictive means only require a $\bigo{N}$ inner product.
This computation can be reduced to $\bigo{1}$ when used in conjunction with Kernel Interpolation for Scalable Structured GP models (KISS-GP, see \cref{sec:kissgp}).
We will briefly discuss predictive mean computations in \cref{sec:motivation_means}.

However, these computational savings do not naturally extend to predictive uncertainties.
Computing the predictive variance $\vartest{\bxtest}$ requires computing $\trainK^{-1} \bk_{\bX\bxtest}$ (see \cref{eqn:predictive_var}), which depends on the test point $\bxtest$ and therefore cannot be computed upfront.
Consequentially the most expensive computations of \cref{eqn:predictive_var} cannot be easily pre-computed and cached.
Additionally, drawing samples from the predictive distributions---a necessary operation in many applications---is similarly expensive.
Existing fast approximations for these operations \cite{papandreou2011efficient,wilson2015thoughts,wang2017max} typically incur a significant amount of error.
Matching the reduced complexity of predictive mean inference without sacrificing accuracy has remained an open problem.
The majority of this chapter is therefore dedicated to reducing the computational requirements of predictive variances and sampling.

In this chapter, we provide a matrix-vector multiplication (MVM) solution based on the tridiagonalization algorithm of \citet{lanczos1950iteration}.
Our method takes inspiration from BBMM and uses a matrix-vector multiplication (MVM) algorithm for scalability, programmability, and GPU acceleration.
We express the predictive covariance between $\bxtest$ and $\bxtestprime$ as
$\bk_{\bX\bxtest}^\top \blue{\bC} \: \bk_{\bX\bxtest}$,
where $\blue \bC \approx \trainK^{-1}$ is low-rank $N \times N$ approximation.
Using the Lanczos algorithm, we can efficiently form this low-rank decomposition using $J \ll N$ matrix-vector multiplications with $\trainK$.
After this one-time upfront computation, all variances can be computed in \emph{linear time} -- $\bigo{JN}$ -- per (co)variance entry.
When used in conjunction with KISS-GP, this complexity can be further reduced to \emph{constant time}, and posterior samples can be drawn in \emph{linear time}.

We refer to this method as LanczOs Variance Estimates, or LOVE{} gor short.
LOVE{} has the lowest asymptotic complexity for computing predictive (co)variances and drawing samples with GPs.
We empirically validate LOVE{}, used in conjunction with KISS-GP, on seven datasets and find that it consistently provides substantial speedups over existing methods \emph{without sacrificing accuracy}.
Variances and samples are accurate to within four decimals, and can be computed \emph{up to 18,000 times faster.}
