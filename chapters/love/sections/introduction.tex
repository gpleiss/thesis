%!TEX root=../main.tex
\section{Introduction}
Applying a Gaussian process model to previously-unseen test data returns a \emph{predictive distribution} rather than a point prediction.
Gaussian process predictive distribution are non-parametric and naturally adapt to the amount of training data.
As a result, these predictions tend to be well-calibrated---even for data points that lie far away from previously-seen training data \cite{rasmussen2006gaussian,wilson2014thesis}.
This property is crucial for applications where incorrect predictions could have catastrophic consequences, such as in medicine \cite{schulam2017if} or large-scale robotics \cite{deisenroth2015gaussian}.

This non-parametric formulation unfortunately comes with a computational downside.
Computing predictions with a GP requires performing linear solves with the $N \times N$ training covariance matrix (see \cref{eqn:predictive_mean,eqn:predictive_var}).
BBMM and recent advances in {inducing point methods} can be used to reduce some of these computational requirements.
Using a simple caching strategy (described in the next section), predictive means can be computed in $\bigo{N}$ time.
%For a test point $\bxtest$, the predictive mean is expressed as $\bk_{\bX\bxtest}^\top {\blue \ba}$, where $\blue \ba = \trainK^{-1} \by$ is a pre-computed solve dependent only on training data.
%After computing $\blue \ba$ in $\bigo{N^2}$ time using conjugate gradients, subsequent predictive means only require a $\bigo{N}$ inner product.
This computation can be reduced to $\bigo{1}$ when used in conjunction with Kernel Interpolation for Scalable Structured GP models (KISS-GP, see \cref{sec:kissgp}).

Unfortunately, predictive uncertainties remain a computational bottleneck, even with BBMM.
The predictive variance $\vartest{\bxtest}$ requires computing $\trainK^{-1} \bk_{\bX\bxtest}$ (see \cref{eqn:predictive_var}), which depends on the test point $\bxtest$ and therefore cannot be computed upfront.
%Consequentially the most expensive computations of \cref{eqn:predictive_var} cannot be easily pre-computed and cached.
%Additionally, drawing samples from the predictive distributions---a necessary operation in many applications---is similarly expensive.
%Existing fast approximations for these operations \cite{papandreou2011efficient,wilson2015thoughts,wang2017max} typically incur a significant amount of error.
Matching the complexity of predictive mean inference without sacrificing accuracy has remained an open problem.
The majority of this chapter is therefore dedicated to reducing the computational requirements of predictive variances.

We provide a matrix-vector multiplication (MVM) solution based on the tridiagonalization algorithm of \citet{lanczos1950iteration}.
We express the predictive covariance between $\bxtest$ and $\bxtestprime$ as
$\bk_{\bX\bxtest}^\top {\blue{\bC}} \: \bk_{\bX\bxtest}$,
where $\blue \bC \approx \trainK^{-1}$ is a low-rank $N \times N$ approximation.
Using the Lanczos algorithm, we can efficiently form this low-rank decomposition using $J \ll N$ matrix-vector multiplications with $\trainK$.
After this one-time upfront computation, all variances can be computed in \emph{linear time} -- $\bigo{JN}$ -- per (co)-variance entry.
When used in conjunction with KISS-GP, this complexity can be further reduced to \emph{constant time}, and posterior samples can be drawn in \emph{linear time}.

We refer to this method as LanczOs Variance Estimates, or LOVE{} for short.
LOVE{} has the lowest asymptotic complexity for computing predictive (co)-variances with GPs.
We empirically validate LOVE{} on seven datasets and find that it consistently provides substantial speedups over existing methods \emph{without sacrificing accuracy}.
Variances and samples are accurate to within four decimals, and can be computed \emph{up to 18,000 times faster.}
