\section{LanczOs Variance Estimates (LOVE)}
\label{sec:love_method}

\input algorithms/love

We propose to overcome these limitations through an altered pre-computation step.
In particular, we can approximate $\blue \trainK$ in \eqref{eqn:predictive_covar_2} as a low rank matrix.
Letting $\blue \bR$ be a $J \times N$ matrices such that $\blue \bR^\top \blue{\bR'} \approx \blue \trainK^{-1}$, we rewrite \eqref{eqn:predictive_covar_2} as:
%
\begin{align}
  \covtest{\bxtest}{\bxtestprime}
  &= k(\bxtest, \bxtestprime) - ({\color{blue} \bR} \bk_{\bX\bxtest})^\top ({\color{blue} \bR} \bk_{\bX\bxtestprime})
  \label{eqn:predictive_covar_2_fast}
\end{align}
%
Variance computations with \eqref{eqn:predictive_covar_2_fast} take $\bigo{JN}$ time.

\paragraph{An MVM-based low-rank approximation with Lanczos.}
There are many possible ways to form a low-rank approximation of $\trainK^{-1}$.
Our proposed method will make use of the Lanczos algorithm from \cref{sec:lanczos}, which will generate the low-rank approximation through matrix-vector multiplication (MVMs).
This makes it possible to easily transfer this approximation to specialty GP models (e.g. scalable models, multitask models, etc.) and the computation will effectively utilize GPU acceleration.
Moreover, as we will demonstrate in \cref{sec:love_results}, the Lanczos low-rank approximation rapidly converges to the true inverse.

Recall from \cref{sec:lanczos} that $J$ iterations of Lanczos tridiagonalization approximates matrix solves:
\[
  \trainK^{-1} \bb \approx \bQ \bT^{-1} \bQ^{\top} \bb,
\]
where the orthonormal matrix $\bQ \in \reals^{N \times J}$ and tridiagonal matrix $\bT \in \reals^{J \times J}$ are computed with respect to probe vector $\bb$.
As argued by \citet{parlett1980new}, \citet{saad1987lanczos}, and \citet{schneider2001krylov}, the $\bQ$ and $\bT$ matrices can be used to approximate subsequent solves
$\trainK^{-1} \bb' \approx \bQ \bT^{-1} \bQ^\top \bb'$.
We exploit this fact and use $\bQ \bT^{-1} \bQ^\top$ to be a general-purpose approximation to $\trainK^{-1}$.
When $J \ll N$ (e.g. $J \approx 100$), then this will be a low-rank approximation.

To compute the $\bR$ matrix in \cref{eqn:predictive_covar_2_fast}, we simply run $J$ iterations of Lanczos:
\begin{align*}
  {\color{blue} \trainK^{-1}} &\approx \overbracket{\color{blue} \bQ \bT^{-1} \bQ^\top}^{\text{apply Lanczos}}
  \\
  &=
  \underbracket{ \left( {\color{blue} \bQ \bL_\bT^{-\top}} \right)}_{\color{blue} \bR^\top}
  \underbracket{ \left( {\color{blue} \bL_\bT^{-1} \bQ^\top} \right)}_{\color{blue} \bR}
\end{align*}
%
where $\bL_\bT$ is the Cholesky factor of $\bT$.
Applying Lanczos to $\trainK$ requires $J$ MVMs for a total of $\bigo{J \mvm(\trainK)}$ time ($\mvm(\trainK)$ is the complexity of one MVM with $\trainK$, which is nominally $\bigo{N^2}$).
Computing and applying the Cholesky factor $\bL_\bT$ is $\bigo{J}$ time due to the tridiagonal structure of $\bT$.

In total, the entire pre-computation phase takes $\bigo{J N^2}$ time for standard GPs.
This is the same amount of time of a single marginal likelihood computation using BBMM.
After pre-computation, each covariance takes $\bigo{JN}$ time.
We refer to this fast covariance approximation algorithm as {\bf LanczOs Variance Estimates}, or {\bf LOVE} for short.
It is summarized in \cref{alg:love} and \cref{tab:running_times}.

$J$, the size of the low-rank approximation, depends on the conditioning of $\trainK$ and not its size.
Empirically find that $J\leq100$ is sufficient for most matrices for $N \leq 20,\!000$, and therefore $J$ can be considered to be constant.



\section{LOVE with KISS-GP}

\input algorithms/love_kissgp

As with BBMM, LOVE solely relies on matrix-vector multiplication and makes no assumption about the structure of $\trainK$.
Therefore, \cref{alg:love} can be applied out-of-the-box to any specialty GP model including scalable approximations.
In this section, we demonstrate that LOVE is an especially compelling algorithm for the scalable KISS-GP framework.
With a few modifications to \cref{alg:love}, KISS-GP + LOVE can achieve \emph{constant-time covariance approximations} and \emph{linear time posterior samples}.

\paragraph{Constant-time co-variances with KISS-GP + LOVE.}
The KISS-GP approximation $\color{blue} \bW_\bX^\top \bK_{\bZ \bZ} \bW_\bX$ allows us to make additional pre-computations to further reduce test-time complexity.
In particular,
%
\begin{align}
  \covtest(\bxtest, \bxtestprime)
  &\approx k(\bxtest, \bxtestprime) - \bk_{\bX \bxtest}^\top {\color{blue} \bR^\top \bR} \bk_{\bX \bxtestprime}
  \nonumber
  \\
  &\approx k(\bxtest, \bxtestprime) - \left( \bw^\top_{\bxtest} \right. \underbracket{\left. {\color{blue} \bK_{\bZ\bZ} \bW_\bX} \right) {\color{blue} \bR^\top}}_{\color{blue} \widetilde \bR^\top}
  \underbracket{{\color{blue} \bR} \left( {\color{blue} \bW_\bX^\top \bK_{\bZ\bZ}} \right.}_{\color{blue} \bR} \left. \bw_{\bxtestprime} \right)
  \nonumber
  \\
  &\approx k(\bxtest, \bxtestprime) -
  \left( {\color{blue} \widetilde \bR} \bw^\top_{\bxtest} \right)^\top
  \left( {\color{blue} \widetilde \bR} \bw^\top_{\bxtestprime} \right)
  \label{eqn:pred_covar_ski_fast}
\end{align}
%
The matrix $\color{blue} \widetilde \bR = \bR \bK_{\bZ\bZ} \bW_\bX$ is a $J \times M$ matrix.
Variance computations with \eqref{eqn:pred_covar_ski_fast} take $\bigo{J}$ time due to the sparsity of $\bw_{\bxtest}$ and $\bw_{\bxtestprime}$.
Choosing $J \approx 100$, which is sufficient for the conditioning of most matrices, KISS-GP covariance computations with \cref{eqn:pred_covar_ski_fast} take \emph{constant time}.

Moreover, this additional precomputation step takes negligible time.
The complexity of computing $\color{blue} \bR$ is $\bigo{J(N + M \log M)}$, as Lanczos requires $J$ MVMs with $\color{blue} \trainK$ and KISS-GP affords $\bigo{N + M \log M}$ MVMs.
Forming $\color{blue} \widetilde \bR$ requires multiplying the $J \! \times \! N$ $\color{blue} \bR$ matrix by $\color{blue} \bK_{\bZ\bZ}$ and $\color{blue} \bW_\bX$, which also takes $\bigo{J(N + M \log M)}$ time.
Therefore, the modified KISS-GP + LOVE precomputation is \emph{near-linear time}.
It is summarized in \cref{alg:love_kissgp} and \cref{tab:running_times}.



\subsection{Predictive distribution sampling with LOVE{}}
\label{sec:sampling_method}

LOVE{} can also be used to compute predictive \emph{covariances} and operations involving the predictive covariance matrix.
Let $\bXtest = [\bxtest_1, \ldots, \bxtest_t]$ be a test set of $t$ points.
%In many applications \eqref{eq:pred_covar} will be used only to compute the predictive variance terms for each $\bxtest_i$,  i.e. $k_{f \mid \dset} (\bxtest_i, \bxtest_i)$.
To draw samples from $\bfntest \! \mid \! \dset$ --- the predictive function evaluated on $\bxtest_1, \ldots, \bxtest_t$, the cross-covariance terms (i.e. $k_{f \mid \dset} (\bxtest_i, \bxtest_j)$) are necessary in addition to the variance terms ($k_{f \mid \dset} (\bxtest_i, \bxtest_i)$).
Sampling GP posterior functions is a common operation.
In Bayesian optimization for example, several popular acquisition functions -- such as predictive entropy search \cite{hernandez2014predictive}, max-value entropy search \cite{wang2017max}, and knowledge gradient \cite{frazier2009knowledge} -- require posterior sampling.

However, posterior sampling is an expensive operation when querying at many test points.
The predictive distribution $\bfntest \! \mid \! \dset$ is multivariate normal with mean $\mu_{f \mid \dset} (\bX^*) \in \reals^t$ and covariance $k_{f \mid \dset} (\bX^*, \bX^*) \in \reals^{t \times t}$.
We sample $\bfntest \! \mid \! \dset$ by reparametrizing Gaussian noise samples $\mathbf{v} \sim \normaldist{0}{\bI{}^{t\!\times \!t}}$:
%
\begin{equation}
  \mu_{f \mid \dset} (\bX^*) + S \mathbf{v},
  \label{eq:sample}
\end{equation}
%
where $S$ is a matrix such that $S S^\top = k_{f \mid \dset} (\bX^*, \bX^*)$.
Typically $S S^\top$ is taken to be the Cholesky decomposition of $k_{f \mid \dset} (\bX^*, \bX^*)$.
Computing this decomposition incurs a $\bigo{t^3}$ cost on top of the $\bigo{t^2}$ covariance evaluations.
This may be costly, even with constant-time covariance computations.
Parametric approximations are often used instead of exact sampling \cite{deisenroth2011pilco}.

\paragraph{A Fast Low-Rank Sampling Matrix.} We use LOVE{} and KISS-GP to rewrite \eqref{eq:pred_covar_ski_fast} as
%
\begin{align}
  k_{f \mid \dset} (\bX^*, \bX^*)
  &\approx \bW^\top_{X^*} \blue{K_{UU}} W_{X^*} - (\blue \bR \bW_{X^*})^\top (\blue{\bR'} \bW_{\bXtest})
    \notag \\
    &= \bW^\top_{\bX^*} \blue{\left( \bK_{UU} - \bR^\top \bR' \right)} \bW_{X^*}.
    \label{eq:pred_covar_ski_interp_form12}
\end{align}
%
where $W_{X^*} = [\bw_{x^*_1}, \ldots, \bw_{x^*_n}]$ is the interpolation matrix for test points.
We have reduced the full covariance matrix to a test-independent term ($\blue{ K_{UU} - R^\top R' }$) that can be pre-computed.
We apply the Lanczos algorithm on this term during pre-computation to obtain a rank-$k$ approximation:
%
\begin{align}
  \blue{K_{UU} - R^\top R' \approx Q'_k T'_k Q_k^{\prime\top} }.
  \label{eqn: lancapprox}
\end{align}
%
This Lanczos decomposition requires $k$ matrix vector multiplies with $\blue{ K_{UU} - R^{\top}R' }$, each of which requires $\bigo{m \log m}$ time.
Substituting \eqref{eqn: lancapprox} into \eqref{eq:pred_covar_ski_interp_form12}, we get:
%
\begin{align}
  k_{f \mid \dset} (\bX^*, \bX^*) = W^\top_{X^*} \blue{Q'_{k}T'_{k}Q_{k}^{\prime\top}} W_{X^*}.
  \label{eq:sampling_pre_cholesky}
\end{align}
If we take the Cholesky decomposition of $T'_k = \!L L^\top$ (a $\bigo{k}$ operation since $T'_{k}$ is tridiagonal), we rewrite \eqref{eq:sampling_pre_cholesky} as
%
\begin{align}
  k_{f \mid \dset} (\bX^*, \bX^*)
  &\approx  W^\top_{X^*} \blue{\underbrace{Q'_k L}_{S}} \blue{\underbrace{L^\top Q_k^{\prime\top}}_{S^\top}} W_{X^*}.
    \label{eq:pred_covar_ski_interp_form}
\end{align}
%
Setting $\blue{ S= Q'_k L_{T_k} }$, we see that $k_{f \mid \dset} (\bX^*, \bX^*) = (W_X^\top \blue S)(W_X^\top \blue S)^{\top}$.
$\blue S \! \in \! \reals^{m \times k}$ can be pre-computed and cached since it does not depend on test data.
In total, this pre-computation takes $\bigo{k m \log m + m k^2}$ time in addition to what is required for fast variances.
To sample from the predictive distribution, we need to evaluate \eqref{eq:sample}, which involves multiplying $W^\top_{X^*} S \mathbf{v}$.
Multiplying $\mathbf{v}$ by $S$ requires $\bigo{mk}$ time, and finally multiplying by $W^{\top}_{X^{*}}$ takes $\bigo{tk}$ time.
Therefore, drawing $s$ samples (corresponding to $s$ different values of $\mathbf{v}$) takes $\bigo{sk(t + m)}$ time total during the testing phase (see \autoref{tab:running_times}) -- a \emph{linear} dependence on $t$.


\subsection{Extension to additive kernel compositions}
LOVE{} is applicable even when the KISS-GP approximation is used with an additive composition of kernels,
%
\begin{equation}
  \tilde{k}(\bx^{(i)}, \bx^{(j)}) =
  \bw^{(1)\top}_{\bx^{(i)}} K^{(1)}_{U \! U} \bw^{(1)}_{\bx^{(j)}} + \ldots + \bw^{(d)\top}_{\bx^{(i)}} K^{(d)}_{U \! U} \bw^{(d)}_{\bx^{(j)}}.
  \notag
\end{equation}
Additive structure has recently been a focus in several Bayesian optimization settings, since the cumulative regret of additive models depends linearly on the number of dimensions
\cite{kandasamy2015high,wang2017batched,gardner2017discovering,wang2017max}.
Additionally, deep kernel learning GPs \citep{wilson2016stochastic,wilson2016deep} typically uses sums of one-dimensional kernel functions.
To apply LOVE{}, we note that additive composition can be re-written as
%
\begin{equation}
  \tilde{k}(\bx^{(i)}, \bx^{(j)}) =
  \begin{bmatrix}
    \bw^{(1)}_{\bx^{(i)}} \\
    \vdots \\
    \bw^{(d)}_{\bx^{(i)}}
  \end{bmatrix}^\top
  \!
  \begin{bmatrix}
    K^{(1)}_{U \! U} & \!\! \ldots \!\! & 0 \\
    \vdots & \!\! \ddots \!\! & \vdots \\
    0 & \!\! \ldots \!\! & K^{(d)}_{U \! U}
  \end{bmatrix}
  \!
  \begin{bmatrix}
    \bw^{(1)}_{\bx^{(j)}} \\
    \vdots \\
    \bw^{(d)}_{\bx^{(j)}}
  \end{bmatrix}.
  \label{eq:multi_dimensional_kernel_block}
\end{equation}
%
The block matrices in \eqref{eq:multi_dimensional_kernel_block} are analogs of their 1-dimensional counterparts in \eqref{eq:ski}.
Therefore, we can directly apply \autoref{alg:fast_pred_var}, replacing $W_X$, $\bw_{\bxtest_i}$, $\bw_{\bxtest_j}$, and $K_{UU}$ with their block forms.
The block $\bw$ vectors are $\bigo{d}$-sparse, and therefore interpolation takes $\bigo{d}$ time.
MVMs with the block $K_{UU}$ matrix take $\bigo{dm\log m}$ time by exploiting the block-diagonal structure. With $d$ additive components, predictive variance computations cost only a factor $\bigo{d}$ more than their 1-dimensional counterparts.
