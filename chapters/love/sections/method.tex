\section{LanczOs Variance Estimates (LOVE)}
\label{sec:love_method}

\input algorithms/love

We propose to overcome these limitations through an altered pre-computation step.
In particular, we can approximate $\blue \trainK$ in \cref{eqn:predictive_covar_2} as a low rank matrix.
Letting $\blue \bR$ be a $J \times N$ matrix such that $\blue \bR^\top \blue{\bR} \approx \blue \trainK^{-1}$, we rewrite \cref{eqn:predictive_covar_2} as:
%
\begin{align}
  \covtest{\bxtest}{\bxtestprime}
  &= k(\bxtest, \bxtestprime) - ({\color{blue} \bR} \bk_{\bX\bxtest})^\top ({\color{blue} \bR} \bk_{\bX\bxtestprime}) + \sigma^2_\text{obs}
  \label{eqn:predictive_covar_2_fast}
\end{align}
%
Variance computations with \cref{eqn:predictive_covar_2_fast} take $\bigo{JN}$ time.

\paragraph{An MVM-based low-rank approximation with Lanczos.}
There are many possible ways to form a low-rank approximation of $\blue \trainK^{-1}$.
Our proposed method will make use of the Lanczos algorithm from \cref{sec:lanczos}, which will generate the low-rank approximation through matrix-vector multiplication (MVMs).
%This makes it possible to easily transfer this approximation to specialty GP models (e.g. scalable models, multitask models, etc.) and the computation will effectively utilize GPU acceleration.
As we will demonstrate in \cref{sec:love_results}, the Lanczos low-rank approximation rapidly converges to the true inverse.

Recall from \cref{sec:lanczos} that $J$ iterations of Lanczos tridiagonalization approximates matrix solves:
\[
  {\color{blue} \trainK^{-1}} \bb \approx {\color{blue} \bQ \bT^{-1} \bQ^{\top}} \bb,
\]
where the orthonormal matrix ${\color{blue} \bQ} \in \reals^{N \times J}$ and tridiagonal matrix ${\color{blue} \bT} \in \reals^{J \times J}$ are computed with respect to probe vector $\bb$.
As argued by \citet{parlett1980new}, \citet{saad1987lanczos}, and \citet{schneider2001krylov}, the $\blue \bQ$ and $\blue \bT$ matrices can be used to approximate subsequent solves
${\color{blue} \trainK^{-1}} \bb' \approx {\color{blue} \bQ \bT^{-1} \bQ^\top} \bb'$.
We exploit this fact and use $\blue \bQ \bT^{-1} \bQ^\top$ to be a general-purpose approximation to $\blue \trainK^{-1}$.
By running $J \ll N$ Lanczos iterations (e.g. $J \approx 100$), the resulting approximation will be low-rank.
In particular, we have
\begin{align*}
  {\color{blue} \trainK^{-1}} &\approx \overbracket{\color{blue} \bQ \bT^{-1} \bQ^\top}^{\text{apply Lanczos}}
  \\
  &=
  \underbracket{ \left( {\color{blue} \bQ \bL_\bT^{-\top}} \right)}_{\color{blue} \bR^\top}
  \underbracket{ \left( {\color{blue} \bL_\bT^{-1} \bQ^\top} \right)}_{\color{blue} \bR}
\end{align*}
%
where $\blue \bL_\bT$ is the Cholesky factor of $\blue \bT$.
Applying Lanczos to $\blue \trainK$ requires $J$ MVMs for a total of $\bigo{J \mvm{\trainK}}$ time ($\mvm{\trainK}$ is the complexity of one MVM with $\blue \trainK$, which is nominally $\bigo{N^2}$).
Computing and applying the Cholesky factor $\blue \bL_\bT$ is $\bigo{J}$ time due to the tridiagonal structure of $\blue \bT$.

In total, the entire pre-computation phase takes $\bigo{J N^2}$ time for standard GPs.
This is the same amount of time of a single marginal likelihood computation using BBMM.
After pre-computation, each covariance takes $\bigo{JN}$ time.
We refer to this fast covariance approximation algorithm as {\bf LanczOs Variance Estimates}, or {\bf LOVE} for short.
It is summarized in \cref{alg:love} and \cref{tab:running_times}.

$J$, the size of the low-rank approximation, depends on the conditioning of $\blue \trainK$ and not its size.
Empirically, we find that $J\leq100$ is sufficient for most matrices with $N \leq 20,\!000$; therefore $J$ can be considered to be constant.



\begin{table*}[t!]
  \caption[Asymptotic complexities of predictive (co)-variances with LOVE versus other methods.]{
    Asymptotic complexities of predictive (co)-variances ($N$ training points, $M$ inducing points, $J$ Lanczos/CG iterations).
    \label{tab:running_times}
  }
  \vspace{0.5ex}
  \centering
  \resizebox{\textwidth}{!}{%
    \input{tables/love_complexity}
  }
  \vspace{-2ex}
\end{table*}

\begin{table*}[t!]
  \caption[Asymptotic complexities of posterior sampling with LOVE + KISS-GP versus other methods.]{
    Asymptotic complexities of posterior sampling
		($N$ training points, $M$ inducing points, $J$ Lanczos/CG iterations, $S$ samples, $T$ test points).
    \label{tab:running_times_sampling}
  }
  \vspace{0.5ex}
  \centering
  \resizebox{\textwidth}{!}{%
    \input{tables/love_sampling_complexity}
  }
  \vspace{-2ex}
\end{table*}


\subsection{Programmability}

Because LOVE is an MVM-based algorithm, it affords the same modularity as BBMM.
When LOVE is used in conjunction with scalable GP approximations/multitask models, we can take advantage of fast kernel MVMs for a $o(N^2)$ asymptotic complexity.
In GPyTorch we use the {\tt LazyTensor} construct from \cref{sec:programmability} to adapt LOVE to specialty models.
The same {\tt \_matmul} function we use for mBCG can also be used for fast (co)-variances with LOVE.





\section{LOVE with KISS-GP}
\label{sec:love_method_kissgp}

\input algorithms/love_kissgp

In this section, we demonstrate that LOVE is an especially compelling algorithm for the scalable KISS-GP framework.
With a few modifications to \cref{alg:love}, KISS-GP + LOVE can achieve \emph{constant-time covariance approximations} and \emph{linear time posterior samples}.

\subsection{Constant-Time (Co)-Variances with KISS-GP + LOVE}
The KISS-GP approximation $\color{blue} \bW_\bX^\top \bK_{\bZ \bZ} \bW_\bX$ allows us to make additional pre-computations to further reduce test-time complexity.
In particular,
%
\begin{align}
  \covtest{\bxtest}{\bxtestprime}
  &\approx k(\bxtest, \bxtestprime) - \bk_{\bX \bxtest}^\top {\color{blue} \bR^\top \bR} \bk_{\bX \bxtestprime} + \sigma^2_\text{obs}
  \nonumber
  \\
  &\approx k(\bxtest, \bxtestprime) - \left( \bw^\top_{\bxtest} \right. \underbracket{\left. {\color{blue} \bK_{\bZ\bZ} \bW_\bX} \right) {\color{blue} \bR^\top}}_{\color{blue} \widetilde \bR^\top}
  \underbracket{{\color{blue} \bR} \left( {\color{blue} \bW_\bX^\top \bK_{\bZ\bZ}} \right.}_{\color{blue} \widetilde \bR} \left. \bw_{\bxtestprime} \right) + \sigma^2_\text{obs}
  \nonumber
  \\
  &\approx k(\bxtest, \bxtestprime) -
  \left( {\color{blue} \widetilde \bR} \bw^\top_{\bxtest} \right)^\top
  \left( {\color{blue} \widetilde \bR} \bw^\top_{\bxtestprime} \right) + \sigma^2_\text{obs}
  \label{eqn:pred_covar_ski_fast}
\end{align}
%
The matrix $\color{blue} \widetilde \bR = \bR \bK_{\bZ\bZ} \bW_\bX$ is a $J \times M$ matrix.
Variance computations with \cref{eqn:pred_covar_ski_fast} take $\bigo{J}$ time due to the sparsity of $\bw_{\bxtest}$ and $\bw_{\bxtestprime}$.
Taking $J$ to be a constant (as $J \approx 100$ suffices for most kernel matrices), KISS-GP covariance computations with \cref{eqn:pred_covar_ski_fast} take \emph{constant time}.

Moreover, this additional work to compute $\color{blue} \widetilde \bR$ from $\blue \bR$ takes negligible time.
The complexity of computing $\color{blue} \bR$ is $\bigo{J(N + M \log M)}$, as Lanczos requires $J$ MVMs with $\color{blue} \trainK$ and KISS-GP affords $\bigo{N + M \log M}$ MVMs.
Forming $\color{blue} \widetilde \bR$ requires multiplying the $J \! \times \! N$ $\color{blue} \bR$ matrix by $\color{blue} \bK_{\bZ\bZ}$ and $\color{blue} \bW_\bX$, which also takes $\bigo{J(N + M \log M)}$ time.
It is summarized in \cref{alg:love_kissgp} and \cref{tab:running_times}.

In addition to these fast predictive (co)-variances, LOVE + KISS-GP offers two additional speedups that are specific to KISS-GP models.




\subsection{Predictive Distribution Sampling with LOVE{} + KISS-GP}
\label{sec:sampling_method}

When used in conjunction with KISS-GP, LOVE{} can also be used to compute samples from the posterior covariance matrix.
This is a very common operation: in Bayesian optimization, several popular acquisition functions---such as predictive entropy search \cite{hernandez2014predictive}, max-value entropy search \cite{wang2017max}, and knowledge gradient \cite{frazier2009knowledge}---require posterior sampling.

Let $\bXtest = [ \bxtest_1, \ldots, \bxtest_T ]$ be a test set of $T$ points.
To draw samples from $p( f(\bxtest_1), \ldots, f(\bxtest_T)  \! \mid \! \dset)$---the posterior function evaluated on $\bxtest_1, \ldots, \bxtest_T$,
the cross-covariance terms (i.e. $\covtest{\bxtest_i}{\bxtest_j}$) are necessary in addition to the variance terms ($\vartest{\bxtest_i}$).
We sample $\bepsilon \sim p( f(\bxtest_1), \ldots, f(\bxtest_T) \! \mid \! \dset)$ through the reparameterization trick \cite{kingma2014auto,rezende2014stochastic}:
%
\[
  \bmeantest = \begin{bmatrix} \meantest{\bxtest_1} \\ \vdots \\ \meantest{\bxtest_1} \end{bmatrix},
  \quad
  \Covtest = \begin{bmatrix}
    \covtest{\bxtest_1}{\bxtest_1} & \cdots & \covtest{\bxtest_1}{\bxtest_T} \\
    & \ddots & \\
    \covtest{\bxtest_T}{\bxtest_1} & \cdots & \covtest{\bxtest_T}{\bxtest_T} \\
  \end{bmatrix},
\]
\begin{equation}
  \bepsilon = \bmeantest + \bS \bepsilon' \sim p( \bfn(\bxtest_1), \ldots, \bfn(\bxtest_T) \! \mid \! \dset)
  \label{eqn:sample}
\end{equation}
%
where $\bepsilon' \sim \normaldist{\bzero}{\bI}$ and $\bS$ is some matrix such that $\bS \bS^\top = \Covtest$.
Typically $\bS \bS^\top$ is taken to be the Cholesky decomposition of the posterior covariance matrix.
Computing this decomposition incurs a $\bigo{T^3}$ cost on top of the $\bigo{T^2}$ covariance evaluations.

\paragraph{A fast KISS-GP sampling matrix.}
We use LOVE{} and KISS-GP to rewrite \cref{eqn:pred_covar_ski_fast} as
%
\begin{align}
  \Covtest
  &\approx
  \bK_{\bXtest, \bXtest} - \overbracket{
    \bW^\top_{\bXtest} \left( {\color{blue} \widetilde \bR^\top \widetilde \bR} \right) \bW_{\bXtest}
  }^\text{LOVE + KISS-GP approximation}
  \nonumber
  \\
  &\approx \overbracket{
    \bW^\top_{\bXtest} {\color{blue} \bK_{\bZ\bZ} } \bW_{\bXtest}
  }^\text{KISS-GP approximation}
   - \bW^\top_{\bXtest} \left( {\color{blue} \widetilde \bR^\top \widetilde \bR} \right) \bW_{\bXtest}
  \nonumber
  \\
  &= \bW^\top_{\bXtest} \left( {\color{blue} \bK_{\bZ\bZ} - \widetilde \bR^\top \widetilde \bR } \right) \bW_{\bXtest}
  \label{eqn:pred_covar_ski_interp_form12}
\end{align}
%
where $\bW_{\bXtest} = [\bw_{\bxtest_1}, \ldots, \bw_{\bxtest_T}]$ is the interpolation matrix for test points.
We have thus reduced the full covariance matrix to a test-independent term ($\blue{ \bK_{\bZ\bZ} - \widetilde \bR^\top \widetilde \bR }$) that can be pre-computed.
We apply the Lanczos algorithm on this term during pre-computation to obtain a rank-$J$ approximation:
%
\begin{align}
  { \color{blue} \bK_{\bZ\bZ} -  \widetilde \bR^\top \widetilde \bR' }
  \approx
  { \color{blue} \bQ' \bT' \bQ^{\prime\top} },
  \label{eqn:lancapprox}
\end{align}
%
where again ${\blue \bQ'} \in \reals^{M \times J}$ is orthonormal and ${\blue \bT'} \in \reals^{J \times J}$ is tridiagonal.
This Lanczos decomposition $\blue \bQ' \bT' \bQ^{\prime\top}$ requires $J$ matrix-vector multiplies with $\blue{ \bK_{\bZ\bZ} - \widetilde \bR^{\top} \widetilde \bR }$, each of which requires $\bigo{M \log M}$ time.
Substituting \cref{eqn:lancapprox} into \cref{eqn:pred_covar_ski_interp_form12}, we get:
%
\begin{align}
  \Covtest
  &\approx \bW^\top_{\bXtest} \left( {\color{blue} \bQ' \bT' \bQ^{\prime\top} } \right) \bW_{\bXtest}
  \nonumber
  \\
  &= \bW^\top_{\bXtest} \left( {\color{blue} \bQ' \bL_{\bT'} } \right)
  \left( {\color{blue} \bL_{\bT^{\prime\top}} \bQ^{\prime\top} } \right) \bW_{\bXtest}
  \label{eqn:sampling_pre_cholesky}
\end{align}
where ${\color{blue} \bL_{\bT'} }$ is the Cholesky factor of $\blue \bT'$ (a $\bigo{J}$ operation due to the tridiagonal structure).
Setting $\blue{ \bS = \bQ' \bL_{\bT} }$, we see that $\Covtest = (\bW_{\bXtest}^\top {\blue \bS}) (\bW_{\bXtest}^\top {\blue \bS})^{\top}$.
Moreover, ${\blue \bS} \in \reals^{M \times J}$ can be pre-computed and cached since it does not depend on test data.
In total, pre-computing $\blue \bS$ takes $\bigo{J M \log M + M J^2}$ time in addition to what is required for fast variances.
A matrix-vector multiplication with $(\bW_\bX^\top {\blue \bS})$ takes $\bigo{TJ}$ time due to the $\bigo{T}$ sparsity of $\bW_\bX$.
Therefore, drawing $S$ samples (corresponding to $S$ different values of $\bepsilon'$) takes $\bigo{SJ(T + M)}$ time (see \cref{tab:running_times_sampling})---a \emph{linear} dependence on $T$.



\subsection{Extension to Additive KISS-GP Kernel Compositions}
LOVE{} is applicable even when the KISS-GP approximation is used with an additive composition of kernels,
%
\begin{equation}
  \widetilde{k}(\bx, \bx') =
  \bw^{(1)\top}_{\bx} {\color{blue} \bK^{(1)}_{\bZ \bZ}} \bw^{(1)}_{\bx'} + \ldots + \bw^{(D)\top}_{\bx} {\color{blue} \bK^{(D)}_{\bZ \bZ}} \bw^{(D)}_{\bx'}.
  \notag
\end{equation}
Additive structure has recently been a focus in several Bayesian optimization settings, since the cumulative regret of additive models depends linearly on the number of dimensions
\cite{kandasamy2015high,wang2017batched,gardner2017discovering,wang2017max}.
Additionally, deep kernel learning GPs \citep{wilson2016deep,wilson2016stochastic} typically use sums of one-dimensional kernel functions.
To apply LOVE{}, we note that additive composition can be re-written as
%
\begin{equation}
  \widetilde{k}(\bx, \bx') =
  \begin{bmatrix}
    \bw^{(1)}_{\bx} \\
    \vdots \\
    \bw^{(D)}_{\bx}
  \end{bmatrix}^\top
  \!
  {\color{blue}
  \begin{bmatrix}
    \bK^{(1)}_{\bZ \bZ} & \!\! \ldots \!\! & 0 \\
    \vdots & \!\! \ddots \!\! & \vdots \\
    0 & \!\! \ldots \!\! & \bK^{(D)}_{\bZ \bZ}
  \end{bmatrix}
  }
  \!
  \begin{bmatrix}
    \bw^{(1)}_{\bx} \\
    \vdots \\
    \bw^{(D)}_{\bx'}
  \end{bmatrix}.
  \label{eqn:multi_dimensional_kernel_block}
\end{equation}
%
The block matrices in \cref{eqn:multi_dimensional_kernel_block} are analogs of their 1-dimensional counterparts in \cref{eqn:ski}.
Therefore, we can directly apply \cref{alg:love_kissgp}, replacing $\blue \bW_\bX$, $\bw_\bxtest$, $\bw_\bxtestprime$, and $\blue \bK_{\bZ\bZ}$ with their block forms.
The block $\bw_\bxtest$, $\bw_\bxtestprime$ vectors are $\bigo{D}$-sparse, and therefore interpolation takes $\bigo{D}$ time.
MVMs with the block $\blue \bK_{\bZ\bZ}$ matrix take $\bigo{DM\log M}$ time by exploiting the block-diagonal structure.
With $D$ additive components, predictive variance computations cost only a factor $\bigo{D}$ more than their 1-dimensional counterparts.
