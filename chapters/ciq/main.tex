\chapter{Extending BBMM to Approximate/Variational Gaussian Processes}
\label{chapter:ciq}

\section{Background}

Assume we have $M$ inducing points $\bZ = [\bz_1, \ldots, \bz_m]$.
Let $q(\bu) = \normaldist{\bu; \bm}{\bS}$ be the approximate function distribution at the inducing points $\bZ$.

For an arbitrary point $\bxtest$, the predictive distribution for the approximate Gaussian process is given by
%
\begin{equation}
  q \left( f(\bxtest) \right) = \Evover{q(\bu)}{ \: p\left( f(\bxtest) \mid \bu \right) \: }
  = \normaldist{ \bxtest; \ameantest{ \bxtest }} { \avartest{ \bxtest }}
  \label{eqn:approx_pred}
\end{equation}
%
where $\ameantest{\cdot}$ and $\avartest{\cdot}$ are the (approximate) predictive mean and covariance functions, given by:
%
\begin{align}
  \ameantest{\bxtest} &= \bk_{\bZ\bxtest}^\top \bK_{\bZ\bZ}^{-1} \bm
  \label{eqn:approx_pred_mean} \\
  \avartest{\bxtest} &= k(\bxtest, \bxtest) -
    \bk_{\bZ\bxtest}^\top \bK_{\bZ\bZ}^{-1} \left( \bK_{\bZ\bZ} - \bS \right) \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bxtest}
  \label{eqn:approx_pred_covar}
\end{align}
%
We learn the parameters $\bm$ and $\bS$ (as well as the inducing locations $\bZ$ and associated kernel/likelihood hyperparameters) through gradient-based optimization.
We can either optimize the evidence lower bound \cite{hensman2015scalable} for variational inference or the predictive log likelihood \cite{jankowiak2020parametric} for regularized maximum likelihood estimation:
%
\begin{align}
	-\loglik_\text{ELBO} &= -\sum_{i=1}^N \Evover{q(f(\bx^{(i)}))}{  \: \log p( y^{(i)} \mid f(\bx^{(i)}) ) \: } + \kl{ q(\bu) }{ p(\bu) },
	\label{eqn:elbo}
	\\
	-\loglik_\text{Pred.} &= -\sum_{i=1}^N \: \log \Evover{q(f(\bx^{(i)}))}{  \: p( y \mid f(\bx) ) \: } + \beta_\text{reg} \kl{ q(\bu) }{ p(\bu) }.
	\label{eqn:predll}
\end{align}
%
For the predictive log likelihood $\loglik_\text{Pred.}$ the $\beta_\text{reg}$ term is a tunable hyperparameter which controls the amount of regularization.



\subsection{The Whitened Parameterization}

Expanding the KL divergence term in \cref{eqn:elbo,eqn:predll} gives us
%
\begin{equation}
	\kl{ q(\bu) }{ p(\bu) } = \frac{1}{2} \Bigl( \bm^\top \bK_{\bZ\bZ}^{-1} \bm + \tr{ \bK_{\bZ\bZ}^{-1} \bS } - \log \vert \bK_{\bZ\bZ}^{-1} \bS \vert - M \Bigr).
	\label{eqn:kldiv}
\end{equation}
%
%The KL divergence is minimized when $\bm = \bzero$ and $\bS = \bK_{\bZ\bZ}$.
This term can cause optimization challenges because the $\bm$ and $\bS$ parameters will need to adapt to changes in $\bK_{\bZ\bZ}$ as the kernel hyperparameters and inducing point locations are optimized.
A solution to this challenge is to perform optimization in the \emph{whitened coordinate space} \cite{matthews2017scalable}.
\[ \bu' = \bR^{-1} \bu, \]
where $\bR$ is a matrix such that $\bR \bR^\top = \bK_{\bZ\bZ}$.
In this coordinate system, note that $p(\bu') = \normaldist{ \bu'; \bzero}{\bI}$.
If we directly optimize the \emph{whitened parameters} $\bm' = \bR^{-1} \bm,$ $\bS' = \bR^{-1} \bS \bR^{-\top},$
the resulting KL divergence becomes
%
\begin{equation}
	\kl{ q(\bu') }{ p(\bu') } = \frac{1}{2} \Bigl( \bm^{\prime \top} \bm' + \tr{ \bS' } - \log \vert \bS' \vert - M \Bigr).
	\label{eqn:kldiv_whitened}
\end{equation}
%
This term does not depend on $k(\cdot,\cdot)$ or $\bZ$, which makes optimization much more stable.
Under the whitened coordinate system, the predictive mean and variance of \cref{eqn:approx_pred_mean,eqn:approx_pred_covar} become
%
\begin{align}
  \ameantest{\bxtest} &= \bk_{\bZ\bxtest}^\top \bR^{-\top} \bm'
  \label{eqn:approx_pred_mean_white} \\
  \avartest{\bxtest} &= k(\bxtest, \bxtest) -
    \bk_{\bZ\bxtest}^\top \bR^{-\top} \left( \bI - \bS' \right) \bR^{-1} \bk_{\bZ\bxtest}
  \label{eqn:approx_pred_covar_white}
\end{align}
%
In practice, it is common to set $\bR$ to the Cholesky factor of $\bK_{\bZ\bZ}$.
It is worth noting that all possible choices of $\bR$ are equivalent up to an orthonormal rotation.
\gp{Jake/David: pretty sure I convinced myself this is true, but confirm plz.}


\section{Contour Integral Quadrature}
Computing the Cholesky factor of $\bK_{\bZ\bZ}$ incurs $\bigo{M^3}$ computational complexity and requires $\bigo{M^2}$ storage.
These $\bigo{M^3}$ computations are also not very amenable to GPU acceleration as the Cholesky algorithm is inherently sequential.
Consequentially it is common to keep $M \ll N$ (in practice no greater than 1000), which limits the predictive capabilities of the approximate GP.
Our goal is to replace the Cholesky decomposition with a more scalable computation that allows for much larger values of $M$.

Rather than setting $\bR$ to the Cholesky decomposition, we instead choose to set $\bR = \bK_{\bZ\bZ}^{1/2}$.
At first glance this may appear a counter-intuitive choice.
Directly computing a matrix square root requires an eigen-decomposition of $\bK_{\bZ\bZ}$, which would offer no computational or space savings over the Cholesky decomposition.
Moreover, performing gradient descent would require computing the partial $\partial \bK_{\bZ\bZ}^{-1/2} / \partial \bK_{\bZ\bZ}$, which has no closed form \gp{Confirm this} and typically is solved through expensive iterative methods.\footnote{
  In particular, $\partial \bK_{\bZ\bZ}^{-1/2} / \partial \bK_{\bZ\bZ}$ is the solution to a Lyapunov equation.
}
We will overcome both the computational complexity and gradient issues by computing $\bK_{\bZ\bZ}^{-1/2}$ through an \emph{approximate contour integral}.
In fact, by using a specially designed quadrature method, $\bK_{\bZ\bZ}^{-1/2} \bk_{\bZ\bxtest}$ can be computed up to arbitrary precision with less computational and space complexity than Cholesky.

\paragraph{Formulating $\bK_{\bZ\bZ}^{-1/2}$ as matrix solves.}
The inverse square root of a positive definite matrix $\bA$ can be expressed by the following intergral:
%
\begin{equation}
	\bA^{-1/2} = \frac{i}{\pi} \int_\Gamma \left( t^2 \bI - \bA \right)^{-1} \intd t.
	\label{eqn:contour_integral}
\end{equation}
%
where $\Gamma$ is a contour in the complex plane that surround the spectrum of $\bA^{-1/2}$.
(See \cite{hale2008computing} or \citep[][Ch. 6]{higham2008functions} for a derivation.)
While this integral has primarily been used in a theoretical context, \citet{hale2008computing} demonstrate that a real-valued quadrature approximation of \cref{eqn:contour_integral} is a numerically viable way to compute matrix roots:
%
\begin{equation}
	\bA^{-1/2} \approx \sum_{q=1}^Q w_q \left( t_q^2 \bI - \bA \right)^{-1}, \quad w_q, t_q \in \reals.
	\label{eqn:contour_integral_quad}
\end{equation}
%
The geometry of the matrix root function affords a quadrature strategy that converges exponentially and only depends logarithmically on the conditioning of $\bA$.
In particular, if the quadrature locations $t_q$ and weights $w_q$ are chosen using Jacobi elliptic functions,\footnote{
  Confusing equation below! It's not important.
  \[
    w_q = \frac{ 2 \lambda_\text{min}^{-1/2} \kappa }{ \pi Q } \text{cn}\left( i\frac{(q - 0.5)\kappa}{Q} \right)  \text{dn} \left( i\frac{(q - 0.5)\kappa}{Q} \right), \quad t_q = \lambda_\text{min}^{-1/2} \text{sn} \left( i\frac{(q - 0.5)\kappa}{Q} \right).
  \]
}
then the convergence of this formula is quite fast, even for ill conditioned matrices.
%%
\citet{hale2008computing} demonstrate that, even for matrices with a condition number of $10^4$, choosing $Q \approx 20$ is sufficient to obtain accuracy up to full machine precision.

Importantly, we have reduced computing matrix square roots into a linear combination of matrix solves.
For example, the whitened predictive mean can be computed as
\begin{equation}
  \ameantest{\bxtest} = \bk_{\bZ\bxtest}^\top \bK_{\bZ\bZ}^{-1/2} \bm'
  \approx \sum_{q=1}^Q \: w_q \: \bk_{\bZ\bxtest}^\top \left( t_q^2 \bI - \bK_{\bZ\bZ} \right)^{-1} \bm'.
  \label{eqn:approx_pred_mean_ciq}
\end{equation}
%
Moreover, by directly differentiating through the quadrature, we can obtain a closed-form expression for derivatives involving $\bK_{\bZ\bZ}^{-1/2}$:
%
\begin{equation}
  \frac{\partial \left( \bk_{\bZ\bxtest}^\top \bK_{\bZ\bZ}^{-1/2} \bm' \right)} {\partial K_{\bZ\bZ}}
  \approx \sum_{q=1}^Q \: w_q \: \left( t_q^2 \bI - \bK_{\bZ\bZ} \right)^{-1} \bm' \: \bk_{\bZ\bxtest}^\top \left( t_q^2 \bI - \bK_{\bZ\bZ} \right)^{-1}.
  \label{eqn:approx_pred_mean_ciq_deriv}
\end{equation}
%
Analogous expressions for the whitened predictive variance (and its derivative) can be derived in a similar fashion.

\subsection{Efficiently Computing the Contour Integral Solves}
%
While \cref{eqn:contour_integral_quad} expressive the matrix root in an arguably simpler form, it naively has the same asymptotic complexity as the Cholesky decomposition.
Our goal is therefore to perform the matrix solves in $< \bigo{M^3}$ time with the option of $< \bigo{M^2}$ storage as well.

\paragraph{Option 1: Krylov Subspace Methods}

We can take inspiration from the matrix multiplication-based inference of [Gardner et al., 2018, Wang et al., 2019].
More specifically, we aim to compute all the solves $(t_q^2 \bI - \bK_{\bZ\bZ})^{-1} \bk_{\bZ \bxtest}$ only through matrix multiplication with $\bK_{\bZ\bZ}$.
We will do so using a modified version of the method of minimum residuals (MINRES) which enables us to compute each of the $Q$ solves efficiently and simultaneously.

MINRES \gp{CITE} is an iterative approach to solving $\bA^{-1} \bb$ that is similar to the method of conjugate gradients.
At each iteration $j$, MINRES produces the best approximate solve $\bc_j \approx \bA^{-1} \bb$ within the \emph{Krylov subspace} of $\bA$ and $\bb$:
%
\[ \bc_j = \min_{\bc \in \mathcal{K}_j (\bA, \bb)} \Vert \bA \bc - \bb \Vert_2^2, \]
%
where $\mathcal{K}_j (\bA, \bb)$ is the $j^\text{th}$ Krylov subspace of $\bA$ and $\bb$:
%
\begin{equation}
  \mathcal{K}_j (\bA, \bb) \triangleq
  \text{span} \left\{
    \bb, \:\: \bA \bb, \:\: \bA^2 \bb, \:\: \ldots, \:\: \bA^{j-1} \bb
  \right\}.
\end{equation}
%
(Note that each vector in the subspace can be formed simply by multiplying its preceding vector by $\bA$.)
From this high-level view, we can begin to see how such an iterative method could be constructed through a dynamic programming algorithm.
At each iteration $j+1$, we perform the matrix-vector multiplication $\bA \left( \bA^{j-1} \bb \right)$ to form the next Krylov subspace:
%
\[ \mathcal{K}_{j+1} (\bA, \bb) = \text{span} \left\{ \mathcal{K}_{j} (\bA, \bb) \:\: \cup \:\: \{ \bA \left( \bA^{j-1} \bb \right) \} \right\}.  \]
%
In practice, we can produce each $\bc_j$ through a three-vector recurrence relationship with only a single matrix-vector multiplication at each time step.

Rather than running MINRES separately for each quadrature point, it turns out we can create a modified MINRES that computes each $( t_q^2 \bI - \bK_{\bK\bK})^{-1} \bk_{\bZ \bxtest} )$ simultaneously.
The key insight is that the $(t_q^2 \bI - \bK_{\bK\bK})$, $\bk_{\bZ \bxtest}$ pairs each share the same Krylov subspace:
\[
  \mathcal{K}_j (\bK_{\bZ\bZ}, \bk_{\bZ\bxtest}) =
  \mathcal{K}_j ((t_q^2 \bI - \bK_{\bZ\bZ}), \bk_{\bZ\bxtest}) \quad \forall t_q \in \reals.
\]
From this, we can form an extremely efficient modified MINRES that solves all systems simultaneously.
Each iteration requires only a single matrix-vector multiplication with $\bK_{\bZ\bZ}$, and the whole algorithm only requires storing $3Q$ vectors of size $M$.
If we run $J$ iterations of the modified MINRES, the total computation time of $\bK_{\bZ\bZ}^{-1/2} \bk_{\bZ\bxtest}$ is $\bigo{J M^2}$.

\paragraph{Option 2: Partial Pivoted Cholesky}

Taking even more inspiration from Gardner et al., 2018\footnote{
  Such smart people\ldots
}, we can alternatively compute the partial pivoted Cholesky decomposition of $\bK_{\bZ\bZ}$:
%
\begin{equation}
  \bK_{\bZ\bZ} \approx \bP \bP^\top, \quad \bP \in \reals^{M \times J}, \quad J \ll M.
  \label{eqn:partial_pc}
\end{equation}
%
This is a low-rank approximation to $\bK_{\bZ\bZ}$.
We can then use the Woodbury matrix identity to compute $( t_q^2 \bI - \bP \bP )^{-1}$ in $\bigo{ J^3 + J M^2 }$ time.
