\section{Discussion}

This chapter extends the MVM-based methods from previous chapters to non-regression tasks (via SVGP) and optimization problems (via BayesOpt with Thompson sampling).
We have introduced Cauchy Integral Quadrature (CIQ)---a MVM-based method for computing $\trainK^{1/2} \bb$ and $\trainK^{-1/2} \bb$.
In sampling and whitening applications, CIQ can be used as a $\bigo{N^2}$ drop-in replacement for the $\bigo{N^3}$ Cholesky decomposition.
Its scalability and GPU utilization enable us to use more inducing points with SVGP models and larger candidate sets with BayesOpt acquisition functions.
In both applications, this increased fidelity results in better performance.


\paragraph{Advantages and disadvantages.}
One advantage of the Cholesky decomposition is its reusability.
While the initial cost of computing $\bL \bL^\top = \bK$ is cubic, applying $\bL$ to a vector (either for sampling and whitening) is only quadratic.
Therefore, its cubic complexity is well amortized if we wish to draw $\bigo{M}$ samples or whiten $\bigo{M}$ vectors.
Whitening or sampling $\bigo{M}$ vectors with CIQ would incur a $\bigo{M^3}$ cost, eroding its computational benefits.

There are two conditions in which CIQ is clearly advantageous:
%
\begin{enumerate*}
  \item when we wish to apply $\bK^{1/2}$ or $\bK^{-1/2}$ to a limited number of vectors; and
  \item when $\bK$ is otherwise too large to apply Cholesky.
\end{enumerate*}
%
The first condition is met by both the SVGP and BayesOpt applications.
During SVGP training the $\bK_{\bZ\bZ}$ changes during every optimization iteration.
Similarly, during BayesOpt the posterior covariance changes after each Thompson sampling iteration.


\paragraph{Using CIQ with structured kernel matrices.}
Throughout this chapter we only apply CIQ to dense matrices that do not afford $o(N^2)$ MVMs.
Our results demonstrate that CIQ offers a substantial computational benefit in this setting.
We suggest that future work should explore applications of the CIQ algorithm that involve sparse or structure matrices.
A key advantage of MVM methods is their ability to exploit efficient MVM routines in these settings.
We restrict our experiments to non-structured kernel matrices to be most generally compatible with SVGP and Bayesian optimization.
However, it is reasonable to expect that CIQ becomes even more computational advantages when, for example, used in conjunction with KISS-GP models.


\paragraph{Scaling beyond $M=10,\!000$ and $T=50,\!000$.}
Historically, it has been common to use only $M\approx1,\!000$ inducing points with SVGP models.
In this chapter, we have used \emph{an order of magnitude} more inducing points which results in demonstrably better predictive performance.
As $M$ continues to grow, the primary bottleneck of CIQ-SVGP becomes the quadratic memory costs of the variational parameters $\bm'$ and $\bS'$.
While a scalable approximation of $\bK_{\bZ\bZ}$ (with fast MVMs) can reduce the computational cost of whitening, the $\bS'$ matrix in general does not afford space-saving structure.
Efficient variational parameter repressions will be necessary to scale to even larger $M$.
This has been the topic of some recent work \cite{wilson2016stochastic,cheng2017variational,salimbeni2018orthogonally,shi2019sparse}, though such structure can sometimes result in poor conditioning and worse optimization.

Scaling Thompson sampling beyond $T=50,\!000$ is to some extent more straightforward, as it does not require storing learnable parameters.
%One option is to use Thompson sampling in conjunction with approximate GP methods that afford faster MVMs.
%However, choosing a suitable approximate method may be difficult when modelling blackbox functions without knowing much prior structure.
Nevertheless, scaling $T$ will naturally require more computation, which may result in the acquisition function becoming increasingly computationally demanding.
The next section introcues a simple strategy to utilize multiple GPUs/distributed resources to alleviate this bottleneck.
