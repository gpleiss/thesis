\section{Introduction}

The BBMM and LOVE algorithms address training and predictions with GP regression models.
However, other GP tasks---such as black-box optimization and non-conjugate inference (e.g. classification)---require two additional kernel matrix operations.
\begin{itemize}
  \item {\bf Sampling} from GP posteriors---as described in \cref{sec:sampling_method}---is a common inference operation especially in the context of Bayesian optimization \cite{thompson1933likelihood,frazier2009knowledge,wang2017max}.
  Since we cannot directly sample a function $f(\cdot) \sim p(f(\cdot) \mid \dset)$, it is more common to draw samples from  $f(\cdot)$ evaluated on a finite test set $\bXtest = [\bxtest_1, \ldots, \bxtest_T]$
  If $\normaldist{\bmeantest}{\Covtest}$ is the GP posterior evaluated on $\bXtest$, then we can draw samples $\bepsilon'$ via the reparameterization trick \cite{kingma2014auto,rezende2014stochastic}:
  %
  \begin{equation}
    \bepsilon = \bmeantest + \left( {\Covtest}^{\frac 1 2} \right) \bepsilon'.
    \label{eqn:sampling}
  \end{equation}
  %
  where $\bepsilon' \sim \normaldist{\bzero}{\bI}$ is a standard normal vector.

  \item {\bf ``Whitening''} is essentially the inverse of the sampling operation---and it is often necessary for non-conjugate GP approximations.
  If $\bb$ is a random variable with mean $\bmu$ and covariance $\bK$ (e.g. a sample from a GP prior), then the whitening operation gives us a coordinate transformation
  %
  \begin{equation}
    \bb' = \bK^{- \frac 1 2} \left( \bb - \bmu \right)
    \label{eqn:whitening}
  \end{equation}
  %
  so that the resultant vector $\bb'$ has zero mean and unit covariance.
  As we will discuss in \cref{sec:variational_results}, the whitening operation can significantly accelerate the convergence of variational Gaussian process inference \cite{kuss2005assessing,hensman2013gaussian,matthews2017scalable}
\end{itemize}
%
Note that \cref{eqn:sampling,eqn:whitening} both require applying the matrix square root (or its inverse) to a vector: $\bK^{1/2} \bb$.
In practice, $\bK^{1/2}$ can be replaced with any matrix $\bR$ such that $\bR \bR^\top = \bK$.
All such $\bR$ matrices are equivalent to $\bK^{1/2}$ up to an orthonormal rotation \cite{golub2012matrix}.

Existing methods for sampling and whitening typically rely on the Cholesky factorization: $\bK = \bL \bL^\top$, where $\bL$ is lower triangular.\footnote{
  As stated above, $\bL$ is equivalent to $\bK^{1/2}$ up to an orthonormal rotation.
}
$\bL\bepsilon$, $\bepsilon \sim \normaldist{\bzero}{\bI}$ can be used to draw samples from from $\normaldist{\bzero}{\bK}$ and $\bL^{-1} \bb$ can be used to ``whiten'' the vector $\bb$.
However, the Cholesky factor requires $\bigo{N^3}$ computation and $\bigo{N^2}$ memory for an $N \times N$ covariance matrix $\bK$.
Consequentially, sampling and whitening are typically accomplished with randomized algorithms \citep[e.g.][]{rahimi2008random}, low-rank/sparse approximations \citep[e.g.][]{wilson2020efficiently}, or alternative distributions \citep[e.g.][]{wang2017max}.
While the previous chapter offers an efficient sampling method for KISS-GP models; this method is not generally applicable to other classes of GP models (see \cref{sec:sampling_method}).

In this chapter, we make several contributions to address these issues:
%
\begin{itemize}
  \item We introduce a MVM approach for computing $\bK^{-1 / 2} \bb$ that is accurate to several decimal points.
    Our approach, {\bf Cauchy Integral Quadrature (CIQ)}, uses an insight from \citet{hale2008computing} that expresses the matrix square root as a sum of shifted matrix inverses.

  \item To efficiently compute these shifted inverses, we introduce a modified version of the MINRES algorithm \cite{paige1975solution} that performs \emph{multiple shifted solves} through a single iteration of MVMs.
    This {\bf multi-shift MINRES (msMINRES)} can be preconditioned for fast convergence.
    Achieving 4 decimal places of accuracy typically requires \emph{fewer than 100 MVMs}, which can be highly accelerated through GPUs.

  \item We derive a scalable backward pass for $\bK^{-1/2} \bb$ that enables our approach to be used as part of learning and optimization.

  \item We apply our $\bK^{-1/2} \bb$ and $\bK^{1/2} \bb$ routines to two applications:
    \begin{enumerate*}
      \item variational Gaussian processes with up to $M = 10,\!000$ inducing points (where we additionally introduce a $\bigo{M^2}$ MVM-based natural gradient update); and
      \item sampling from Gaussian process posteriors in the context of Bayesian optimization with up to $50,\!000$ candidate points.
      %\item an image super-resultion Gibbs sampling problem in $25,\!000$ dimensions.
    \end{enumerate*}
\end{itemize}
