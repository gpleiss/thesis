\section{Introduction}

High dimensional multivariate Gaussian distributions arise frequently in machine learning, especially in the context of Bayesian non-parametric models.
For example, Gaussian process regression assume that the entries in a dataset $(\bx^{(1)}, y^{(1)})$, $\ldots$, $(\bx^{(N)}, y^{(N)})$ are jointly Gaussian a priori and a posteriori.
Inferences require operations on the prior distribution $p([y^{(1)}, \ldots, y^{(N)}]) = \normaldist{\bzero}{\bK \in \reals^{N \times N}}$,
which typically results in expensive non-linear manipulations with the $N \times N$ covariance matrix $\bK$.

Historically, $\bigo{N^3}$ operations on covariance matrices and $\bigo{N^2}$ memory requirements have limited the tractability of large-scale multivariate Gaussians.
Gaussian process regression for example has typically been limited to a few thousand data points before approximations are necessary \cite{hensman2013gaussian}.
A growing line of research is focused using GPU-accelerated memory-efficient covariance operations \gp{CITE} to make large-scale problems possible.
Covariance matrix operations, such as linear solves \gp{CITE} and log determinants \gp{CITE}, can be reframed as iterative optimizations involving \emph{matrix-vector multiplication} (MVMs).
MVM approaches has two primary advantages:
\begin{enumerate*}
  \item the covariance matrix does not have explicitly instantiated (only $\bigo{N}$ memory is required) \cite{wang2019exact,charlier2020kernel}, and
  \item the MVM operations utilize GPU acceleration better direct methods like Cholesky \cite{gardner2018gpytorch}.
\end{enumerate*}

In this paper, we use MVM methods to address a common computational bottleneck for high-dimensional Gaussian distributions: computing the products $\bK^{\frac 1 2} \bb$ and $\bK^{-\frac 1 2} \bb$ with covariance matrix $\bK$.
This operation occurs frequently in Gaussian process methods and inverse problems \gp{CITE}.
For example, if $\bb$ is a standard Normal vector, then $\bK^{\frac 1 2} \bb$ is a method of generating samples from $\normaldist{\bzero}{\bK}$ via the reparameterization trick \cite{kingma2013auto}.
This operations appears frequently in Bayesian optimization \gp{CITE} and Gibbs sampling \gp{CITE}.
$\bK^{-\frac 1 2} \bb$ can also used to project data and parameters into a ``whitened'' coordinate space \gp{CITE}---an transformation that accelerates the convergence of variational Gaussian process approximations \cite{hensman2013gaussian,matthews2017scalable}.
Despite its ubiquity and usefulness, the matrix square root is typically avoided for matrices larger than $N=5,\!000$, and (potentially loose) approximations are typically used instead.
Typically, the Cholesky factorization is used to compute $\bK^{-1/2} \bb$ up to an orthonormal rotation, yet this operation scales cubically with $N$ and requires quadratic memory.
To address these limitations, we make the following contributions:
%
\begin{itemize}
  \item We introduce a matrix-vector multiplication approach to $\bK^{-\frac 1 2} \bb$ that is accurate to several decimal points with \emph{quadratic} computational requirements.
    Our approach uses an insight from \citet{hale2008computing} that expresses the matrix square root as a \emph{sum of shifted matrix inverses}.

  \item To efficiently compute these shifted inverses, we introduce a modified version of the minimum residual algorithm (MINRES) \cite{paige1975solution} which performs \emph{multiple shifted solves} through a single iteration of MVMs.
    This {\bf multi-shift MINRES (msMINRES)} can be preconditioned for fast convergence, and only requires $\bigo{N}$ storage when used in conjunction with partitioned MVMs \cite{wang2019exact,charlier2020kernel}.
    Achieving 4 or 5 decimal places of accuracy typically requires \emph{fewer than 100 matrix-vector multiplications}, which can be highly accelerated through GPUs.

  \item We derive a scalable backward pass for $\bK^{-1/2} \bb$ that enables our approach to be used as part of learning and optimization.

  \item We apply our $\bK^{-1/2} \bb$ routine to variational Gaussian process models with $M = 10,\!000$ inducing points.
    To overcome the optimization difficulties associated with many inducing points, we develop a MVM-based natural gradient update that only requires $\bigo{M^2}$ computation.
    Our approach enables models with $2-3\times$ as many inducing points as standard approaches under a fixed computational budget, resulting in more accurate models.

  \item Finally, we use our $\bK^{1/2} \bb$ routine for sampling in the context of Bayesian optimization and inverse problems.
    Used in conjunction with Thompson sampling \cite{thompson1933likelihood}, our method densely sample a search space with up to $50,\!000$ candidate points, resulting in better global optimization.
		We additionally demonstrate sampling in the context of $50,\!000$-dimensional inverse problems like unsupervised image super-resolution.
\end{itemize}
