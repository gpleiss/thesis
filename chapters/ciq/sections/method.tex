\section{Contour Integral Quadrature (CIQ) via Matrix-Vector Multiplication}
\label{sec:ciq_method}

In this section we develop an MVM method to compute $\bK^{-1/2} \bb$ and $\bK^{1/2} \bb$ for sampling and whitening.
%Our approach uses to a \emph{rational approximation} of $\bK^{-1/2}$ in conjunction with \emph{matrix-vector multiplication}-based solvers.
Our approach scales better than existing methods (e.g.~Cholesky) by:
\begin{enumerate*}
  \item reducing computation from $\bigo{N^3}$ to $\bigo{N^2}$;
  \item more effectively using GPU acceleration; and
  \item affording an efficient gradient computation.
\end{enumerate*}

\paragraph{Contour Integral Quadrature (CIQ).}
A standard result from complex analysis is that $\bK^{-1/2}$ can be expressed through Cauchy's integral formula:
%
$$
	\bK^{-1 / 2} = \frac{1}{2 \pi i} \oint_\Gamma \tau^{- 1 / 2} \left( \tau \bI - \bK \right)^{-1} \intd \tau,
  %\label{eqn:contour_integral}
$$
%
where $\Gamma$ is a closed contour in the complex plane that winds once around the spectrum of $\bK^{-\frac 1 2}$ \citep{davies2005computing,hale2008computing,higham2008functions}.
Applying a numerical quadrature scheme to the contour integral yields the rational approximations
%
\begin{equation}
	\bK^{-\frac 1 2} \approx \sum_{q=1}^Q w_q \left( t_q \bI + \bK \right)^{-1}
  \quad \text{and} \quad
	\bK^{\frac 1 2} \approx \bK \sum_{q=1}^Q w_q \left( t_q \bI + \bK \right)^{-1},
	\label{eqn:contour_integral_quad}
\end{equation}
%
where the weights $w_q$ encapsulate the normalizing constant, quadrature weights, and the $t_q^{-\frac 1 2}$ terms.
%(Note that we can compute $\bK^{1/2}$ by multiplying a quadrature estimate of $\bK^{-1/2}$ by $\bK$.)
\citet{hale2008computing} introduce a real-valued quadrature strategy based on a change-of-variables formulation (described in \cref{app:quadrature})
that converges extremely rapidly---often achieving full machine precision with only $Q \approx 20$ quadrature points.
For the remainder of this chapter, applying~\cref{eqn:contour_integral_quad} to compute $\bK^{\pm1/2} \bb$  will be referred to as {\bf Contour Integral Quadrature (CIQ)}.



\subsection{An Efficient Matrix-Vector Multiplication Approach to CIQ with msMINRES.}
%
Using the quadrature method of \cref{eqn:contour_integral_quad} for whitening and sampling requires solving several shifted linear systems.
To compute the shifted solves required by~\cref{eqn:contour_integral_quad} we leverage a variant of the
minimum residuals algorithm (MINRES) developed by \citet{paige1975solution}. At step $j$
MINRES approximates $\bK^{-1} \bb$ by the vector within the Krylov subspace $\mathcal{K}_j(\bK, \bb)$ that minimizes the residual.

\paragraph{msMINRES for multiple shifted solves.}
To efficiently compute all the shifted solves, we leverage techniques~\citep[e.g.][]{datta1991arnoldi,freund1990conjugate,meerbergen2003solution} that exploit the shift-invariance property of Krylov subspaces: i.e.~$\mathcal{K}_J(\bK, \bb) = \mathcal{K}_J(t \bI +  \bK, \bb)$.
We introduce a variant to MINRES, which we refer to as {\bf multi-shift MINRES} or {\bf msMINRES}, that re-uses the same Krylov subspace vectors $[\bb, \: \bK \bb, \: \ldots, \: \bK^{J-1} \bb]$ for all shifted solves $(t \bI + \bK)^{-1} \bb$.
In other words, using msMINRES we can get all $(t_q \bI + \bK)^{-1} \bb$ \emph{essentially for free}, i.e. only requiring $J$ MVMs for the Krylov subspace $\mathcal{K}_J(\bK, \bb)$.
As with standard MINRES, the msMINRES procedure for computing $(t_q \bI + \bK)^{-1}$ from $[\bb, \: \bK \bb, \: \ldots, \: \bK^{J-1} \bb]$ can be reduced to a simple vector recurrence.

\paragraph{msMINRES.}
First, recall from \cref{sec:minres} that the MINRES solution for $\bK^{-1} \bb$ can be formed through Lanczos (see \cref{eqn:minres_qr}):
\begin{equation}
  \bK^{-1} \bb \approx \Vert \bb \Vert_2 \: \bQ_J \left( \bR^{-1}_J \bQU_J^\top \right) \be_1,
  \quad
  \bQU_J \bR_J = \begin{bmatrix} \bT_J \\ \Vert \br_J \Vert_2 \be_J^\top \end{bmatrix},
  \label{eqn:minres_qr2}
\end{equation}
where $\bQ_J$, $\bT_J$, and $\br_J$ are the outputs from $J$ steps of the Lanczos algorithm with initial vector $\bb$:
\[
  \bK \bQ_J = \bQ_J \bT_J + \br_J \be_J^\top.
\]
To adapt MINRES to multiple shifts (i.e. msMINRES), we exploit a well-established fact about the shift invariance of Krylov subspaces (see \citep[e.g.][]{datta1991arnoldi,freund1990conjugate,jegerlehner1996krylov,saad2003iterative}).
%
\begin{observation}
  Let $\bK \bQ_J = \bQ_J \bT_J + \br_J \be_J^\top$ be the Lanczos factorization for $\bK$ given the initial vector $\bb$.
  Then $$(\bK + t \bI) \bQ_J = \bQ_J (\bT_J + t \bI) + \br_J \be_J^\top$$ is the Lanczos factorization for matrix $(\bK + t \bI)$ with initial vector $\bb$.
\end{observation}
%
\noindent
%In other words, if we run Lanczos on $\bK$ and $\bb$, then we get the Lanczos factorization of $(\bK + t \bI)$ \emph{for free}, without any additional MVMs!
Consequently, we can re-use the $\bQ_J$ and $\bT_J$ Lanczos matrices to compute \emph{multiple shifted solves.}
%
\begin{equation}
  (\bK + t \bI)^{-1} \bb \approx \Vert \bb \Vert_2 \: \bQ_J \left( \bR^{(t){-1}}_J \bQU_J^{(t)\top} \right) \be_1,
  \quad
  \bQU_J^{(t)} \bR_J^{(t)} = \begin{bmatrix} \bT_J + t \bI \\ \Vert \br_J \Vert_2 \be_J^\top \end{bmatrix},
  \label{eqn:minres_qr_shifted}
\end{equation}
%
Assuming $\bQ$ and $\bT$ have been previously computed, \cref{eqn:minres_qr_shifted} requires no additional MVMs with $\bK$.
We refer to this multi-shift formulation as {\bf Multi-Shift MINRES}, or {\bf msMINRES}.

\input{algorithms/msminres}

\paragraph{A simple vector recurrence for msMINRES.}
Just as with standard MINRES, \cref{eqn:minres_qr_shifted} can also be computed via a vector recurrence.
We can derive a msMINRES algorithm simply by modifying the existing MINRES recurrence.
Before the QR step in \cref{alg:minres}, we add $t$ to the Lanczos diagonal terms ($\gamma_j + t$, where $\gamma_j = T^{(j,j)}$).
This can be extended to simultaneously handle \emph{multiple shifts} $t_1, \ldots, t_Q$.
Each shift would compute its own QR factorization, its own step size $\varphi_j^{(t_q)}$, and its own search vector $\bd_j^{(t_q)}$.
However, all shifts share the same Lanczos vectors $\bq_j$ and therefore share the same MVMs.
The operations for each shift can be vectorized for efficient parallelization.

To summarize: the resulting algorithm---msMINRES---gives us approximations to $(t_1 \bI + \bK)^{-1} \bb$, $\ldots$, $(t_Q \bI + \bK)^{-1}$ \emph{essentially for free} by leveraging the information we needed anyway to compute $\bK^{-1} \bb$.
Its computational properties will be highlighted below.


\input algorithms/ciq

\subsection{Computational Complexity and Convergence Analysis of msMINRES-CIQ}
Pairing \cref{eqn:contour_integral_quad} with msMINRES is an efficient algorithm for computing $\bK^{1/2} \bb$ and $\bK^{-1/2} \bb$.
\cref{alg:ciq} summarizes this approach; below we highlight its computational properties:

\label{sec:convergence}

\begin{property}[Computation/Memory of msMINRES-CIQ]
  $J$ iterations of msMINRES requires exactly $J$ MVMs with the input matrix $\bK$,
  regardless of the number of quadrature points $Q$.
  The resulting runtime of msMINRES-CIQ is $\bigo{ J \mvm{\bK}}$, where $\mvm{\bK}$ is the time to perform an MVM with $\bK$.
  The memory requirement is $\bigo{ QN }$ in addition to what is required to store $\bK$.
  \label{prop:msminres}
\end{property}
%
\noindent
For arbitrary positive semi-definite $N \! \times \! N$ matrices, the runtime of msMINRES-CIQ is $\bigo{J N^2}$, where often $J \ll N$.
%Performing the MVMs in a map-reduce fashion \cite{wang2019exact,charlier2020kernel} avoids explicitly forming $\bK$, which results in $\bigo{QN}$ total memory.
Below we bound its error:
%
\begin{theorem}
  Let $\bK \succ 0$ and $\bb$ be inputs to msMINRES-CIQ, producing $\ba_J \approx \bK^{1/2} \bb$ after $J$ iterations with $Q$ quadrature points.
  The difference between $\ba_J$ and $\bK^{1/2} \bb$ is bounded by:
  %
  \begin{align*}
    \left\Vert \ba_J - \bK^{\frac 1 2} \bb \right\Vert_2
    &\leq
    \overbracket{
      \bigo{\exp\left( -\frac  {2 Q \pi^2}{\log \kappa(\bK) + 3} \right)}
    }^{\text{Quadrature error}}
    \\
    &\phantom{=} \:\: +
    \overbracket{
      \frac{ 2 Q \log \left( 5 \sqrt{\kappa(\bK)} \right) \kappa(\bK) \sqrt{\lambda_\text{min}} }{\pi}
      \left( \frac{ \sqrt{\kappa(\bK)} - 1}{ \sqrt{\kappa(\bK)} + 1} \right)^{J-1}
      \left\Vert \bb \right\Vert_2.
    }^{\text{msMINRES error}}
  \end{align*}
  %
  where $\lambda_\text{max},\lambda_{\text{min}}$ are the max and min eigenvalues of $\bK$, and $\kappa(\bK) \equiv \tfrac{\lambda_\text{max}}{\lambda_\text{min}}$ is the condition number.
  \label{thm:ciq_convergence}
\end{theorem}
%
\noindent
For $\ba'_J \approx \bK^{-1/2} \bb$, the bound incurs an additional factor of $1/\lambda_\text{min}$.
(See \cref{app:ciq_proofs} for proofs.)
\cref{thm:ciq_convergence} suggests that error in computing $(t_q \bI + \bK)^{-1}\bb$  will be the primary source of error as the quadrature error decays rapidly with $Q$. In many of our applications the rapid convergence of Krylov subspace methods for linear solves is well established, allowing for accurate solutions if desired.
For covariance matrices up to $N=50,\!000$, often $Q=8$ and $J\leq100$ suffices for 4 decimal places of accuracy and $J$ can be further reduced with preconditioning (see \cref{sec:ciq_precond}).



\subsection{Efficient Vector-Jacobi Products for Backpropagation}

In certain applications, such as variational Gaussian process inference, we have to compute gradients of the $\bK^{-1/2} \bb$ operation.
This requires the vector-Jacobian product $\bv^\top ( \partial \bK^{-1/2} \bb / \partial \bK)$, where $\bv$ is the back-propagated gradient.
The form of the Jacobian is the solution to a Lyapunov equation, which requires expensive iterative methods or solving a $N^2 \times N^2$ Kronecker sum $(\bK^{1/2} \oplus \bK^{1/2})^{-1}$.
Both of these options are much slower than the forward pass and are impractical for large $N$.
%We note that all $\bK^{-1/2} \bb$ methods share this same gradient bottleneck.
%
Fortunately, our quadrature formulation affords a computationally efficient approximation to this vector-Jacobian product.
If we back-propagate directly through each term in \cref{eqn:contour_integral_quad}, we have
%
\begin{align}
  \bv^\top \left( \frac{ \partial \bK^{-1/2} \bb }{ \partial \bK } \right)
  &= \frac{ \partial \bv^\top \bK^{-1/2} \bb }{ \partial \bK }
  \nonumber
  \\
  &\approx \frac{ \partial \sum_{q=1}^{Q} w_q \left( \bv^\top (t_q \bI + \bK)^{-1} \bb \right) }{ \partial \bK } =
  \nonumber
  \\
  &\approx
  - \frac{1}{2} \sum_{q=1}^{Q} w_q \left( t_q \bI + \bK \right)^{-1} \left( \bv \bb^\top + \bb \bv^\top \right) \left( t_q \bI + \bK \right)^{-1}.
  \label{eqn:ciq_deriv}
\end{align}
%
Since the forward pass computes the solves with $\bb$, the only additional work needed for the backward pass is computing the shifted solves $(t_q \bI + \bK)^{-1} \bv$, which can be computed with another call to the msMINRES algorithm.
Thus the backward pass takes only $\bigo{J \mvm{\bK}}$ (e.g.~$\bigo{J N^2}$) time.




\paragraph{Programmability.}
As with BBMM and LOVE, CIQ can take full advantage of GPyTorch's {\tt LazyTensor} construct.
The algorithm for CIQ (\cref{alg:ciq}) accesses $\bK$ through Lanczos and msMINRES---each of which only requires black-box access to an MVM routine.
Therefore, CIQ can be implemented for specialty GP models using the efficient {\tt \_matmul} function of {\tt LazyTensor}s.
Moreover, back-propagation via \cref{eqn:ciq_deriv} can be implemented using the {\tt \_deriv} function of LazyTensors.
Recall that the {\tt \_deriv($ \bA, \bB $)} function computes $\partial \tr{ \bA^\top \bK \bB }/{\partial \br}$, where $\br$ is a representation of $\bK$.
If we define $\bA$ and $\bB$ as
%
\begin{align*}
  \bA &= \begin{bmatrix}
    w_1 \left( t_q1\bI - \bK \right)^{-1} \bv
    & \ldots &
    w_Q \left( t_Q \bI - \bK \right)^{-1} \bv
  \end{bmatrix},
  \\
  \bB &= \begin{bmatrix}
    \left( t_q1\bI - \bK \right)^{-1} \bb
    & \ldots &
    \left( t_Q \bI - \bK \right)^{-1} \bb
  \end{bmatrix},
\end{align*}
%
then the vector-Jacobi product in \cref{eqn:ciq_deriv} can be re-written as $\tr{ \bA \bB^\top } = \texttt{ \_deriv($\bA, \bB$)}$.
This allows us to easily implement efficient CIQ variants for structured matrices.


\subsection{Preconditioning}
\label{sec:ciq_precond}

To improve the convergence of \cref{thm:ciq_convergence}, we can introduce a preconditioner $\bP$ where $\bP^{-1} \bK \approx \bI$.
For standard MINRES, applying a preconditioner is straightforward.
We simply use MINRES to solve the system
\[
  \left( \bP^{-1/2} \bK \bP^{-1/2} \right) \bP^{1/2} \bc = \bP^{-1/2} \bb,
\]
which has the same solution $\bc$ as the original system.
In practice the preconditioned MINRES vector recurrence does not need access to $\bP^{-1/2}$---it only needs access to $\bP^{-1}$
(see \citep[][Ch. 3.4]{choi2006iterative} for details).

However, it is not immediately straightforward to apply preconditioning to msMINRES, as preconditioners break the shift-invariance property that is necessary for the $\bigo{JN^2}$ shifted solves \cite{jegerlehner1996krylov,aune2013iterative}.
More specifically, if we apply $\bP$ to msMINRES, then we obtain the solves
%
\[
	\bP^{-1 / 2} (\bP^{-1 / 2} \bK \bP^{-1 / 2} + t_q \bI)^{-1} (\bP^{-1 / 2} \bb).
\]
%
Plugging these shifted solves into the quadrature equation \cref{eqn:contour_integral_quad} therefore gives us
%
\begin{equation}
	\widetilde \ba_J \approx \bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} (\bP^{-\frac 1 2} \bb).
  \label{eqn:not_a_sqrt}
\end{equation}
%
In general, we cannot recover $\bK^{-1/2}$ from \cref{eqn:not_a_sqrt}.
Nevertheless, we can still obtain preconditioned solutions that are equivalent to $\bK^{-1/2} \bb$ and $\bK^{1/2} \bb$ up to an orthogonal rotation.
Let $\bR = \bK \bP^{-1/2} (\bP^{-1/2} \bK \bP^{-1/2})^{-1/2}$.
We have that
%
\begin{align*}
  \bR \bR^\top
	=
	\bK \left( \bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} \right)
	\left( (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} \bP^{-\frac 1 2} \right) \bK
  = \bK.
\end{align*}
%
Thus $ \bR $ is equivalent to $\bK^{1/2}$ up to orthogonal rotation.
We can compute $\bR \bb$ (e.g. for sampling) by applying \cref{eqn:not_a_sqrt} to the initial vector $\bP^{1/2} \bb$:
%
\begin{align}
  \bR \bb
  =
  \bK
  \underbracket{%
    \Bigl[ \bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} \bP^{-\frac 1 2} \Bigr]
    \left( \bP^{\frac 1 2} \bb \right)
  }_{\text{Applying preconditioned msMINRES to $\bP^{1/2} \bb$}}.
  \label{eqn:precond_sqrt}
\end{align}
%
Similarly, $\bR' = \bP^{-1/2} \left( \bP^{-1/2} \bK \bP^{-1/2} \right)^{-1/2}$ is equivalent to $\bK^{-1/2}$ up to orthogonal rotation:
\begin{align*}
  \bR' \bR'^\top
	=
	\left( \bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} \right)
	\left( (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} \bP^{-\frac 1 2} \right)
  = \bK^{-1}.
\end{align*}
We can compute $\bR' \bb$ (e.g. for whitening) via:
%
\begin{align}
  \bR' \bb
  =
  \underbracket{%
    \Bigl[ \bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} \bP^{-\frac 1 2} \Bigr]
    \left( \bP^{\frac 1 2} \bb \right)
  }_{\text{Applying preconditioned msMINRES to $\bP^{1/2} \bb$}}.
  \label{eqn:precond_sqrt_inverse}
\end{align}
%
Crucially, the convergence of \cref{eqn:precond_sqrt,eqn:precond_sqrt_inverse} depends on the conditioning $\kappa(\bP^{-1} \bK) \ll \kappa(\bK)$.

As with standard MINRES, msMINRES only requires access to $\bP^{-1}$, not $\bP^{-1/2}$.
Note however that \cref{eqn:precond_sqrt,eqn:precond_sqrt_inverse} both require multiplies with $\bP^{1/2}$.
If a preconditioner $\bP$ does not readily decompose into $\bP^{1/2} \bP^{1/2}$, we can simply run the CIQ algorithm on $\bP$ to compute $\bP^{1/2} \bb$.
Thus our requirements for a preconditioner are:
\begin{enumerate*}
	\item it affords efficient solves (ideally $o(N^2)$), and
  \item it affords efficient MVMs (also ideally $o(N^2)$) for computing $\bP^{1/2} \bb$ via CIQ.
\end{enumerate*}
%
Note that these requirements are met by the partial pivoted Cholesky preconditioner proposed in \cref{sec:preconditioning}.

%One possible preconditioner is the partial pivoted Cholesky preconditioner proposed by \citet{gardner2018gpytorch} (see \cref{app:preconditioner}).
%The form of $\bP$ is $\bR \bR^\top + \sigma^2 \bI$, where $\bR$ is a low-rank factor and $\sigma^2 \bI$ is a small diagonal component.
%This preconditioner affords $\approx \bigo{N}$ solves using the matrix inversion lemma and $\bigo{N}$ samples using the reparameterization trick \cite{kingma2013auto,rezende2014stochastic}.\footnote{
	%Let $\bepsilon_1'$ and $\bepsilon_2'$ be standard Normal random variables.
	%Then $\bR \bepsilon_1' + \sigma \bepsilon_2'$ is a sample from $\normaldist{\bzero}{\bP}$.
%}
%Moreover, this preconditioner is highly effective on many Gaussian covariance matrices \cite{gardner2018gpytorch,wang2019exact}.





\subsection{Related Work}
Other Krylov methods for $\bK^{1/2} \bb$ and $\bK^{-1/2} \bb,$ often via polynomial approximations~\citep[e.g.][]{higham2008functions}, have been explored.
\citet{chow2014preconditioned} compute $\bK^{1/2} \bb$ via a preconditioned Lanczos algorithm.
Unlike msMINRES, however, they require storage of the entire Krylov subspace. Moreover this approach does not afford a simple gradient computation.
More similar to our work is \cite{aune2013iterative,aune2014parameter}, which uses the quadrature formulation of \cref{eqn:contour_integral_quad} in conjunction with a shifted conjugate gradients solver.
We expand upon their method by:
\begin{enumerate*}
  \item introducing a simple gradient computation,
  \item proving a convergence guarantee, and
  \item enabling the use of simple preconditioners.
\end{enumerate*}
