\section{Cauchy Integral Quadrature (CIQ) via Matrix-Vector Multiplication}
\label{sec:ciq_method}

In this section we develop an MVM method to compute $\bK^{-1/2} \bb$ and $\bK^{1/2} \bb$ for sampling and whitening.
Our approach uses to a \emph{rational approximation} of $\bK^{-1/2}$ in conjunction with \emph{matrix-vector multiplication}-based solvers.
This scales better than existing methods (e.g. Cholesky) by
\begin{enumerate*}
  \item reducing computation from $\bigo{N^3}$ to $\bigo{N^2}$; and
  %\item reducing memory from $\bigo{N^2}$ to $\bigo{N}$; and
  \item more effectively using GPU acceleration.
\end{enumerate*}

\paragraph{Cauchy Integral Quadrature (CIQ).}
A well-established result from complex analysis is that $\bK^{-1/2}$ can be expressed through Cauchy's integral formula \citep{davies2005computing,hale2008computing,golub2012matrix}:
%
\begin{equation}
	\bK^{-\frac 1 2} = \frac{1}{2 \pi i} \int_\Gamma t^{-\frac 1 2} \left( t \bI - \bK \right)^{-1} \intd t,
  \label{eqn:contour_integral}
\end{equation}
%
where $\Gamma$ is a contour in the complex plane that surround the spectrum of $\bK^{-\frac 1 2}$.
Since there is no analytic form for this integral, we instead rely on a quadrature-based rational approximation:
%
\begin{equation}
	\bK^{-\frac 1 2} \approx \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1},
	\label{eqn:contour_integral_quad}
\end{equation}
%
where the weights $w_q$ encapsulate the normalizing constant and the $t^{-\frac 1 2}$ term of \cref{eqn:contour_integral}.
\citet{hale2008computing} introduce a real-valued quadrature strategy based on Jacobi elliptic functions (described in \cref{app:quadrature}) which enables \cref{eqn:contour_integral_quad} to converge extremely rapidly.
%
\begin{lemma}[\citet{hale2008computing}, Thm. 4.1]
  Let $t_1$, $\ldots$, $t_Q$ and $w_1$, $\ldots$, $w_Q$ be the shifts and weights of \citet{hale2008computing}'s quadrature procedure (see \cref{app:quadrature}).
  Then the error of \cref{eqn:contour_integral_quad} is bounded as:
  \[
    \left\Vert \bK \sum_{q=1}^Q w_q \left( t_q \bI - \bK \right)^{-1} - \bK^{\frac 1 2} \right\Vert_2
    \leq \bigo{\exp\left( \frac {\log \kappa(\bK) + 3} {2 Q \pi^2} \right)},
  \]
  where $\kappa(\bK)$ is the condition number of $\bK$.
\label{lemma:hale}
\end{lemma}
%
\noindent
Remarkably, the error of \cref{eqn:contour_integral_quad} is \emph{logarithmically} dependent on the conditioning of $\bK$.
\citeauthor{hale2008computing} demonstrate that, even for matrices with $\kappa(\bK) \approx 10^4$, choosing $Q = 20$ is sufficient to obtain accuracy up to \emph{full machine precision}.
For the remainder of this paper, this method for estimating \cref{eqn:contour_integral} will be referred to as {\bf Cauchy Integral Quadrature (CIQ)}.

It is worth noting that the quadrature procedure of  \citeauthor{hale2008computing} requires estimates of $\bK$'s extreme eigenvalues $\lambda_\text{min}$ and $\lambda_\text{max}$ (see \cref{app:quadrature}).
An efficient estimate of these terms comes from running $\approx 10$ iterations of the Lanczos algorithm to obtain $\bT \approx \bQ^\top \bK \bQ$ (where again $\bT$ is tridiagonal and $\bQ$ has orthonormal columns---see \cref{sec:lanczos}).
The extreme eigenvalues of $\bT$ are known to rapidly converge to the extreme eigenvalues of $\bK$ \citep[e.g.][Ch. 10]{golub2012matrix}.
An eigendecomposition of $\bT$ is an efficient way to estimates $\lambda_\text{min}$ and $\lambda_\text{max}$ since $\bT$ is a small triagonal matrix.
Beyond this small eigendecomposition, the Lanczos eigenvalue estimates only requires $\approx 10$ MVMs with $\bK$.




\subsection{An Efficient Matrix-Vector Multiplication Approach to CIQ with msMINRES}
%
Using the quadrature method of \citet{hale2008computing} for whitening and sampling requires performing solves with multiple shifted matrices.
For example, drawing a sample from $\normaldist{0}{\bK}$ using \cref{eqn:contour_integral_quad} requires computing
$\bK \left( \sum_{q=1}^Q w_q (t_q \bI - \bK )^{-1} \bb \right),$
where $\bb \sim \normaldist{\bzero}{\bI}$.
Importantly, most applications of sampling and whitening (e.g. Bayesian optimization, variational GPs) require repeated computations of matrix square roots.
Therefore these $Q$ solves must be efficient to compute.

\paragraph{MVM approaches to matrix solves.}
As in the previous chapters, we will use MVM methods to perform these $Q$ solves.
However, we will introduce a new Krylov algorithm---based on \citet{paige1975solution}'s minimum residuals algorithm (MINRES)---that is specifically designed for shifted solves.
Our MINRES variant offers substantial computational savings and more parallelism than even mBCG affords.
Moreover, its convergence properties are especially desirable for Gaussian sampling applications, as we will demonstrate in \cref{sec:convergence}.


\paragraph{msMINRES for multiple shifted solves.}
Our variant of MINRES---multi-shift MINRES or {\bf msMINRES}---simultaneously computes all shifted solves.
First, recall from \cref{sec:minres} that the MINRES solution for $\bK^{-1} \bb$ can be formed through Lanczos (see \cref{eqn:minres_qr}):
\begin{equation}
  \bK^{-1} \bb \approx \Vert \bb \Vert_2 \: \bQ_J \left( \bR^{-1}_J \bQU_J^\top \right) \be_1,
  \quad
  \bQU_J \bR_J = \begin{bmatrix} \bT_J \\ \Vert \br_J \Vert_2 \be_J^\top \end{bmatrix},
  \label{eqn:minres_qr2}
\end{equation}
where $\bQ_J$, $\bT_J$, and $\br_J$ are the outputs from $J$ steps of the Lanczos algorithm with initial vector $\bb$:
\[
  \bK \bQ_J = \bQ_J \bT_J + \br_J \be_J^\top.
\]
To adapt MINRES to multiple shifts, we exploit a well-established fact about the Krylov subspaces of shifted matrices (see \citep[e.g.][]{jegerlehner1996krylov}).
%
\begin{observation}
  Let $\bK \bQ_J = \bQ_J \bT_J + \br_J \be_J^\top$ be the Lanczos factorization for matrix $\bK$ given initial vector $\bb$ (see \cref{sec:lanczos}).
  Then $$(\bK + t \bI) \bQ_J = \bQ_J (\bT_J + t \bI) + \br_J \be_J^\top$$ is the Lanczos factorization for matrix $(\bK + t \bI)$ and initial vector $\bb$.
\end{observation}
%
\noindent
In other words, if we run Lanczos on $\bK$ and $\bb$, then we get the Lanczos factorization of $(\bK + t \bI)$ \emph{for free}, without any additional MVMs!
Consequentially, we can re-use the $\bQ_J$ and $\bT_J$ Lanczos matrices to compute \emph{multiple shifted solves.}
%
\begin{equation}
  (\bK + t \bI)^{-1} \bb \approx \Vert \bb \Vert_2 \: \bQ_J \left( \bR^{(t){-1}}_J \bQU_J^{(t)\top} \right) \be_1,
  \quad
  \bQU_J^{(t)} \bR_J^{(t)} = \begin{bmatrix} \bT_J + t \bI \\ \Vert \br_J \Vert_2 \be_J^\top \end{bmatrix},
  \label{eqn:minres_qr_shifted}
\end{equation}
%
Assuming $\bQ$ and $\bT$ have been previously computed, \cref{eqn:minres_qr_shifted} requires no additional MVMs with $\bK$.

\input{algorithms/msminres}

\paragraph{A simple vector recurrence for msMINRES.}
Recall that, for standard MINRES, \cref{alg:minres} reduces \cref{eqn:minres_qr2} to a simple vector recurrence.
Similarly, \cref{eqn:minres_qr_shifted} can also be re-written as a vector recurrence.
We simply modify the existing MINRES algorithm: before the QR step we add $t$ to the Lanczos diagonal terms $\gamma_j' = \bT_J^{(j,j)} + t$.
This can be extended to simultaneously handle \emph{multiple shifts} $t_1, \ldots, t_Q$.
Each shift would compute its own QR factorization, its own step size $\varphi_j^{(t_q)}$, and its own search vector $\bd_j^{(t_q)}$.
However, all shifts share the same Lanczos vectors $\bq_j$ and therefore share the same MVMs.
The shifts' QR, step size, and search vector computations can be vectorized for efficient parallelization (in the same vein as BBMM).

To summarize: the resulting algorithm---msMINRES---gives us $(t_1 \bI - \bK)^{-1} \bb$, $\ldots$, $(t_Q \bI - \bK)^{-1}$ \emph{essentially for free}, simply by computing $\bK^{-1} \bb$.
\cref{alg:msminres} outlines the procedure; below we highlight its computational properties:
%
\begin{property}[Computation/Memory of msMINRES]
  $J$ iterations of msMINRES requires exactly $J$ matrix-vector multiplications (MVMs) with the input matrix $\bK$,
  regardless of the number of shifts $Q$.
  The resulting runtime of msMINRES is $\bigo{ J \mvm{\bK}}$, where $\mvm{\bK}$ is the time to perform an MVM with $\bK$.
  The memory requirement is $\bigo{ QN }$ in addition to what's required to store $\bK$.
\end{property}
%
\noindent
For standard $N \! \times \! N$ kernel matrices the runtime of msMINRES will be $\bigo{J N^2}$, though all quadratic operations will greatly benefit from GPU-acceleration.
This complexity be reduced if $\bK$ is structured, low-rank, or sparse.
%Moreover, by partitioning $\bK$ and performing MVMs in a Map-Reduce fashion \cite{wang2019exact,charlier2020kernel}, we can avoid explicitly computing $\bK$ resulting in $\bigo{QN}$ total memory.




\subsection{Convergence Analysis}
\label{sec:convergence}

Combining msMINRES with the quadrature formula in \cref{eqn:contour_integral_quad} gives us an efficient MVM method for sampling an whitening.
This {\bf CIQ} approach is summarized by \cref{alg:ciq}.

With the msMINRES algorithm, the computational requires of CIQ is basically no different than CG for computing MVM-based solves.
When using \cref{alg:ciq} to produce Gaussian samples from the distribution $\normaldist{\bzero}{\bK}$ we can obtain a bound on the error:
%
\begin{theorem}
  Let $\bK$ and $\bb$ be the inputs to \cref{alg:ciq}, producing the output $\bd_J \approx \bK^{1/2} \bb$ after $J$ iterations.
  The difference between $\bd_J$ and $\bK^{1/2} \bb$ is bounded by:
  %
  \begin{equation}
    \left\Vert \bd_J - \bK^{\frac 1 2} \bb \right\Vert_2
    \leq
    \overbracket{
      \bigo{\exp\left( -\frac{2 Q \pi^2}{\log \kappa(\bK) + 3} \right)}
    }^{\text{Quadrature error}}
    +
    \overbracket{
      2 \left\vert \sum_{q=1}^Q w_q \right\vert
      \left( \frac{ \sqrt{\kappa(\bK)} - 1}{ \sqrt{\kappa(\bK)} + 1} \right)^J
      \left\Vert \bb \right\Vert_2
    }^{\text{msMINRES error}}
  \end{equation}
  %
  where $Q$ is the number of quadrature points and $\kappa(\bK)$ is the condition number of $\bK$.
  \label{thm:convergence}
\end{theorem}
%
As a simply corollary, the bias of the CIQ samples decreases exponentially with more quadrature points $Q$ and more msMINRES iterations $J$.
Though \cref{thm:convergence} implies fast convergence, it also suggests that msMINRES will be the primary source of error as it is more dependent on the conditioning of $\bK$.
We will discuss how to address this with preconditioning in \cref{sec:ciq_precond}.

\input{algorithms/ciq}


\subsection{Efficient Vector-Jacobi Products for Backpropagation}

In certain applications, such as learning variational GPs, we have to compute gradients of the $\bK^{-1/2} \bb$ operation.
The backward pass forms the vector-Jacobian product $\bv^\top ( \partial \bK^{-1/2} \bb / \partial \bK)$, where $\bv$ is the gradient from earlier steps.
The general form of the Jacobian is the solution to a Sylvester equation, which requires expensive iterative methods or solving a $N^2 \times N^2$ Kronecker sum $(\bK^{1/2} \oplus \bK^{1/2})^{-1}$.
Both of these options are much slower than the forward pass and are impractical for large $N$.

Fortunately, the quadrature approximation use that we use in \cref{alg:ciq} affords a computationally efficient vector-Jacobian product.
If we back-propagate directly through the quadrature in \cref{eqn:contour_integral_quad}, we have
%
\begin{align}
  \bv^\top \left( \frac{ \partial \bK^{-1/2} \bb }{ \partial \bK } \right)
   \frac{ \partial \bv^\top \bK^{-1/2} \bb }{ \partial \bK }
  &\approx { \partial \bv^\top \left( \sum_{q=1}^{Q} w_q (t_q \bI - \bK)^{-1} \right) \bb }/{ \partial \bK }
  \nonumber
  \\
  &= - \frac{1}{2} \sum_{q=1}^{Q} w_q
  \left( t_q \bI - \bK \right)^{-1}
  \left( \bv \bb^\top + \bb \bv^\top \right)
  \left( t_q \bI - \bK \right)^{-1}.
  \label{eqn:ciq_deriv}
\end{align}
%
The forward pass computes the solves with $\bb$.
Therefore, the only additional work needed for the backward pass are the shifted solves $(t_q \bI - \bK)^{-1} \bv$, which can be computed with another call to the msMINRES algorithm.
Thus the backward pass takes only $\bigo{J \mvm{\bK}}$ (e.g. $\bigo{J N^2}$) additional time.
To the best of our knowledge, this is the first use of quadrature for matrix-root back-propagation.

\paragraph{Programmability.}
As with BBMM and LOVE, CIQ can take full advantage of GPyTorch's {\tt LazyTensor} construct.
The algorithm for CIQ (\cref{alg:ciq}) accesses $\bK$ through Lanczos and msMINRES---each of which only requires black-box access to an MVM routine.
Therefore, CIQ can be implemented for specialty GP models using the efficient {\tt \_matmul} function of {\tt LazyTensor}s.

Moreover, back-propagation via \cref{eqn:ciq_deriv} can be implemented using the {\tt \_deriv} function of LazyTensors.
Recall that the {\tt \_deriv($ \bA, \bB $)} function computes $\partial \tr{ \bA^\top \bK \bB }/{\partial \br}$, where $\br$ is a representation of $\bK$.
If we define $\bA$ and $\bB$ as
%
\begin{align*}
  \bA &= \begin{bmatrix}
    w_1 \left( t_q1\bI - \bK \right)^{-1} \bv
    & \ldots &
    w_Q \left( t_Q \bI - \bK \right)^{-1} \bv
  \end{bmatrix},
  \\
  \bB &= \begin{bmatrix}
    \left( t_q1\bI - \bK \right)^{-1} \bb
    & \ldots &
    \left( t_Q \bI - \bK \right)^{-1} \bb
  \end{bmatrix},
\end{align*}
%
then the vector-Jacobi product in \cref{eqn:ciq_deriv} can be re-written as $\tr{ \bA \bB^\top } = \partial \tr{ \bA^\top \bK \bB}/\partial \br = \texttt{ \_deriv($\bA, \bb$)}$.
This allows us to easily implement efficient CIQ variants for many classes of structured matrices.


\subsection{Preconditioning}
\label{sec:ciq_precond}

To improve the convergence of \cref{thm:convergence}, we will introduce a preconditioner matrix $\bP$ where $\bP^{-1} \bK \approx \bI$.
It is not immediately straightforward to apply preconditioning to \cref{alg:ciq}, as the preconditioner breaks the geometry exploited by msMINRES \cite{jegerlehner1996krylov,aune2013iterative}.
More specifically, if we apply a preconditioner to msMINRES, the added shifts modify the preconditioned system and not the original system.
In other words we obtain the solves
%
\begin{equation*}
	\bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2} + t_q \bI)^{-1} (\bP^{-\frac 1 2} \bb).
\end{equation*}
%
Plugging these shifted solves into the quadrature equation \cref{eqn:contour_integral_quad} therefore gives us
%
\begin{equation}
	\widetilde \bd_J \approx \bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} (\bP^{-\frac 1 2} \bb).
  \label{eqn:not_a_sqrt}
\end{equation}
%
In general, we cannot recover $\bK^{-1/2}$ from \cref{eqn:not_a_sqrt}.
However, note if  we choose $\bb \sim \normaldist{\bzero}{\bP}$, then
%
\begin{align*}
	\Evover{\bb}{\widetilde \bd_J \widetilde \bd_J}^\top
	&=
	\left( \bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} \bP^{-\frac 1 2} \right)
	\Evover{\bb}{ \bb_J \bb_J^\top}
	\left( \bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} \bP^{-\frac 1 2} \right)
	\\
	&=
	\left( \bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} \bP^{-\frac 1 2} \right)
	\bP
	\left( \bP^{-\frac 1 2} (\bP^{-\frac 1 2} \bK \bP^{-\frac 1 2})^{-\frac 1 2} \bP^{-\frac 1 2} \right)
	=
	\bK^{-1}.
\end{align*}
%
Therefore, $\bK \widetilde \bd_J$ is a valid sample from $\normaldist{\bzero}{\bK}$.
Importantly, the convergence of $\bd_j$ now depends on the conditioning $\kappa(\bP^{-1} \bK) \ll \kappa(\bK)$.
The only requirements we have for $\bP$ are that
\begin{enumerate*}
	\item it affords efficient solves (ideally $o(n^2)$), and
	\item it affords efficient samples $\bb \sim \normaldist{\bzero}{\bP}$ (also ideally $o(n^2)$).
\end{enumerate*}
%
Note that these requirements are met by the partial pivoted Cholesky preconditioner proposed in \cref{sec:preconditioning}.

%One possible preconditioner is the partial pivoted Cholesky preconditioner proposed by \citet{gardner2018gpytorch} (see \cref{app:preconditioner}).
%The form of $\bP$ is $\bR \bR^\top + \sigma^2 \bI$, where $\bR$ is a low-rank factor and $\sigma^2 \bI$ is a small diagonal component.
%This preconditioner affords $\approx \bigo{N}$ solves using the matrix inversion lemma and $\bigo{N}$ samples using the reparameterization trick \cite{kingma2013auto,rezende2014stochastic}.\footnote{
	%Let $\bepsilon_1'$ and $\bepsilon_2'$ be standard Normal random variables.
	%Then $\bR \bepsilon_1' + \sigma \bepsilon_2'$ is a sample from $\normaldist{\bzero}{\bP}$.
%}
%Moreover, this preconditioner is highly effective on many Gaussian covariance matrices \cite{gardner2018gpytorch,wang2019exact}.





\subsection{Related Work}
%There has been growing interest in applying Krylov methods to Gaussian process training and inference \gp{CITE}, especially due to its memory efficiency and ability to utilize GPU acceleration \cite{gardner2018gpytorch,wang2019exact}.
There have been some Krylov methods for $\bK^{-1/2} \bb$ have been proposed by the scientific computing community \cite{aune2013iterative,aune2014parameter,chow2014preconditioned,saibaba2013flexible}.
\citet{aune2013iterative,aune2014parameter} draw samples using a similar quadrature formulations in conjunction with shifted conjugate gradients.
However, their method does not allow for the use of preconditioning.
\citet{chow2014preconditioned} draw samples using a preconditioned Lanczos algorithm.
Their method does not rely on Cauchy's integral formula, and therefore it relies on storing the entire $\bQ_J$ and $\bT_J$ matrices.
Our proposed Krylov approach differs from these prior works in that
\begin{enumerate*}
  \item it relies on vector-recurrence that can be preconditioned for fast convergence; and
  \item it affords a simple derivative for back-propagation.
\end{enumerate*}
