\chapter{Introduction}
\label{chapt:introduction}

%Machine learning hype <- success in many different areas.
%Success in many different areas <- advances in algorithms.
%Advances of algorithms <- both in terms of raw potential and in terms of the practical.
%The best algorithms are ones that offer both potential and practicality.
%No one algorithm is perfect <- we need a quiver of different possible algorithms.

%Don't talk about advances.
%Find a lead in to discuss algorithms.

%At this point in machine learning we have several sets of algorithms.

%Questions we need to answer:
%- Why is it important to have multiple algorithms?
%- What kind of algorithms are we looking for?

%Of course, no one algorithm is good at every possible task.
%For example, on many computer vision tasks it is commonly assumed there will be some sort of convolutional layers.
%On many Kaggle competitions, a majority of winning submissions utilize random forests.
%Therefore, it is arguably good to have a variety of possible algorithms.



%Neural networks.

Rapid advances in machine learning have led to numerous empirical successes and wide-scale adoption in many application domains.
%Here we will highlight two broad categories of methodological innovation that have contributed to this surge.
Researchers have improved the {\bf predictive capabilities} of machine learning models.
Algorithms are now able to achieve superhuman performance on complex tasks like object recognition \cite{} and strategy game-playing \cite{}.
There have also been important strides making these predictions more risk-aware \cite{}, interpretable \cite{}, and societally equitable \cite{}.
At the same time, the machine learning community has addressed many {\bf pragmatic concerns} around computational efficiency and easy-of-use.
Models which used to require several weeks of training can now be trained in under an hour \cite{goyal2017accurate}.
Transfer learning techniques and high-quality software frameworks have additionally made it easier for practitioners to rapidly-prototype new models.
While many other factors have contributed to the machine learning surge, we will limit our focus to these two categories.
Ever-increasing predictive capabilities have opened up new possibilities, while pragmatic improvements have accelerated the rate of innovation and adoption.

Arguably, the machine learning algorithms which have had the broadest impact are the ones that seamlessly offer \emph{both} predictive potential and practicality.
Deep neural networks perhaps best exemplify this trend.
Recent innovations in
network architecture \citep[e.g.][]{krizhevsky2012imagenet,he2016deep,vaswani2017attention,devlin2018bert,huang2019convolutional},
optimization \citep[e.g.][]{hochreiter1997flat,ioffe2015batch,izmailov2018averaging},
and theoretical understanding \citep[e.g.][]{keskar2016large,jacot2018neural,arora2019fine}
have led to massive performance improvements on increasingly complex vision and natural language tasks.
Moreover, these innovations have been complemented by
the effective use of specialty compute hardware (such as GPUs and TPUs),
the introduction of automatic differentiation \citep[e.g.][]{paszke2017automatic},
and the development of of several easy-to-use software implementations~\citep[e.g.][]{jia2014caffe,chen2015mxnet,abadi2016tensorflow,paszke2019pytorch}.
These practical advances make it easy for practitioners to experiment with and rapidly prototype new deep learning models, which has undoubtedly contributed to research innovations and its wide-spread adoption \cite{goodfellow2016deep}.

Gradient-boosted trees are another algorithm class with a similar powerful-and-practical story.
Since their inception \cite{friedman2001greedy,friedman2002stochastic}, gradient boosted trees have excelled in many applications \citep[e.g.][]{richardson2007predicting,burges2010ranknet,li2010robust}.
The predictive power of these models is a product of several key attributes: for example, their remarkable generalization properties \citep{freund1997decision,schapire2013boosting} and their ability to handle incomplete features \cite{friedman2001greedy}.
Equally important, these models can be simple and computational efficient, in large part due to specialty parallel algorithms \citep[e.g.][]{panda2009planet,tyree2011parallel,ke2017lightgbm} and easy-to-use software implementations such as XGBoost \cite{chen2016xgboost}.
These advantages have made gradient-boosted decision trees a workhorse algorithm for many practitioners in a wide variety of application domains.
According to the most recent survey collected by Kaggle \cite{kaggle2019kaggle}, $75\%$ of the responding data scientists regularly use gradient-boosted decision trees and the XGBoost software.

However, for many machine learning algorithms there is still a trade-off between predictive potential and its practical limitations.
The focus of this thesis is {\bf Gaussian process models} (GPs), which perhaps best exemplify this tension.
Within the machine learning community, GPs have been well-regarded as a powerful model class with many desirable properties---such as calibrated uncertainty estimates and interpretable model priors.
Recent work on hierarchical modelling \citep[e.g.][]{damianou2013deep} and scalability \citep[e.g.][]{wilson2015kernel} have furthered their applicability to increasingly complex datasets.
However, Gaussian processes have historically not scaled well to large datasets, and the tools most commonly used for inference do not effectively utilize modern compute hardware.
Scalable approximations can remedy these concerns to some extent, yet such approximations can sometimes bias the model's predictions \cite{turner2011two,bauer2016understanding}.
Finally, new models require significant implementation, as simple modifications like an additional output dimension might require different learning and inference procedures.
These practical considerations hinder the adoption of GP models, while also limiting researchers' abilities to rapidly-prototype and make new developments.
This thesis aims to address these limitations so that Gaussian processes can be powerful-\emph{and}-practical models.

% Even as Moore's law runs out, specialty hardware

\section{Predictive Power of Gaussian Processes Models}

Before addressing these issues, it is worth discussing why Gaussian processes are an invaluable model class in domains such as blackbox optimization \cite{snoek2012practical}, robotics \cite{deisenroth2011pilco}, and health care \cite{schulam2015framework}:
\begin{enumerate}
  \item {\bf Closed-form marginalization over hypotheses.}
    Many predictive machine learning algorithms (such as neural networks) construct a single predictive model by optimizing over thousands or millions of parameters.
    Gaussian processes on the other hand marginalize over all possible predictive models $f(\cdot)$:
    \[
      p_\text{GP} ( y \mid \bx ) = \int_{f(\cdot)} p( y \mid \bx, f(\cdot)) \: p(f(\cdot)) \: \intd f(\cdot).
    \]
    (See \cref{sec:gps} for more details).
    As a result, the predictions incorporate modelling uncertainty are are less prone to overfitting \cite{rasmussen2006gaussian}.

  \item {\bf Well-calibrated uncertainty estimates.}
    The output of a Gaussian process is a predictive \emph{distribution}, which incorporates both modelling uncertainty (e.g. how many different models could fit the data) and data uncertainty (e.g. how noisy are the training data).
    Consequentially, the predictive uncertainties tend to be very well calibrated to the data distribution.

  \item {\bf A flexible language for encoding prior knowledge.}
    A Gaussian process' generalization to unseen data is almost entirely determined by its modelling priors.
    Crucially, the modelling priors of GP directly encode functional properties---such as smoothness, periodicity, or monotonicity---rather than beliefs about certain parameters.
    These functional properties are determined by the choice of \emph{kernel function}, which can be easily composed to express complex priors (see \cref{sec:common_kernels}).
    With the appropriate choice of prior, it is possible to generalize on datasets with as few as 10 observations \citep[e.g.]{rasmussen2006gaussian,gardner2017discovering}.
    Several recent works have simplified the task of constructing appropriate kernels, either through composition \citep{duvenaud2013structure} or through differentiable learning \citep{wilson2013gaussian}.

  \item {\bf Interpretable predictions.}
    The predictions from Gaussian processes (see \cref{eqn:predictive_mean,eqn:predictive_var}) are not only expressive and powerful; they are also very intuitive.
    If we view the GP's kernel function as a similarity/distance measure between two points, then the prediction at a given point $\bx$ is simply an interpolation of nearby training points.
    The prediction's confidence interval is tight when $\bx$ is close to training dataset points, and large when $\bx$ is too far from points for accurate interpolation.
\end{enumerate}

The benefits listed above are obviously applicable to the ``small data'' regime, where priors and marginalization are critical for meaningful predictive performance.
However, it is worth noting that many of these desirable properties also apply to large data regimes as well.
Gaussian processes (with certain kernels) are universal approximators \cite{micchelli2006universal}, and their modelling capacity increases with the amount of available training data.
Larger datasets make it possible to use more powerful families of covariance functions \citep{wilson2013gaussian,wilson2016deep,benton2019function}, which is especially useful for extrapolating on complex spatial-temporal data.
Moreover, using Gaussian processes in conjunction with hierarchical modelling \cite{wilson2016deep,salimbeni2017doubly,jankowiak2020deep} can learn powerful models for complex datasets.


\section{Practical Concerns with Gaussian Processes Models}

While Gaussian processes offer great potential several compelling benefits, there are several issues which hinder their usability.
Most of these issues are especially pertinent to larger and more complex datasets.

\subsection{Computational Complexity and Memory Requirements}
Given $N$ training data points, Gaussian process models require $\bigo{N^3}$ computation and $\bigo{N^2}$ storage.
This complexity comes from computing a $N \times N$ covariance matrix of all training data and computing several non-linear operations with it (see \cref{sec:gp_models}).
Historically, this has limited exact Gaussian process inference to datasets with fewer than $1,\!000$ data points \cite{hensman2013gaussian,wilson2014thesis}.

\subsection{Use of Modern Compute Hardware}
It is worth noting that this $\bigo{N^3}$ computational complexity is not insurmountable given the available computational hardware.
For example, some deep learning models require as many floating point operations (FLOPs) as similarly-sized Gaussian process models.
A 264-layer DenseNet model \cite{huang2017densely}, common on many computer vision tasks, requires $2.4 \times 10^{18}$ FLOPs to train $1.2$ million images.
This is essentially a cubic computational requirement, and would be wholly impractical if it were not for specialty compute hardware like GPUs.
Such a model would probably require months to train on standard CPUs, yet can be trained on 8 GPUs in a matter of hours \cite{howard2018training}.

Given the effectiveness of GPU acceleration on large neural networks, one might expect similar computational performance from large-scale Gaussian processes.
Unfortunately, many GP implementations rely on the Cholesky factorization (see \cref{sec:gp_models}), which does not benefit as readily from modern compute hardware.
GPUs are specially designed for massively-parallelizable operations such as matrix-multiplication---the primary building block of neural network training/inference.
For example, a matrix multiply between two $1,\!000 \times 1,\!000$ matrices is $10,\!000$ times faster on a GPU than on a CPU,\footnote{
	As measured on a NVIDIA 1080-TI GPU verses an 8-core Intel i7 CPU.
}, which is why large neural networks are practical to train.
On the other hand, the Cholesky factorization is an inherently sequential algorithm that affords minimal parallelization.
More concretely, computing the Cholesky factorization of a $1,\!000 \times 1,\!000$ matrix is only $10$ times faster on a GPU than on a CPU.
This is why we cannot expect neural-network-level speedups for Cholesky-based GPs.

Moreover, computing and using the Cholesky factorization requires $\bigo{N^2}$ storage.
This amounts to a terabyte of memory for $N=1,\!000,\!000$---well beyond the capacity of most GPU clusters.

\subsection{Choosing Appropriate Approximations}
To reduce the computational and memory burden, researchers have proposed numerous methods that approximate Gaussian processes with simpler models.
Such models employ low-rank or structured approximations of the $N \times N$ matrices (see \cref{sec:approx_gps}).
Numerous advances have made these approximate methods more powerful while retaining compelling asymptotic complexities.

However, choosing a suitable scalable approximation involves many design choices that depend on the particular dataset.
Firstly, each methods makes a unique set of trade-offs which may not be well suited to certain datasets.
Variational approaches \cite{titsias2009variational}---which are a popular general-purpose approximation---tend to overestimate the observational noise which results in worse predictive uncertainties \cite{turner2011two,bauer2016understanding}.
Structured interpolation methods \cite{wilson2015kernel} alleviate these biases, yet such methods are limited to low-dimensional problems with specific covariance matrices.
Additionally, all approximate methods introduces additional hyperparameters that control the speed/accuracy trade-off.
While some theoretical guarantees can guide these design decisions \cite{wilson2015thoughts,burt2019rates}, a good approximate model typically relies on a large hyperparameter sweep and expert knowledge.


\subsection{Implementation and Programmability}
One compelling advantage of neural networks is the modularity of their architectures.
While creating novel architectures requires significant thought and experimentation, \emph{implementing} these architectures requires very little software engineering effort.
Seemingly complex models like DenseNets \cite{huang2017densely} and Transformers \cite{vaswani2017attention} take no more than 300 lines of code using libraries of compositional layers and sub-routines.
Small modifications, such as adding an additional output dimension, often require only a single additional line of code.

Gaussian processes models on the other hand often require significant implementation effort.
Often, the \emph{model} and the \emph{learning/inference procedures} are tightly coupled.
As an example, consider converting a standard Gaussian process into a Gaussian process over multiple output dimensions \cite{bonilla2008multi}.
While this is seemingly a simple modelling change, it may require a completely different implementation.
This is because the additional output dimension adds Kronecker-product structure to the prior covariance matrix (see \cref{sec:advantages}), which modifies the equations used for efficient inference.
In the popular GPy software package,\footnote{
	Available at \url{https://github.com/SheffieldML/GPy}
} multi-output GPs and standard GPs are implemented as separate models, with multi-output GPs requiring 100 lines of additional code.
Compared to the one-line change required for multi-output neural networks, Gaussian processes are significantly more difficult to rapidly prototype.


\section{Outline of Contributions}

This thesis proposes several methods to alleviate these issues without sacrificing the predictive power or desirable properties of Gaussian processes.
At the heart of these methods is a unified learning/inference framework that better utilizes modern compute hardware and simplifies implementation efforts.
Taking inspiration from neural networks, we restrict our computational procedures to matrix multiplication and element-wise operations, finding opportunities to unify or batch operations wherever possible.
\gp{CONTINUE}
% In fact, we are not simply going to take inspiration from Neural networks, we are going to directly exploit those tools

%\paragraph{Preventing catastrophic failure.}

%\paragraph{Improved predictive pipelines.}

%\paragraph{Detecting anomalous inputs.}
%Machine learning models will only generalize to data that are sufficiently similar to the training data.
%If a model encounters \emph{out-of-distribution} (OOD) inputs -- inputs that deviate from the distribution of training data -- its predictions are likely to be erroneous or nonsensical \cite{begoli2019uncertainty,jiang2012calibrating}.
%%This may occur if the model is used in scenarios that experience covariate shift \cite{sugiyama2007covariate} or if the model encounters previously-unseen categories of data \cite{yu2017occ,hassen2018openset}.
%%Such scenarios are examples of \emph{out-of-distribution} (OOD) inputs.
%This phenomenon is illustrated in \cref{fig:ood_teaser}, which displays predictions from a neural network trained to predict prices of middle-class houses in Kentucky.
%The model is able to make sensible predictions on other Kentucky houses (left and center-left images).
%At the same time, the model vastly underestimates the price of a California mansion (center-right) and predicts and absurdly large price for a chair (right), as these inputs are not similar to any of the training set inputs.
%This is because the range of predicted prices for the out-of-distribution matches the range of Kentucky housing prices.
%A practitioner would see that the predictions are well within the model's expected output values and would be unaware that these predictions are nonsensical given the supplied inputs.

%Well-modeled uncertainty estimates can to identify potentially anomalous data and prevent such erroneous predictions.
%In this example, \gp{finish}

%\paragraph{A principled exploration/exploitation tradeoff.}

%\paragraph{Interpretability and trustworthiness.}
%Good uncertainty estimates can provide a valuable extra bit of information to users of machine learning models when predictions may otherwise be difficult to interpret.
%As machine learning algorithms become increasingly complex, they also appear more ``black box'' to users of such systems.
%This presents several challenges, especially for models that are used to aid human decision makers in domains such as medicine, finance, and policy \gp{cite}.

%For such circumstances it is therefore desirable for predictions to be understandable or interpretable \gp{cite LIME, saliency, etc.}.
%There are many definitions for what constitutes a good ``explanation'' of black-box predictions, coming from policy makers \gp{cite gdpr, etc} and the research community \gp{cite} alike.
%Though there are disagreements between these various sources, a well-calibrated uncertainty estimate is typically seen as a bare-minimum requirement for an interpretable prediction \gp{cite}.
%Most humans -- even if they are unable to perform simple inferences \cite{gigerenzer2003simple} -- have natural intuition for interpreting confidence estimates as event-occurrence frequencies \cite{cosmides1996humans,hoffrage1998using}.
%Therefore, well-calibrated confidence estimates from ML models can be easily interpreted by users.
%Moreover, the presence of uncertainty estimates can affect a user's trust in a machine learning model.
%In a study by \citet{zhou2017effects}, humans were asked to plan a budget for construction tasks based on information provided by a machine learning model.
%Some participants received both predictions and confidence intervals from the machine learning model, while other users only received the predictions.
%On tasks with low-to-moderate cognitive overhead, participants who saw uncertainty scores reported higher levels of trust in the machine learning model.
