\chapter{Introduction}
\label{chapter:introduction}

%Machine learning hype <- success in many different areas.
%Success in many different areas <- advances in algorithms.
%Advances of algorithms <- both in terms of raw potential and in terms of the practical.
%The best algorithms are ones that offer both potential and practicality.
%No one algorithm is perfect <- we need a quiver of different possible algorithms.

%Don't talk about advances.
%Find a lead in to discuss algorithms.

%At this point in machine learning we have several sets of algorithms.

%Questions we need to answer:
%- Why is it important to have multiple algorithms?
%- What kind of algorithms are we looking for?

%Of course, no one algorithm is good at every possible task.
%For example, on many computer vision tasks it is commonly assumed there will be some sort of convolutional layers.
%On many Kaggle competitions, a majority of winning submissions utilize random forests.
%Therefore, it is arguably good to have a variety of possible algorithms.



%Neural networks.

The past decade has witnessed a wide-scale adoption of machine learning methods across numerous application domains.
%Here we will highlight two broad categories of methodological innovation that have contributed to this surge.
This surge is due to the confluence of several factors, of which we will highlight two.
First, researchers have demonstrated the unparalleled {\bf predictive capabilities} of several machine learning algorithms.
Models can achieve superhuman performance on complex tasks like object recognition \cite{he2016deep}, machine translation \cite{vaswani2017attention}, and strategy game-playing \cite{silver2017mastering}.
At the same time, the community has developed algorithms that are increasingly {\bf practical and easy-to-use}.
Many models can be trained rapidly on consumer-level computer hardware \cite{howard2018training}, and high-quality software frameworks enable practitioners to rapidly develop new models.
%While many other factors have contributed to the machine learning surge, we will limit our focus to these two categories.
While machine learning's predictive successes has opened up new possibilities, its new-found ease-of-use has accelerated innovation and adoption.

Arguably, the machine learning algorithms which have had the broadest impact are the ones that seamlessly offer \emph{both} predictive power and practicality.
Deep neural networks perhaps best exemplify this trend.
Recent innovations in
network architecture \citep[e.g.][]{krizhevsky2012imagenet,he2016deep,vaswani2017attention,devlin2018bert,huang2019convolutional},
optimization \citep[e.g.][]{hochreiter1997flat,ioffe2015batch,izmailov2018averaging},
and theoretical understanding \citep[e.g.][]{keskar2016large,jacot2018neural,arora2019fine}
have led to massive performance improvements on increasingly complex datasets.
Moreover, these innovations have been complemented by
the effective use of specialty compute hardware (such as GPUs and TPUs),
the introduction of automatic differentiation \citep[e.g.][]{paszke2017automatic},
and the development of of several high-quality software implementations~\citep[e.g.][]{jia2014caffe,abadi2016tensorflow,paszke2019pytorch}.
These pragmatic advances make it easy for practitioners to experiment with new models and architectures, which has undoubtedly contributed to its profound and wide-spread successes \cite{goodfellow2016deep}.

Gradient-boosted trees are another algorithm class with a similar powerful-yet-practical story.
Since their inception \cite{friedman2001greedy,friedman2002stochastic}, gradient boosted trees have excelled in many applications \citep[e.g.][]{richardson2007predicting,burges2010ranknet,li2010robust}.
The predictive power of these models is a product of several key attributes: for example, their remarkable generalization properties \citep{freund1997decision,schapire2013boosting} and their ability to handle incomplete features \cite{friedman2001greedy}.
Equally important, these models are simple and computational efficient, in large part due to specialty parallel algorithms \citep[e.g.][]{panda2009planet,tyree2011parallel,ke2017lightgbm} and easy-to-use software implementations such as XGBoost \cite{chen2016xgboost}.
These advantages have made gradient-boosted decision trees a workhorse algorithm for many practitioners in a variety of application domains.
According to a survey collected by \citet{kaggle2019kaggle}, $75\%$ of the responding data scientists regularly use gradient-boosted decision trees and the XGBoost software.

Nevertheless, for many machine learning algorithms there is still a trade-off between predictive potential and its practical limitations.
The focus of this thesis is {\bf Gaussian process models} (GPs), which perhaps best exemplify this tension.
Within the machine learning community, GPs have been well-regarded as a powerful model class with many desirable properties---such as calibrated uncertainty estimates and interpretable model priors.
Recent work on hierarchical modelling \citep[e.g.][]{damianou2013deep} and scalability \citep[e.g.][]{wilson2015kernel} have furthered their applicability to increasingly complex tasks.
However, Gaussian processes have historically only been used on small datasets, and the tools most commonly used for inference do not effectively utilize modern compute hardware.
%Scalable approximations can remedy these concerns to some extent, yet such approximations can sometimes bias the model's predictions \cite{turner2011two,bauer2016understanding}.
%Finally, new
Using GPs requires significant implementation effort, as simple modifications like an additional output dimension might require different learning/inference procedures.
These practical considerations hinder the adoption of GPs, while also limiting researchers' abilities to rapidly-prototype and make new developments.
This thesis aims to address these limitations so that Gaussian processes can be powerful-\emph{and}-practical models.

% Even as Moore's law runs out, specialty hardware

\section{The Predictive Power of Gaussian Processes Models}

Before addressing these issues, it is worth discussing why Gaussian processes are an invaluable model class for blackbox optimization \citep[e.g.][]{snoek2012practical}, robotics \citep[e.g.][]{deisenroth2011pilco}, health care \citep[e.g.][]{schulam2015framework}, and many other domains:
\begin{enumerate}
  \item {\bf Closed-form marginalization over hypotheses.}
    Many machine learning algorithms (such as neural networks) construct a single model by optimizing over thousands or millions of parameters.
    Gaussian processes on the other hand marginalize over all possible predictive models $f(\cdot)$:
    \[
      p_\text{GP} ( y \mid \bx ) = \int_{f(\cdot)} p( y \mid \bx, f(\cdot)) \: p(f(\cdot)) \: \intd f(\cdot).
    \]
    As a result, the predictions incorporate modelling uncertainty are are less prone to overfitting \cite{rasmussen2006gaussian}.

  \item {\bf Well-calibrated uncertainty estimates.}
    The output of a Gaussian process is a predictive \emph{distribution}, which incorporates both modelling uncertainty (e.g. how many different models could fit the data) and data uncertainty (e.g. how noisy are the training data).
    Consequentially, the predictive uncertainties tend to be very well calibrated to the data distribution.

  \item {\bf A flexible language for encoding prior knowledge.}
    A Gaussian process' generalization to unseen data is almost entirely determined by its modelling priors.
    Crucially, the modelling priors of GP directly encode functional properties---such as smoothness, periodicity, or monotonicity---rather than beliefs about certain parameters.
    These functional properties are determined by the choice of \emph{kernel function}, which can be easily composed to express complex priors (see \cref{sec:common_kernels}).
    With the appropriate choice of prior, it is possible to generalize on datasets with as few as 10 observations \citep[e.g.]{rasmussen2006gaussian,gardner2017discovering}.
    %Several recent works have simplified the task of constructing appropriate kernels, either through composition \citep{duvenaud2013structure} or through differentiable learning \citep{wilson2013gaussian}.

  \item {\bf Interpretable predictions.}
    The predictions from Gaussian processes (see \cref{eqn:predictive_mean,eqn:predictive_var}) are not only expressive and powerful; they are also very intuitive.
    If we view the GP's kernel function as a similarity/distance measure between two points, then the prediction at a given point $\bx$ is simply an interpolation of nearby training points.
    The prediction's confidence interval is small when $\bx$ is close to training dataset points, and large when $\bx$ is too far from points for accurate interpolation.
\end{enumerate}

\noindent
These benefits are obviously applicable to the ``small data'' regime, where priors and marginalization are critical for meaningful predictive performance \cite{rasmussen2001occam}.
However, these properties also are beneficial for large datasets.
Good uncertainty estimates and interpretable predictions are increasingly desirable for large-scale machine learning models.
%GPs (with certain kernel functions) are universal approximators \cite{micchelli2006universal}, and their modelling capacity increases with the amount of available training data.
In addition, large datasets make it possible to use powerful families of covariance functions \citep{wilson2013gaussian,wilson2016deep,benton2019function} or hierarchical (``deep'') GP models \cite{wilson2016deep,salimbeni2017doubly,jankowiak2020deep}.
This makes GPs especially powerful on big-data tasks like large-scale extrapolation \citep[e.g.][]{jankowiak2020parametric} and high-dimensional optimization \citep[e.g.][]{eriksson2019scalable}.


\section{Practical Concerns with Gaussian Processes Models}

While Gaussian processes offer great predictive potential, there are several practical issues that are especially pertinent to larger and more complex datasets.

\subsection{Computational Complexity and Memory Requirements}
Given $N$ training data points, Gaussian process models na\"{i}vely require $\bigo{N^3}$ computation and $\bigo{N^2}$ storage.
This complexity comes from computing a $N \times N$ covariance matrix of all training data and computing several non-linear operations (see \cref{sec:gp_models}).
Historically, this has limited exact Gaussian process models to datasets with fewer than $1,\!000$ data points \cite{hensman2013gaussian}.

\subsection{Use of Modern Compute Hardware}
It is worth noting that this $\bigo{N^3}$ computational complexity is not necessarily insurmountable given the available computational hardware.
For example, some deep learning models require as many floating point operations (FLOPs) as similarly-sized GPs.
A 264-layer DenseNet model \cite{huang2017densely}, common on many computer vision tasks, requires $2.4 \times 10^{18}$ FLOPs to train $1.2$ million images.
This is essentially a cubic computational requirement, and would be wholly impractical if it were not for specialty compute hardware like GPUs.
Such a model would probably require months to train on standard CPUs, yet can be trained on 8 GPUs in a matter of hours \cite{howard2018training}.

Given the effectiveness of GPU acceleration on large neural networks, one might expect similar performance for large-scale Gaussian processes.
Unfortunately, many GP implementations rely on the Cholesky factorization (see \cref{sec:gp_models}), which does not benefit as readily from modern compute hardware.
GPUs are designed for massively-parallelizable operations such as matrix-multiplication---the primary numerical operation of neural networks.
A matrix-multiply between two $1,\!000 \times 1,\!000$ matrices is $10,\!000$ times faster on a GPU than on a CPU!\footnote{
	As measured on a NVIDIA 1080-TI GPU verses an 8-core Intel i7 CPU.
}
%which is why large neural networks are practical to train.
The Cholesky algorithm on the other hand is inherently sequential and affords minimal parallelization;
factorizing a $1,\!000 \times 1,\!000$ matrix is only $10$ times faster on GPU than on CPU.
This is why we cannot expect neural-network-level speedups for Cholesky-based GPs.
Moreover, the Cholesky factorization requires $\bigo{N^2}$ storage.
This amounts to a terabyte of memory for $N=1,\!000,\!000$---well beyond the capacity of most GPU clusters.

\subsection{Choosing Appropriate Approximations}
To reduce the computational and memory burden, researchers have proposed numerous methods that approximate Gaussian processes with simpler models.
Such models employ low-rank or structured approximations of the $N \times N$ matrices (see \cref{sec:approx_gps}).
Numerous advances have made these approximate methods more powerful while retaining compelling asymptotic complexities.

However, choosing a suitable approximation involves many design choices.
All approximate methods introduce hyperparameters that control the speed/accuracy trade-off, while also making assumptions that might not be well suited to certain datasets.
For example, variational approaches \cite{titsias2009variational}---which are a popular general-purpose approximation---tend to overestimate the observational noise, leading to worse predictive uncertainties \cite{turner2011two,bauer2016understanding}.
Structured interpolation methods \cite{wilson2015kernel} alleviate these biases, yet they are limited to low-dimensional problems with specific kernel functions.
While some theoretical guarantees can guide these design decisions \cite{burt2019rates}, choosing a good approximate model is ultimately dataset specific.
Good performance often requires a large hyperparameter sweep and expert knowledge.

\subsection{Implementation and Programmability}
One compelling advantage of neural networks is their modularity.
While creating novel architectures requires significant thought and experimentation, \emph{implementing} these architectures requires very little software engineering effort.
Seemingly complex models like DenseNets \cite{huang2017densely} and Transformers \cite{vaswani2017attention} have surprisingly simple implementations using compositional layers and sub-routines.
Small modifications, such as adding an additional output dimension, often require only a single additional line of code.

Gaussian processes models on the other hand require significant implementation effort.
Often, the \emph{model} and the \emph{learning/inference procedures} are tightly coupled.
As an example, consider converting a standard Gaussian process into a Gaussian process over multiple output dimensions \cite{bonilla2008multi}.
While this is seemingly a simple modelling change, it requires a completely different implementation since the additional output dimension changes the structure of the prior covariance matrix (see \cref{sec:programmability}), modifying the equations used for efficient inference.
In the popular \citet{gpy2014} software package, multi-output GPs and standard GPs are implemented as separate models, with multi-output GPs requiring an additional 100 lines of code.
Compared to the one-line change required for multi-output neural networks, GPs are significantly more difficult to implement.



\section{Outline of Contributions}
This thesis introduces a framework that addresses these issues without sacrificing the desirable properties of GPs.
Our approach is centered on a \emph{single} critical design decision:
taking inspiration from neural networks, we build GP training and inference algorithms \emph{using only matrix-multiplication} and element-wise operations.
As we will demonstrate, this single design choice reduces the asymptotic complexity of GPs, improves their GPU utilization, expands the applicability of exact methods, and simplifies implementation of specialty models.
The following chapters introduce the components of our matrix-multiplication-based framework:

\begin{itemize}
  \item In {\bf \cref{chapter:bbmm}}, we introduce the {\bf BlackBox~Matrix~$\times$~Matrix~(BBMM)} approach for training Gaussian process regression models.
    BBMM uses a modified version of preconditioned conjugate gradients (mPCG) that reduces GP training to a series of \emph{matrix-multiplications}.
    We demonstrate that this approach effectively uses GPU acceleration and is up to $30\times$ over existing inference methods.
    Additionally, we show that implementing specialty GP models with BBMM only requires writing an efficient kernel matrix-multiplication routine.
    %Whereas previous inference approaches require the user to provide routines for computing the full GP marginal log likelihood for a sufficiently complex model,
    %our framework only requires access to a blackbox routine that performs matrix-matrix multiplications with the kernel matrix and its derivative.
    %Accordingly, we refer to our method as BlackBox Matrix$\times$Matrix (BBMM) Inference.
    %In contrast to the Cholesky decomposition, which is at the heart of many existing inference engines, matrix-matrix multiplications fully utilize GPU acceleration.
    %We will demonstrate that this matrix-matrix approach also significantly eases implementation for a wide class of existing GP models from the literature.

  \item {\bf \cref{chapter:love}} extends BBMM to making predictions with Gaussian processes.
    Computing GP predictive distributions requires expensive computations involving the training covariance matrix.
    We introduce an algorithm---{\bf LancZos~Variance~Estimates~(LOVE)}---that efficiently pre-computes these training-data specific terms.
    As with BBMM training, LOVE relies entirely on \emph{matrix-multiplication}, which is especially beneficial for models with fast kernel routines.
    After the LOVE precomputation, computing a GP prediction is \emph{linear} in the amount of training data, or $\bigo{1}$ time if used in conjunction with structured kernel interpolation \cite{wilson2015kernel}.

  \item {\bf \cref{chapter:variational}} extends the BBMM framework to Gaussian process models with non-Gaussian likelihood functions---i.e. when GPs are used to model heavy-tailed data, arrival processes, or classification problems.
    Unlike standard regression, these models necessitate the use of approximate Bayesian inference.
    We introduce a matrix-multiplication method based on {\bf Cauchy Integral Quadrature (CIQ)} which can be used to optimize a re-parameterized variational training objective.
    On several large-scale spatial datasets, CIQ enables faster optimization and higher-fidelity approximations than existing variational methods.
    We also demonstrate that CIQ can be used to efficiently sample from GP posteriors.

  \item This thesis culminates with with {\bf \cref{chapter:largeexact}}, which utilizes the prior chapter's methods to scale GP regression to extremely large datasets.
    Combining BBMM and LOVE with partitioned matrix-multiplication routines, we demonstrate that Gaussian processes can be trained \emph{without approximation} on datasets with \emph{over 1 million data points}.
    GPU-acceleration makes these large-scale GPs roughly as fast as approximate methods, despite their larger asymptotic complexity.
    We perform the first-ever comparison of exact GPs against scalable approximations on datasets with $10^4$---$10^6$ data ps oints, showing dramatic performance improvements.

\end{itemize}

\noindent
Finally, we package together these controbutions into {\bf GPyTorch},\footnote{
  \url{http://gpytorch.ai}
}
an open-source implementation of BBMM, LOVE, and CIQ.
GPyTorch can be used to build small-scale or large-scale GPs with modular neural-network-like building blocks.
Moreover, the package seamlessly integrates with PyTorch \cite{paszke2019pytorch}, Pyro \cite{bingham2019pyro}, and BoTorch \cite{balandat2019botorch} to combine GPs with neural networks, probablistic models, and blackbox optimizers.
Throughout this thesis, we will discuss how the various algorithms (BBMM, LOVE, and CIQ) are implemented in GPyTorch, and how a practitioner can build on top of them to develop novel GP models.

We begin with a brief overview of Gaussian process models, common kernel functions, and scalable GP approximations.
Additionally, we will introduce Krylov-subspace methods---a family of numerical algorithms that compute complex matrix functions through matrix-vector products.
These methods form the foundation of our matrix-multiplication-based GP inference.

% In fact, we are not simply going to take inspiration from Neural networks, we are going to directly exploit those tools

%\paragraph{Preventing catastrophic failure.}

%\paragraph{Improved predictive pipelines.}

%\paragraph{Detecting anomalous inputs.}
%Machine learning models will only generalize to data that are sufficiently similar to the training data.
%If a model encounters \emph{out-of-distribution} (OOD) inputs -- inputs that deviate from the distribution of training data -- its predictions are likely to be erroneous or nonsensical \cite{begoli2019uncertainty,jiang2012calibrating}.
%%This may occur if the model is used in scenarios that experience covariate shift \cite{sugiyama2007covariate} or if the model encounters previously-unseen categories of data \cite{yu2017occ,hassen2018openset}.
%%Such scenarios are examples of \emph{out-of-distribution} (OOD) inputs.
%This phenomenon is illustrated in \cref{fig:ood_teaser}, which displays predictions from a neural network trained to predict prices of middle-class houses in Kentucky.
%The model is able to make sensible predictions on other Kentucky houses (left and center-left images).
%At the same time, the model vastly underestimates the price of a California mansion (center-right) and predicts and absurdly large price for a chair (right), as these inputs are not similar to any of the training set inputs.
%This is because the range of predicted prices for the out-of-distribution matches the range of Kentucky housing prices.
%A practitioner would see that the predictions are well within the model's expected output values and would be unaware that these predictions are nonsensical given the supplied inputs.

%Well-modeled uncertainty estimates can to identify potentially anomalous data and prevent such erroneous predictions.
%In this example, \gp{finish}

%\paragraph{A principled exploration/exploitation tradeoff.}

%\paragraph{Interpretability and trustworthiness.}
%Good uncertainty estimates can provide a valuable extra bit of information to users of machine learning models when predictions may otherwise be difficult to interpret.
%As machine learning algorithms become increasingly complex, they also appear more ``black box'' to users of such systems.
%This presents several challenges, especially for models that are used to aid human decision makers in domains such as medicine, finance, and policy \gp{cite}.

%For such circumstances it is therefore desirable for predictions to be understandable or interpretable \gp{cite LIME, saliency, etc.}.
%There are many definitions for what constitutes a good ``explanation'' of black-box predictions, coming from policy makers \gp{cite gdpr, etc} and the research community \gp{cite} alike.
%Though there are disagreements between these various sources, a well-calibrated uncertainty estimate is typically seen as a bare-minimum requirement for an interpretable prediction \gp{cite}.
%Most humans -- even if they are unable to perform simple inferences \cite{gigerenzer2003simple} -- have natural intuition for interpreting confidence estimates as event-occurrence frequencies \cite{cosmides1996humans,hoffrage1998using}.
%Therefore, well-calibrated confidence estimates from ML models can be easily interpreted by users.
%Moreover, the presence of uncertainty estimates can affect a user's trust in a machine learning model.
%In a study by \citet{zhou2017effects}, humans were asked to plan a budget for construction tasks based on information provided by a machine learning model.
%Some participants received both predictions and confidence intervals from the machine learning model, while other users only received the predictions.
%On tasks with low-to-moderate cognitive overhead, participants who saw uncertainty scores reported higher levels of trust in the machine learning model.
