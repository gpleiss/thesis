\section{Programmability with BBMM}
\label{sec:programmability}
We have discussed how the BBMM framework is more hardware efficient than existing training methods and avoids numerical instabilities with Lanczos.
Another key advantage of BBMM is that it can easily be adapted to complex GP models or structured GP approximations.

Indeed BBMM is \emph{blackbox} by nature, only requiring a routine to perform matrix-multiplications with the kernel matrix and its derivative.
Here we provide examples of how existing GP models and scalable approximations can be easily implemented in this framework.
The implementations of many specialty models require at most \emph{50 lines of Python code}.

\subsection{GPyTorch's {\tt LazyTensor} Construct}
In GPyTorch, we use the construct of a {\tt LazyTensor} object (or lazily-evaluated tensor) to represent kernel matrices $\bK_{\bX\bX}$.
A {\tt LazyTensor} represents a (potentially) structured matrix $\bK \in \reals^{N \times N}$ that is defined through some $\leq \bigo{N^2}$ representation $\br$.
As an example, a diagonal matrix $\bK$ can be represented by its diagonal vector.
Consequentially, the representation $\br$ for the {\tt DiagLazyTensor} class is simply a $\reals^N$ vector of diagonal entries.

Each {\tt LazyTensor} sub-class defines a {\tt \_matmul($\cdot$)} method (for performing $\bK \left( \bB \right)$ for some matrix $\bB$) and a {\tt \_deriv($\cdot,\cdot$)} method for computing $\partial \tr{ \bA^\top \bK \bB }/{\partial \br}$,
where $\bA$ and $\bB$ are arbitrary matrices.
The {\tt \_matmul($\cdot$)} method is used by mBCG, exploiting any structure of the {\tt LazyTensor} for fast matrix multiplication.
The {\tt \_deriv($\cdot,\cdot$)} function can be used to compute an unbiased estimate of \cref{eqn:log_lik_deriv}:
\begin{itemize}
  \item The $\frac{1}{T}\sum_{i=1}^{T}\left(\bz^{(i)^\top} \trainK^{-1}\right)\left(\frac{\partial \trainK}{\partial \btheta}\bz^{(i)} \right)$ term,
    which approximates the trace term in \cref{eqn:log_lik_deriv},
    can be rewritten as
    \[
      \frac{1}{T} \:\: \frac{ \partial \tr{ (\bZ^\top \trainK^{-1}) \trainK (\bZ) }}{ \partial \br } \:\: \frac{ \partial \br }{\partial \btheta},
    \]
    which can be computed by calling {\tt \_deriv($\trainK^{-1} \bZ$, $\bZ$)}.

  \item The $(\by^\top \trainK^{-1}) \frac{\partial \trainK}{\partial \btheta} (\trainK^{-1} \by)$ term in \cref{eqn:log_lik_deriv}
    can be rewritten as
    \[
      \frac{\partial (\by^\top \trainK^{-1}) \trainK (\trainK^{-1} \by) }{\partial \br} \:\: \frac{\partial \br}{\partial \btheta},
    \]
    which can be computed by calling {\tt \_deriv($\trainK^{-1} \by$, $\trainK^{-1} \by$)}.
\end{itemize}
%
While all {\tt LazyTensor} subclasses define {\tt \_matmul($\cdot$)} and {\tt \_deriv($\cdot$, $\cdot$}),
the superclass defines additional methods necessary for GP training, such as
%
\begin{itemize}
  \item {\tt inv\_matmul($\cdot$)} (for computing matrix solves $\trainK^{-1} \by$)
  \item {\tt logdet()} (for computing the log determinant $\log \vert \trainK \vert$)
  \item {\tt inv\_quad\_logdet($\cdot$)} (for simultaneously computing $\by^\top \trainK^{-1} \by$ and $\log \vert \trainK \vert$).
\end{itemize}
%
Each of these methods runs the mBCG algorithm using the subclass' {\tt \_matmul($\cdot$)} method.
Moreover, each method uses {\tt \_deriv($\cdot$,$\cdot$)} in conjunction with PyTorch's autograd \cite{paszke2017automatic} to compute the appropriate derivatives.
In GPyTorch, we use the {\tt inv\_quad\_logdet($\cdot$)} method to compute \cref{eqn:log_lik}---as it can compute all log likelihood and derivative terms with a single mBCG call.

For standard GP regression (with no approximations), there is no exploitable structure for the kernel matrix.
We therefore use the aptly named {\tt NonLazyTensor}, which simply represents an arbitrary matrix---i.e. $\br = \bK_{\bX\bX}$.
The {\tt \_matmul($\cdot$)} and {\tt \_deriv($\cdot$, $\cdot$)} routines are straightforward:
%
\begin{minted}{python3}
class NonLazyTensor(LazyTensor):
    def __init__(self, matrix):
        self.matrix = matrix

    def _matmul(self, B):
        return self.matrix @ B

    def _deriv(self, A, B):
        # d(tr(A^T K B)) / d(K) = A B^T
        return A @ B.transpose(-1, -2)
\end{minted}
%
\noindent
For a full GP code example that uses {\tt LazyTensors} under the hood, see \cref{app:standard_gp_example} or \url{http://github.com/cornellius-gp/gpytorch}.


\subsection{Examples of {\tt LazyTensor}s and Specialty GP Models}
\paragraph{Bayesian linear regression} can be viewed as GP regression with the special kernel matrix
\[
  \bK_{\bX\bX}^{(\text{lin})} = \bX\bX^{\top}.
\]
A matrix multiply with this kernel against an $N \times T$ matrix $\bB$, $(\bX\bX^{\top})\bV$ requires $\bigo{TND}$ time.
Therefore, BBMM requires $\bigo{JTND}$ time, and is exact in $\bigo{TND^2}$ time.
This running time complexity matches existing efficient algorithms for Bayesian linear regression, \emph{with no additional derivation}.

In GPyTorch's, the {\tt LinearKernel} class outputs a {\tt RootLazyTensor}:
%
\begin{minted}{python3}
class RootLazyTensor(LazyTensor):
    """ The output of gpytorch.kernels.LinearKernel """
    def __init__(self, X):
        self.X = X  # X is a (N X D) matrix

    def _matmul(self, B):
        # If B is a (N X T) matrix, this is O(T N D)
        return (self.X @ self.X.transpose(-1, -2) @ B

    def _deriv(self, A, B):
        # d(tr(A^T K B)) / d(X) = A B^T X + B A^T X
        # if A and B are (N X T), this is O(T N D)
        return A @ B.transpose(-1, -2) @ X + \
            B @ A.transpose(-1, -2) @ X
\end{minted}
%
\noindent
Note that the {\tt RootLazyTensor}'s {\tt \_matmul($\cdot$)} function encodes the efficient MVM necessary for $\bigo{JTND}$ inference.

\paragraph{Multi-task Gaussian processes} \cite{bonilla2008multi} use a kernel function that combines covariance between inputs $\bx, \bx'$ and covariance between tasks $c, c'$:
$k^{(\text{input})} \left( \bx, \bx' \right) \: k^{(\text{task})} \left( c, c' \right)$.
Constructing the kernel matrix over all training inputs $\bX$ and tasks $[1, C]$ results in Kronecker structure:
%
\[
  \bK_{\bX\bX, \bc\bc}^{(\text{multi})} = \bK^{(\text{task})}_{\bX\bX} \otimes \bK^{(\text{task})}_{\bc\bc},
\]
%
where $\otimes$ represents the Kronecker product.
Though $\bK_{\bX\bX, \bc\bc}^{(\text{multi})}$ is a $(NC) \times (NC)$ matrix (for $C$ tasks), exploiting the Kronecker structure results in $\bigo{N C^2 + N^2 C}$ matrix-vector multiplications.
(This can be reduced further if $\bK^{(\text{input})}_{\bX\bX}$ is structured and affords fast multiplications.)
Thus, multi-task Gaussian processes can be implemented simply by using a Kronecker matrix multiplication routine, with a time complexity that matches state-of-the-art \citep{bonilla2008multi}.

\emergencystretch 2em
The {\tt MultitaskKernel} object in GPyTorch returns a {\tt KroneckerProductLazyTensor}.
(The forward and backward pass methods are adopted from \citet{saatcci2012scalable}.)
Here, we assume that the $\bK^{(\text{input})}_{\bX\bX}$ and $\bK^{(\text{task})}_{\bc\bc}$ matrices are themselves represented as {\tt LazyTensors}.
%
\begin{minted}{python3}
class KroneckerProductLazyTensor(LazyTensor):
    """ The output of gpytorch.kernels.MultitaskKernel """
    def __init__(self, K_input, K_task):
        self.K_input = K_input  # a (N X N) LazyTensor
        self.K_task = K_task  # a (C X C) LazyTensor

    def _matmul(self, B):
        # Kronecker prod, adapted from Saatci, 2012
        # O(N C^2 + N C^2), less if inputs are structured
        # B is (CN X T)
        out = B.clone().view(self.K_task.size(-1), -1)  # C X NT
        out = self.K_task._matmul(out)
        out = out.view(self.K_task.size(-1), -1, B.size(-1))
        out = out.transpose(-3, -2)  # N X C X T
        out = out.view(self.K_input.size(-1), -1)  # N X CT
        out = self.K_input._matmul(out)
        out = out.view(-1, B.size(-1))  # CN X T
        return out

    def _deriv(self, A, B):
        # Kronecker deriv
        # O(N C^2 + N C^2), less if inputs are structured
        # See Saatci, 2012, or github.com/cornellius-gp/gpytorch
\end{minted}
%
\noindent
Importantly, implementing multitask GPs in GPyTorch simply requires changing the kernel module (which changes the {\tt LazyTensor})---no additional implementation is required.
This is demonstrated by the code example in \cref{app:multitask_gp_example}.

\paragraph{Compositions of kernels} can often be handled automatically.
For example, given a BBMM routine for $\bK_{1},\bK_{2},\bK_{3}$, we can automatically perform $(\bK_{1}\bK_{2}+\bK_{3}) \bB = \bK_{1}(\bK_{2} \bB ) + \bK_{3} \bB$.
To handle the sum of two kernels, we can define a {\tt SumLazyTensor} that takes two {\tt LazyTensor}s and distributes their matrix-multiplication.
For the product of kernels, one can use the method of \citet{gardner2018product}.
%
\begin{minted}{python3}
class SumLazyTensor(LazyTensor):
    def __init__(self, *Ks):
        self.Ks = Ks  # list of (N X N) LazyTensors

    def _matmul(self, B):
        return sum(K._matmul(B) for K in self.Ks)

    def _deriv(self, A, B):
        return [K._deriv(A, B) for K in self.Ks]
\end{minted}

\paragraph{Sparse Gaussian Process Regression (SGPR)}
\cite{titsias2009variational} and many other sparse GP techniques \cite{quinonero2005unifying,snelson2006sparse,hensman2013gaussian} use the subset of regressors (SoR) approximation for the kernel (see \cref{sec:approx_gps}):
\[
  \bK_{\bX\bX}^{(\text{SGPR})} = \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX}.
\]
Performing a matrix-matrix multiply with this matrix requires $\bigo{TNM + TM^{3}}$ time by distributing the vector multiply and grouping terms correctly.
This computation is \emph{asymptotically faster} than the $\bigo{NM^{2} + M^{3}}$ time required by Cholesky based inference.
Augmenting the SoR approximation with a diagonal correction, e.g. as in FITC \cite{snelson2006sparse}, is similarly straightforward.
(We omit the code example here: see \url{http://github.com/cornellius-gp/gpytorch}.)

\paragraph{KISS-GP}
\cite{wilson2015kernel}---see \cref{sec:kissgp}---also known as SKI, is an inducing point method designed to provide fast matrix vector multiplies (MVMs) for use with Krylov subspace methods.
KISS-GP is thus a natural candidate for BBMM and can benefit greatly from hardware acceleration.
The KISS-GP approximation applied to the training covariance matrix gives us
\[
  \bK_{\bX\bX}^{(\text{KISS-GP})} = \bW_\bX^\top \bK_{\bZ\bZ} \bW_{\bX}.
\]
where $\bW$ is a $\bigo{N}$ sparse interpolation matrix and $\bK_{\bZ\bZ}$ has Toeplitz structure for $\bigo{M \log M}$ matrix-vector multiplies.
Thus KISS-GP provides $\bigo{TN \! + \! TM \log M}$ matrix-matrix multiplies.
(We omit the code example here.)



\subsection{{\tt LazyTensor}s and Pivoted Cholesky Preconditioning}
In order to compute the partial pivoted Cholesky preconditioner for arbitrary {\tt LazyTensor}s, we need routines for
\begin{enumerate*}
  \item computing the row of a matrix, and
  \item computing the matrix diagonal.
\end{enumerate*}
%
Each {\tt LazyTensor} can optionally implement its own {\tt \_get\_row($\cdot$)} method; however, a simple default implementation uses the {\tt \_matmul} function:
$
  \bk^{(i)} = \bK_{\bX\bX} \be_i
$
($\be_i$ is the $i^\text{th}$ unit vector).
There is no standard default for the {\tt diagonal()} function; however, they tend to be straightforward to write.
We include some examples for various {\tt LazyTensor} sub-classes below.
%
\begin{minted}{python3}
# For NonLazyTensor
def diagonal(self):
    return K.diagonal(dim1=-1, dim2=-2)

# For RootLazyTensor
def diagonal(self):
    # diag(X X^T) = (X \cdot X) e_1
    # where \cdot is elementwise multiplication
    return self.X.pow(2).sum(dim=-1)

# For KroneckerProductLazyTensor
def diagonal(self):
    # A neat trick: the KP-diagonal is a flattened
    # outer product of the K_input/K_task diagonals
    return torch.ger(
        self.K_input.diagonal(), self.K_task.diagonal(),
    ).view(-1)

# For SumLazyTensor
def diagonal(self):
    return sum(K.diag for K in self.Ks)
\end{minted}
