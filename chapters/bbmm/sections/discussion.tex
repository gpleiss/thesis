%!TEX root=main_arxiv.tex
\section{Discussion}
In this chapter, we have presented a novel algorithm for Gaussian process training (BBMM) based on blackbox matrix-matrix multiplication routines with kernel matrices.
Below we discuss our findings and discuss extensions of BBMM that will be explored in future chapters.

\paragraph{Avoiding the Cholesky decomposition.}
An important takeaway of this chapter is that it is beneficial to avoid the Cholesky decomposition for GP training, even when no structured approximations are made.
We will explore the exact GP setting in more detail in \cref{chapter:largeexact} and further demonstrate the computational benefits of BBMM.

The Cholesky decomposition performs a large amount of computation to get a linear solve when fast iterative methods suffice.
Ultimately, the Cholesky decomposition of a full matrix takes $\bigo{N^3}$ time while CG takes $\bigo{N^2}$ time.
Indeed, as shown in \cref{fig:cg_error}, CG may even provide \emph{better} linear solves than the Cholesky decomposition.

\paragraph{Non-Gaussian likelihoods.}
When GP models are used with non-conjugate likelihoods (e.g. for classification or heavy-tailed noise models), we cannot compute the GP's marginal log likelihood (i.e. \cref{eqn:log_lik,eqn:log_lik_deriv} do not apply).
We must instead use variational approximations of the marginal log likelihood, which require a different set of training and inference equations.
We will discuss how MVM techniques can be applied to this problem in \cref{chapter:ciq}.

\paragraph{Computing predictive distributions.}
The focus of this chapter has been optimizing the GP marginal log likelihood.
The next chapter will focus on making predictions after a GP has been trained.
The equations for computing predictive distributions---\cref{eqn:predictive_mean,eqn:predictive_var}---require matrix solves.
In the next chapter, we will introduce a MVM based method that \emph{pre-computes and caches} most of the required CG computation.
