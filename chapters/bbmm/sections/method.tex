%!TEX root=main.tex
%\newpage
\section{Gaussian process inference through blackbox matrix multiplication}

\label{sec:method}
The goal of our paper is to replace existing inference strategies with a unified framework that utilizes modern hardware efficiently.
We additionally desire that complex GP models can be used in a blackbox manner without additional inference rules. To this end,
our method reduces the bulk of GP inference to one of the most efficiently-parallelized computations: \emph{matrix-matrix multiplication}.
We call our method BlackBox Matrix$\times$Matrix inference (BBMM) because it only requires a user to specify a matrix multiply routine for the kernel $\trainK M$ and its derivative $\frac{d\trainK}{d\theta}M$.

\paragraph{Required operations.}
An \emph{inference engine} is a scheme for computing all the equations discussed above: the predictive distribution~\eqref{eq:pred_mean_covar}, the loss, and its derivative~\eqref{eq:log_lik_and_deriv}.
These equations have three operations in common that dominate its time complexity:
1) the linear solve $\trainK^{-1}\by$,
2) the log determinant $\log \vert \trainK \vert$,
and 3) a trace term $\tr{\trainK^{-1}\frac{d\trainK}{d\theta}}$.
In many implementations, these three quantities are computed using the Cholesky decomposition of $\trainK$,
which is computationally expensive, requiring $\bigo{n^3}$ operations, and does not effectively utilize parallel hardware.

Recently, there is a growing line of research that computes these operations with iterative routines based on matrix-vector multiplications (MVMs).
$\trainK^{-1}\by$ can be computed using \emph{conjugate gradients} (CG) \cite{cunningham2008fast,cutajar2016preconditioning,saatcci2012scalable,wilson2015kernel},
and the other two quantities can be computed using calls to the iterative Lanczos tridiagonalization algorithm \cite{ubaru2017fast,dong2017scalable}.
MVM-based methods are asymptotically faster and more space efficient than Cholesky based methods \cite{wilson2015kernel,dong2017scalable}.
Additionally, these methods are able to exploit algebraic structure in the data for further efficiencies \cite{cunningham2008fast,saatcci2012scalable,wilson2015kernel}.
However, they also have disadvantages.
The quantities are computed via several independent calls to the CG and stochastic Lanczos quadrature subroutines, which are inherently sequential and therefore do not fully utilize parallel hardware. Additionally, the Lanczos tridiagonalization algorithm requires $\bigo{np}$ space for $p$ iterations and suffers from numerical stability issues due to loss of orthogonality \cite{golub2012matrix}.

\paragraph{Modified CG.}
Our goal is to capitalize on the advantages of MVM-based methods (space-efficiency, ability to exploit structure, etc.) but with efficient routines that are optimized for modern parallel compute hardware.
For this purpose, our method makes use of a \emph{modified Batched Conjugate Gradients Algorithm} (mBCG) algorithm.
Standard conjugate gradients takes as input a vector $\by$ and a routine for computing a matrix vector product $\trainK\by$, and, after $p$ iterations, outputs an approximate solve $\bu_{p} \approx \trainK^{-1}\by$ (with exact equality when $p = n$).
We modify conjugate gradients to (1) perform linear solves with multiple right hand sides simultaneously, and (2) return tridiagonal matrices corresponding to partial Lanczos tridiagonalizations of $\trainK$ with respect to each right hand side.\footnote{
  mBCG differes from Block CG algorithms \cite{o1980block} in that mBCG returns Lanczos tridiagonalization terms.
} Specifically, mBCG takes as input a matrix $\left[\begin{array}{cccc}\by & \bz_{1} & \cdots & \bz_{t}\end{array}\right]$, and outputs:
\begin{equation}
  \label{eq:mod_cg_call}
  \left[\begin{array}{cccc}\bu_{0} & \bu_{1} & \cdots & \bu_{t}\end{array}\right] = \trainK^{-1}\left[\begin{array}{cccc}\by & \bz_{1} & \cdots & \bz_{t}\end{array}\right]\;\;\;\;\textrm{and}\;\;\;\; \tilde{T}_{1},...,\tilde{T}_{t}
\end{equation}
where $\tilde{\bT}_{1},\ldots,\tilde{\bT}_{t}$ are partial Lanczos tridiagonalizations of $\trainK$ with respect to the vectors $\bz_{1},\ldots,\bz_{t}$, which we describe shortly.
In what follows, we show how to use a single call to mBCG to compute the three GP inference terms: $\trainK^{-1} \by$, $\tr{ \trainK^{-1} \frac{\partial \trainK}{\partial \theta} }$, and $\log \vert \trainK \vert$.
%
$\trainK^{-1}\by$ is equal to $\bu_{0}$ in~\eqref{eq:mod_cg_call}, directly returned from mBCG. We describe the other two terms below.
%
\input algorithms/mbcg

\paragraph{Estimating $\tr{ \trainK^{-1} \frac{\partial \trainK}{\partial \theta} }$}
from CG relies on \emph{stochastic trace estimation} \cite{avron2011randomized,fitzsimons2016improved,hutchinson1990stochastic}, which allows us to treat this term as a sum of linear solves.
\gp{Reference background}
Given i.i.d. random variables $\bz_1, \ldots, \bz_t$ so that $\Ev{\bz_i}=0$ and $\Ev{\bz_i \bz_i^{\top}}=I,
$ (e.g., $\bz_{i} \sim \mathcal{N}(0, I)$)
the matrix trace $\tr{\bA}$ can be written as
$
  \tr{\bA} = \Ev{\bz_i^\top \bA\bz_i}
$
, such that
%
\begin{equation}
  \label{eq:trace_deriv_estimate}
  \tr{\trainK^{-1}\frac{d\trainK}{d\theta}}  = \Ev{\bz_i^\top  \trainK^{-1}\frac{d\trainK}{d\theta} \bz_i} \approx \frac{1}{t}\sum_{i=1}^{t}\left(\bz_{i}^\top \trainK^{-1}\right)\left(\frac{d\trainK}{d\theta}\bz_{i} \right)
\end{equation}
%
is an unbiased estimator of the derivative. This computation motivates the $\bz_1, \ldots, \bz_t$ terms in \eqref{eq:mod_cg_call}:
the mBCG call returns the solves $\trainK^{-1}[\bz_1 \ldots \bz_t]$, which yields $\mathbf{u}_i=\bz_{i}^\top \trainK^{-1}$ . A single matrix multiply with the derivative $\frac{d\trainK}{d\theta}[\bz_1 \ldots \bz_t]$ yields the remaining terms on the RHS. The full trace can then be estimated by elementwise multiplying these terms together and summing, as in
\eqref{eq:trace_deriv_estimate}.

\paragraph{Estimating $\log \vert \trainK \vert$}
\gp{Reference background}
can be accomplished using the $\bT_{1},...,\bT_{t}$ matrices from mBCG. If $\trainK=\bQ\bT\bQ^{\top}$, with $\bQ$ orthonormal, then because $\trainK$ and $\bT$ have the same eigenvalues:
\begin{equation}
  \log \vert \trainK \vert = \tr{\log \bT}  = \Ev{\bz_i^\top (\log \bT)\bz_i} \approx \sum_{i=1}^{t}\bz_{i}^\top \left( \log \bT \right) \bz_{i}\label{eq:logdetKQ}
\end{equation}
where $\log \bT$ here denotes the matrix logarithm, and the approximation comes from the same stochastic trace estimation technique used for \eqref{eq:trace_deriv_estimate}. One approach to obtain a decomposition $\trainK=\bQ\bT\bQ^{\top}$ is to use the \emph{Lanczos tridiagonalization algorithm}. This algorithm takes the matrix $\trainK$ and a probe vector $\bz$ and outputs the decomposition $\bQ\bT\bQ^{\top}$ (where $\bz$ is the first column of $\bQ$). However, rather than running the full algorithm, we can instead run $p$ iterations of the algorithm $t$ times, each with a vector $\bz_{1},...,\bz_{t}$ to obtain $t$ decompositions  $\tilde{\bQ}_{1}\tilde{\bT}_{1}\tilde{\bQ}_{1}^{\top},...,\tilde{\bQ}_{t}\tilde{\bT}_{t}\tilde{\bQ}_{t}^{\top}$ with $\tilde{\bQ}_{i} \in \reals^{n \times p}$ and $\tilde{\bT}_{i} \in \reals^{p \times p}$. We can use these partial decompositions to estimate~\eqref{eq:logdetKQ}:
\begin{equation}
  \Ev{\bz_i^\top (\log \bT)\bz_i} = \Ev{\bz_i^\top  \tilde{\bQ}_{i}(\log \tilde{\bT}_{i}) \tilde{\bQ}_{i}^{\top}\bz_{i}} \approx \frac{1}{t} \sum_{i=1}^{t} \bz_i^{\top} \tilde{\bQ}_{i}(\log \tilde{\bT}_{i})\tilde{\bQ}_{i}^{\top}\bz_{i} = \frac{1}{t} \sum_{i=1}^{t}e_{1}^{\top}(\log \tilde{\bT}_{i})e_{1},
  \label{eq:slq}
\end{equation}
where $e_{1}$ is the first row of the identity matrix. Running Lanczos with a starting vector $\bz_{i}$ ensures that all columns of $\tilde{\bQ}_{i}$ are orthogonal to $\bz_{i}$ except the first, so $\tilde{\bQ}_{i}\bz_{i} = e_{1}$ \cite{dong2017scalable,ubaru2017fast,golub2009matrices}.

In mBCG, we adapt a technique from \citet{saad2003iterative} which allows us to compute $\tilde{\bT}_{1},\ldots,\tilde{\bT}_{t}$ corresponding to the input vectors $\bz_{1},\ldots,\bz_{t}$  to mBCG from the coefficients of CG in $\bigo{1}$ additional work per iteration.
This approach allows us to compute a log determinant estimate identical to \eqref{eq:slq} \emph{without running the Lanczos algorithm}.
Thus we avoid the extra computation, storage, and numerical instability associated with Lanczos iterations.
We describe the details of this adaptation in \cref{app:mod_cg}.

\paragraph{Runtime and space.}
As shown above, we are able to approximate all inference terms from a single call to mBCG.
These approximations improve with the number of mBCG iterations.
Each iteration requires one matrix-matrix multiply with $\trainK$, and the subsequent work to derive these inference terms takes negligible additional time (\cref{app:method_runtime}).
Therefore, $p$ iterations of mBCG requires $\bigo{nt}$ space (see \cref{app:method_runtime}) and $\bigo{p \: \mmm{\trainK}}$ time,
where $\mmm{\trainK}$ is the time to multiply $\trainK$ by a $n \times t$ matrix.
This multiplication takes $\bigo{n^2 t}$ time with a standard matrix.
It is worth noting that this is a lower asymptotic complexity that standard Cholesky-based inference, which is $\bigo{n^3}$.
Therefore, BBMM offers a computational speedup for exact GP inference.
As we will show in \cref{sec:advantages}, this time complexity can be further reduced with structured data or sparse GP approximations.
