%!TEX root=main.tex
%\newpage
\section{Gaussian Process Training Through Matrix Multiplication}

\label{sec:bbmm_method}
The goal of this chapter is to replace existing training strategies with a unified framework that utilizes modern hardware efficiently.
We additionally desire that complex GP models can be used in a blackbox manner without additional training rules.
To this end, our method reduces the bulk of GP inference to one of the most efficiently-parallelized computations: \emph{matrix-matrix multiplication}.
We call our method BlackBox Matrix$\times$Matrix inference (BBMM) because it only requires a user to specify a matrix multiply routine for the kernel $\trainK ( \cdot )$ and its derivative $\frac{\partial \trainK}{\partial \btheta} ( \cdot )$.

\paragraph{Required operations.}
To train a GP we must compute the marginal log likelihood~(\cref{eqn:log_lik}) and its derivative~(\cref{eqn:log_lik_deriv}).
We rewrite the equations here, assuming a prior mean of zero for brevity:
\begin{align*}
  -\log p( \by \mid \bX, \btheta)
  &\propto \log \left \vert \trainK \right \vert + \by^{\top} \trainK^{-1} \by,
  \\
  \frac{\partial - \log p( \by \mid \bX, \btheta )}{\partial \btheta}
  &\propto
   \tr{\trainK^{-1}\frac{\partial \trainK}{\partial \btheta}} -
	\by^{\top} \trainK^{-1}\frac{\partial \trainK}{\partial \btheta}\trainK^{-1} \by,
\end{align*}
where again $\trainK$ is the training kernel matrix plus observational noise ($\trainK = \bK_{\bX\bX} + \sigma^2_\text{obs} \bI$).
These equations have three operations that dominate their time complexity:
\begin{enumerate}
  \item the linear solve $\trainK^{-1}\by$,
  \item the log determinant $\log \vert \trainK \vert$, and
  \item a trace term $\tr{\trainK^{-1}\frac{\partial \trainK}{\partial \btheta}}$.
\end{enumerate}

\paragraph{The Cholesky decomposition} is used in many GP implementations to compute these three terms.
This procedure factorizes $\trainK$ as $\bL \bL^\top$, where $\bL$ is lower triangular.
The columns of $\bL = [ \bl^{(1)}, \:\: \ldots, \:\: \bl^{(N)} ]$ are computed recursively \citep{golub2012matrix}.
Computing $\bL$ requires $\bigo{N^3}$ time and $\bigo{N^2}$ memory.
After computing this factorization, matrix solves and log determinants take $\bigo{N^2}$ and $\bigo{N}$ time respectively.
In general, this asymptotic complexity cannot be reduced even if $\trainK$ has nice structure (e.g. Toeplitz).
Although concurrent work by \citet{nguyen2019exact} uses the Cholesky decomposition for large scale GP inference through distributed computing, it requires quadratic communication costs and quadratic memory.
Furthermore, its recursive nature makes the Cholesky algorithm less amenable to GPU acceleration since GPUs are designed to parallelize matrix-vector multiplications.

\paragraph{MVM-based training methods.}
Recently, there is a growing line of research that computes these operations with iterative routines based on matrix-vector multiplications (MVMs).
As described in \cref{sec:mvms},
$\trainK^{-1}\by$ can be computed using \emph{conjugate gradients} (CG) \cite{cunningham2008fast,cutajar2016preconditioning},
and the other two quantities can be computed using the Lanczos tridiagonalization algorithm \cite{ubaru2017fast,dong2017scalable}.
These MVM-based methods are asymptotically faster and more space efficient than Cholesky based methods \cite{wilson2015kernel,dong2017scalable}.
Additionally, these methods are able to exploit algebraic structure in the data for further efficiencies \cite{cunningham2008fast,saatcci2012scalable,wilson2015kernel}.
We aim to expand on these methods.
In particular, our goals are to
\begin{enumerate}
  \item introduce a unified MVM algorithm that simultaneously computes all terms and thus improve parallelization/GPU utilization; and
  \item avoid using Lanczos tridiagonalization for the second two terms, as it suffers from numerical instabilities \cite{golub2012matrix}.
\end{enumerate}

\subsection{Modified Batched Conjugate Gradients (mBCG)}
\label{sec:mbcg}

For this purpose, we introduce a \emph{modified Batched Conjugate Gradients Algorithm} (mBCG) algorithm.
Standard conjugate gradients takes as input a vector $\by$ and a routine for computing a matrix vector product $\trainK\by$, and, after $J$ iterations, outputs an approximate solve $\bc_{J} \approx \trainK^{-1}\by$ (with exact equality when $J = N$).
We modify conjugate gradients to 1. perform linear solves with multiple right hand sides simultaneously, and 2. return tridiagonal matrices corresponding to partial Lanczos tridiagonalizations of $\trainK$ with respect to each right hand side.
%\footnote{
  %mBCG differes from Block CG algorithms \cite{o1980block} in that mBCG returns Lanczos tridiagonalization terms.
%}
Specifically, mBCG takes as input a matrix $\left[ \by, \:\: \bz^{(1)}, \:\: \cdots, \:\: \bz^{(T)} \right]$, and outputs:
\begin{equation}
  \label{eqn:mod_cg_call}
  \left[ \bc^{(0)}, \:\: \bc^{(1)}, \:\: \cdots, \:\: \bc^{(T)} \right] = \trainK^{-1} \left[ \by, \:\: \bz^{(1)}, \:\: \cdots, \:\: \bz^{(T)} \right], \quad \bT^{(1)}, \: \ldots, \: \bT^{(T)}
\end{equation}
where $\bT^{(1)}, \ldots, \bT^{(T)}$ are the partial Lanczos tridiagonalizations of $\trainK$ with respect to the vectors $\bz^{(1)}, \ldots, \bz^{(T)}$ (see \cref{sec:lanczos}).

\paragraph{Using mBCG for GP training.}
Before describing the details of the mBCG algorithm, we will first discuss how its outputs can be used to compute the three GP training terms:
$\trainK^{-1} \by$, $\tr{ \trainK^{-1} \frac{\partial \trainK}{\partial \btheta} }$, and $\log \vert \trainK \vert$.

\begin{enumerate}
  \item $\trainK^{-1}\by$ is equal to $\bc^{(0)}$ in~\cref{eqn:mod_cg_call}, directly returned from mBCG.

  \item $\tr{ \trainK^{-1} \frac{\partial \trainK}{\partial \btheta} }$ can be approximated using \emph{stochastic trace estimation} \cite{hutchinson1990stochastic,fitzsimons2016improved}, which treats this term as a sum of linear solves.
    Given i.i.d. random variables $\bz^{(1)}, \ldots, \bz^{(T)}$ so that $\Ev{\bz^{(i)}}=0$ and $\Ev{\bz^{(i)} \bz^{(i)^\top}}=\bI,
    $
    the matrix trace can be written as
    $
      \tr{\bA} = \Ev{\bz^{(i)^\top} \bA\bz^{(i)}}.
    $
    Thus,
    %
    \begin{align}
      \tr{\trainK^{-1}\frac{\partial \trainK}{\partial \btheta}} &= \Ev{\bz^{(i)^\top} \trainK^{-1} \frac{\partial \trainK}{\partial \btheta} \bz^{(i)}}
      \nonumber \\
      &\approx \frac{1}{T}\sum_{i=1}^{T}\left(\bz^{(i)^\top} \trainK^{-1}\right)\left(\frac{\partial \trainK}{\partial \btheta}\bz^{(i)} \right)
      \label{eqn:trace_deriv_estimate}
    \end{align}
    %
    is an unbiased estimator of the derivative. This computation motivates the $\bz^{(1)}, \ldots, \bz^{(T)}$ terms in \cref{eqn:mod_cg_call}:
    mBCG returns the solves $\trainK^{-1}[\bz^{(1)}, \:\: \ldots, \:\: \bz^{(T)}]$.
    A single matrix multiply with the derivative $\frac{\partial \trainK}{\partial \btheta}[\bz^{(1)}, \:\: \ldots, \:\: \bz^{(T)}]$ yields the remaining terms on the RHS.
    The full trace can then be estimated by elementwise multiplying these terms together and summing, as in \cref{eqn:trace_deriv_estimate}.

  \item $\log \vert \trainK \vert$
    can be estimated using the stochastic Lanczos quadrature routine of \citet{ubaru2017fast}, as described in \cref{sec:lanczos}.
    To briefly summarize, this approach approximates the matrix logarithm as $\left( \log \bA \right) \bz^{(i)} \approx \bQ^{(i)} \left( \log \bT^{(i)} \right) \bQ^{(i)^\top} \bz^{(i)}$,
    where $\bQ^{(i)}$ and $\bT^{(i)}$ are the orthogonal and tridiagonal matrices from Lanczos with initial vector $\bz^{(i)}$.
    Combining stochastic trace estimation with this approximation gives us
    %
    \begin{align*}
      \log \vert \trainK \vert = \tr{ \log \trainK }
      &\approx \frac{1}{T} \sum_{i=1}^T \bz^{(i)^\top} \bQ^{(i)} \left( \log \bT^{(i)} \right) \bQ^{(i)^\top} \bz^{(i)}
      \\
      &= \frac{1}{T} \sum_{i=1}^T \Vert \bz^{(i)} \Vert_2 \: \be^{(1)^\top} \left( \log \bT^{(i)} \right) \be^{(1)},
    \end{align*}
    %
    where $\be^{(1)} = [1, 0, \:\:, \ldots, \:\: 0]$.
    (The reduction in the second line comes from the orthogonality of $\bQ^{(i)}$ and $\bz^{(i)} / \Vert \bz^{(i)} \Vert_2$ is the first column of $\bQ^{(i)}$.)
    Therefore, we can estimate the log determinant of $\trainK$ simply by computing logarithms of the tridiagonal matrices returned by mBCG in \cref{eqn:mod_cg_call}.
\end{enumerate}
%
We note that our derivative and log determinant estimates are also proposed by \citet{cutajar2016preconditioning} and \citet{dong2017scalable}, respectively.
Notably, \citet{cutajar2016preconditioning} does not return a log determinant estimate and therefore their approach cannot be used for sampling $\btheta$ or Bayesian model selection.
We further differ from \citet{cutajar2016preconditioning} in that we use batched operations to compute all terms simultaneously.
We differ from \citet{dong2017scalable} by avoiding the explicit Lanczos tridiagonalization algorithm (\cref{alg:lanczos}) and thus circumventing its storage and numerical stability issues \cite{golub2012matrix}.

Now that we have motivated the terms produced by mBCG, we will present the algorithm itself.


\input algorithms/mbcg

\paragraph{The mBCG algorithm,} presented in \cref{alg:mod_pcg}, makes two changes to the standard conjugate gradients algorithm (\cref{alg:std_pcg}).
In particular, it performs multiple solves $\bA^{-1} \bB = \left[ \bA^{-1} \bb^{(0)}, \:\: \ldots, \:\:, \bA^{-1} \bb^{(T)} \right]$
simultaneously using {\bf matrix-matrix multiplication} (MMM), and it also returns Lanczos tridiagonalization matrices
$\bT^{(1)}, \: \ldots, \: \bT^{(T)}$
associated with each of the solves.

The majority of the lines in \cref{alg:mod_pcg} are direct adaptations of lines from \cref{alg:std_pcg} to handle multiple vectors simultaneously.
We denote these lines in {\color{\colormat} \colormat}.
For example, performing a matrix-matrix multiply $\bA \bB$ is equivalent to performing a matrix-vector multiply $\bA \bb^{(i)}$ for each column of $\bB$.
Thus we can replace multiple MVM calls with a single MMM call.
In standard PCG, there are two scalar coefficient used during each iteration $j$: $\alpha_j$ and $\beta_j$ (see \cref{alg:std_pcg}).
Note that each solve $\bc^{(0)}, \ldots, \bc^{(T)}$ in mBCG uses different scalar values.
Therefore, mBCG replaces the scalers with \emph{two coefficient vectors}: $\balpha_j \in \reals^{T+1}$ and $\bbeta_j \in \reals^{T+1}$, where each of the vector entries corresponds to a single solve.
There are two types of operations involving these coefficients:
%
\begin{enumerate}
  \item updates (e.g. {\color{\colormat} $\balpha_j$ $\gets$ ${( \bR_{j-1} \circ \bZ_{j-1} )^\top \mathbf 1}/{( \bD_{j-1} \circ \bV_{j} )^\top \mathbf 1}$}) and
  \item scalaing (e.g. {\color{\colormat} $\bU_j$ $\gets$ $\bU_{j-1} +$ \diag{$\balpha_{j}$} $\bD_{j-1}$}).
\end{enumerate}
%
The update rules are batched versions of the update rules in the standard CG algorithm.
For example:
%
\begin{equation*}
  \left[ \begin{array}{c}
    \alpha_j^{(0)}
    \\
    \vdots
    \\
    \alpha_j^{(T)}
  \end{array} \right]
  = \frac{( \bR_{j-1} \circ \bZ_{j-1} )^\top \mathbf 1}{( \bD_{j-1} \circ \bV_{j} )^\top \mathbf 1}
  = \left[ \begin{array}{c}
      \frac{\left( \br_{j-1}^{(0)} \: \circ \: \bz_{j-1}^{(0)} \right) \mathbf 1}
        {\left( \bd_{j-1}^{(0)} \: \circ \: \bv_{j}^{(0)} \right) \mathbf 1}
        \\
        \vdots
        \\
        \frac{\left( \br_{j-1}^{(T)} \: \circ \: \bz_{j-1}^{(T)} \right) \mathbf 1}
        {\left( \bd_{j-1}^{(T)} \: \circ \: \bv_{j}^{(T)} \right) \mathbf 1}
     \end{array} \right]
  = \left[ \begin{array}{c}
      \frac{\left( \br_{j-1}^{(0)\top} \bz_{j-1}^{(0)} \right)}
        {\left( \bd_{j-1}^{(0)\top} \bv_{j}^{(0)} \right)}
        \\
        \vdots
        \\
        \frac{\left( \br_{j-1}^{(T)\top} \bz_{j-1}^{(T)} \right)}
        {\left( \bd_{j-1}^{(T)\top} \bv_{j}^{(T)} \right)}
     \end{array} \right],
\end{equation*}
%
Similarly, for scaling,
%
\begin{align*}
  \begin{bmatrix}
    \bu_j^{(0)} & \cdots & \bu_j^{(T)}
  \end{bmatrix}
  &=
  \bU_{j-1} + \text{diag}(\alpha_j) \bD_{j-1}
  \\
  &=
  \begin{bmatrix}
    \bu_{j-1}^{(0)} & \cdots & \bu_{j-1}^{(T)}
  \end{bmatrix}
  +
  \begin{bmatrix}
    \alpha_{j}^{(0)} \bd_{j-1}^{(0)} & \cdots &  \alpha_{j}^{(T)} \bd_{j-1}^{(T)}.
  \end{bmatrix}
\end{align*}
%
In summary, mBCG is therefore able to perform all solve operations in batch, which enables it to use GPU parallelism.

To compute the Lanczos tridiagonal matrices  that correspond to inputs $\bz^{(1)}, \ldots, \bz^{(T)}$, mBCG adapts a technique from \citet{saad2003iterative}.
From \cref{obs:lanczos_cg}, the diagonal and subdiagonal entries of $\bT^{(1)}, \ldots, \bT^{(T)}$ are simple deterministic transforms of the $\balpha_j$ and $\bbeta_j$ coefficients from mBCG.
The final three lines in {\color{\colornew} \colornew} in \cref{alg:mod_pcg} use the $\balpha_j$ and $\bbeta_j$ coefficients to iteratively compute the Lanczos matrices from \cref{obs:lanczos_cg}.
Notably, these matrices can be formed with $\bigo{T}$ extra computation, and we are able to avoid running the Lanczos algorithm.
This is an advantage over existing MVM-based training methods, as mBCG avoids the numerical instabilities associated with Lanczos (see \cref{sec:lanczos}).


\subsection{Runtime and space.}
As shown above, we are able to approximate all inference terms from \emph{a single call to mBCG}.
These approximations improve with the number of mBCG iterations.
Each iteration requires one matrix-matrix multiply with $\trainK$ and element-wise operations on $N \times T$ matrices.
Therefore, $J$ iterations of mBCG requires $\bigo{NT}$ space and $\bigo{J \: \mmm{\trainK}}$ time,
where $\mmm{\trainK}$ is the time to multiply $\trainK$ by a $N \times (T + 1)$ matrix.\footnote{
	There are $J$ matrix multiplications with $\trainK$, resulting in the $\bigo{J \mmm{\trainK}}$ runtime.
	For the remainder of the algorithm, all matrices involved ($\bU_{j},\bV_j,\bR_j,\bZ_j,\bP_j$) are $n \times t$ matrices.
	All of the lines involving only these matrices perform operations that require $\bigo{NT}$ time.
	For example, elementwise multiplying $\bZ_{j} \circ \bZ_{j}$ accesses each element in $\bZ_{j}$ once, and and then multiplying it by the vector of ones similarly accesses every element in the matrix once.
	Multiplying $\bV_{j}$ by the diagonal matrix with $\ba_{j}$ on the diagonal takes $\bigo{NT}$ time, because we multiply every element $\bV_{j}^{(ik)}$ by $\ba_{j}^{(i)}$.
	Therefore, all other lines in the algorithm are dominated by the matrix multiply with $\trainK$, and the total running time is also $\bigo{J \mmm{\trainK}}$.
	Furthermore, because these intermediate matrices are $N \times T$, the space requirement is also $\bigo{NT}$.
}
This multiplication takes $\bigo{N^2 T}$ time with a standard matrix.
It is worth noting that this is a lower asymptotic complexity that standard Cholesky-based training, which is $\bigo{N^3}$.
Therefore, \emph{BBMM offers a computational speedup for exact GPs}.
As we will show in \cref{sec:programmability}, this time complexity can be further reduced with structured data or sparse GP approximations.

%We first briefly analyze the running time of mBCG (\cref{alg:mod_pcg}) itself.
%The algorithm performs matrix multiplies with $\trainK$ once before the loop and once during every iteration of the loop.
%Therefore, the running time of mBCG is at least $\bigo{J \mmm{\trainK}}$, where $\mmm{\trainK}$ is the time to multiply $\trainK$ by an $N \times T$ matrix.

After using mBCG to produce the solves and tridiagonal matrices, recovering the three training terms takes little additional time and space.
$\trainK^{-1}\by$ requires no additional computation because it is the first output of the algorithm.
The $\tr{\trainK^{-1}\frac{\partial \trainK}{\partial \btheta}}$ estimate is the inner product of the $\trainK^{-1} \bz^{(i)}$ solves with the $\frac{\partial \trainK}{\partial \btheta}[\bz^{(1)} \:\:, \ldots \:\:, \bz^{(T)}]$ vectors.
This only requires an additional $\mmm{\frac{\partial \trainK}{\partial \btheta}} + \bigo{NT}$ time and $\bigo{NT}$ space.\footnote{
	We assume that $\mmm{\frac{\partial \trainK}{\partial \btheta}} \approx \mmm{\trainK}$, which is true for exact GPs and GP approximations.
}
Computing $\log \vert \trainK \vert$ dominates the post-mBCG running time; however, it negligible assuming $J \ll N$ iterations of mBCG.
%mBCG gives us $J \times J$ tridiagonal matrices  ${\bT}^{(1)}, \ldots, {\bT}^{(T)}$.
To compute the log determinant estimate, we must compute $\be^{(1)^\top} \log \bT^{(i)} \be^{(1)}$ for each $i$.
To do this, we eigendecompose ${\bT}^{(i)} = \bV^{(i)} \bLambda^{(i)} \bV^{(i)^\top}$, which takes $\bigo{J^{2}}$ time for tridiagonal matrices.
This is again significantly less than the running time complexity of mBCG itself.
