%!TEX root=main.tex
\section{Related Work}
\paragraph{Conjugate gradients, the Lanczos tridiagonalization algorithm,}
and their relatives are methods from numerical linear algebra for computing linear solves and solving eigenvalue problems \emph{without explicitly computing a matrix}.
These techniques have been around for decades, and are covered in popular books and papers \cite{saad2003iterative,golub2012matrix,demmel1997applied,parlett1980new,lanczos1950iteration,datta2010numerical,paige1970practical}.
These algorithms belong to a broad class of iterative methods known as \emph{Krylov subspace methods}, which access matrices only through matrix-vector multiplies (MVMs).
Historically, these methods have been applied to solving large numerical linear algebra problems, particularly those involving sparse matrices that afford fast MVMs.

Recently, a number of papers have used these MVM methods for parts of GP inference \cite{dong2017scalable,cunningham2008fast,murray2009gaussian,saatcci2012scalable,wilson2014thesis,wilson2015kernel,gardner2018product,pleiss2018constant}.
One key advantage is that MVM approaches can exploit algebraic structure for increased computational efficiencies.
Notably, the structured kernel interpolation (SKI) method \cite{wilson2015kernel} uses structured kernel matrices with fast MVMs to achieve a remarkable asymptotic complexity.
\citet{dong2017scalable} propose MVM methods for computing stochastic estimates of log determinants and their derivatives using a technique based on Lanczos tridiagonalization \cite{golub2009matrices,ubaru2017fast}.
We utilize the same log determinant estimator as \citet{dong2017scalable}, except we avoid explicitly using the Lanczos tridiagonalization algorithm which has storage and numerical stability issues \cite{golub2012matrix}.

\paragraph{Preconditioning} is an effective tool for accelerating the convergence of conjugate gradients.
These techniques are far too numerous to review adequately here; however, \citet{saad2003iterative} contains two chapters discussing a variety of preconditioning techniques.
\citet{cutajar2016preconditioning} explores using preconditioned conjugate gradients for exact GP inference, where they use various sparse GP methods (as well as some classical methods) as preconditioners. However, the methods in \citet{cutajar2016preconditioning} do not provide general purpose preconditioners.
For example, methods like Jacobi preconditioning have no effect when using a stationary kernel \cite{cutajar2016preconditioning,wilson2015thoughts}, and many other preconditioners have $\bigomega{n^{2}}$ complexity, which dominates the complexity of most scalable GP methods.

\paragraph{The Pivoted Cholesky decomposition}
is an efficient algorithm for computing a low-rank decomposition of a positive definite matrix \cite{harbrecht2012low,bach2013sharp}, which we use in the context of preconditioning.
\citet{harbrecht2012low} explores the use of the pivoted Cholesky decomposition as a low rank approximation, although primarily in a scientific computing context.
In proving convergence bounds for our preconditioner we explicitly make use of some theoretical results from \cite{harbrecht2012low} (see \autoref{app:theory}).
\citet{bach2013sharp} considers using random column sampling as well as the pivoted Cholesky decomposition as a low-rank approximation to kernel matrices.
However, \citet{bach2013sharp} treats this decomposition as an approximate training method, whereas we use the pivoted Cholesky decomposition primarily
as a preconditioner, which avoids any loss of accuracy from the low rank approximation as well as the complexity of computing derivatives.
