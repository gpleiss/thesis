%!TEX root=main.tex
\paragraph{Theoretical analysis of CG convergence.}
By leveraging analysis from \cite{harbrecht2012low}, we can show that for RBF kernels the condition number of the preconditioned $\trainK$ matrix decreases exponentially with the rank $k$ of $\bL_k$.
We begin by restating \cite{harbrecht2012low} primary convergence result.
%
\begin{theorem}[\citet{harbrecht2012low}]
If the eigenvalues of an $n \times n$ positive definite and symmetric matrix $K_{XX}$ satisfy $4^{k}\lambda_{k} \lesssim \exp(-bk)$ for some $b>0$, then
the rank $k$ pivoted Cholesky decomposition $P_{k}$ satisfies $\textrm{Tr}(K_{XX} - P_{k}) \lesssim n\exp(-bk)$.
\label{thm:harbrecht}
\end{theorem}
Intuitively, if the eigenvalues of a matrix decay very quickly (exponentially), then it is very easy to approximate with a low rank matrix, and the pivoted Cholesky algorithm rapidly constructs such a matrix.
While there has been an enormous amount of work understanding the eigenvalue distributions of kernel functions \jrg{cite cite cite}, in this paper we prove the following useful bound on the eigenvalue distribution of univariate RBF kernel matrices:
%
\begin{theorem}
\label{thm:eigenvalue_bound}
Given $\bu_1,...,\bu_n \in [0, 1]$, the RBF kernel matrix $K_{XX} \in \mathbb{R}^{n \times n}$ with $K_{ij} = \exp \left(-\gamma(x_i - x_j)^{2}\right)$ has eigenvalues bounded by:
\begin{equation*}
    \lambda_{2l+1} \leq
    2n e^{-\gamma/4} I_{l+1}(\gamma/4) \sim
    \frac{2n e^{-\gamma/4}}{\sqrt{\pi\gamma}}
    \left( \frac{e\gamma}{8(l+1)} \right)^{l+1}
\end{equation*}
where $I_j$ denotes the modified Bessel function of the first kind with parameter $j$.
\end{theorem}
%
Thus, the eigenvalues of an RBF kernel matrix $K_{XX}$ decay \emph{super-exponentially}, and so the bound given by \autoref{thm:harbrecht} applies.
These two theorems together let us argue for the theoretical soundness of the pivoted Cholesky decomposition as a preconditioner.
To bound the condition number of the matrix $\trainP^{-1} \trainK$, we begin bounding the difference between a linear solve with the $\trainP$ and the full matrix $\trainK$.
%
\begin{theorem}
Let $\by$ be any vector, and let $\bu^{*} = \trainK^{-1}\by$ and $\tilde{\bu} = \trainP_{k}^{-1}\by$, where $\trainP_{k}=\bL_{k} \bL_K^\top + \sigma^{2}I$ and $\bL_{k} \bL_k^\top$ is the rank $k$ pivoted Cholesky decomposition of $K_{XX}$.
There exists some $b>0$ so that $\frac{\Vert \bu^{*} - \tilde{\bu}\Vert_{2}}{\Vert\bu^{*}\Vert_{2}} \lesssim \frac{n\exp(-bk)}{\sigma}$.
\end{theorem}
%
Intuitively, this theorem states that the pivoted Cholesky preconditioner converges rapidly to $\trainK^{-1}$, which is the optimal preconditioner in terms of the number of CG iterations.
Athe norm $\Vert \trainP_{k}^{-1} - \trainK^{-1} \Vert$ decreases, the convergence rate of CG increases.
Finally, to explicitly relate the above theorem to the rate of convergence of CG, we can prove the following bound on the condition number:
%
\begin{theorem}
  \label{thm:condition_number}
  Let $P_{k}$ be the rank $k$ pivoted Cholesky decomposition of $K$, and let $\trainP_{k} = P_{k} + \sigma^{2}I$. Then the condition number $\kappa(\trainP^{-1}\trainK)$ satisfies the following inequality:
  \begin{equation}
    \kappa(\trainP_{k}^{-1}\trainK) \triangleq \Vert \trainP_{k}^{-1}\trainK \Vert_{2} \Vert \trainK^{-1}\trainP_{k} \Vert_{2} \leq 1 + \bigo{\frac{n\exp(-bk)}{\sigma}}.
  \end{equation}
\end{theorem}
%
This last result lets us directly speak about the impact of our preconditioner on the convergence of conjugate gradients.
In particular, when solving a system $\trainP_{k}^{-1}\trainK\bu = \trainP_{k}^{-1}\by$, the solution $\bu_{p}$ after $p$ iterations of conjugate gradients in exact arithmetic is related to the optimal solution $\bu^{*}$ by the following inequality involving the condition number \jrg{cite}:
%
\begin{equation}
  \begin{aligned}
  \Vert \trainK^{-1} y - \bu_{k} \Vert_{A}
  &\leq 2 \left( \left( {\sqrt{\kappa\left(\trainP_{k}^{-1}\trainK\right)} - 1} \right) / \left( {\sqrt{\kappa\left(\trainP_{k}^{-1}\trainK\right)} + 1} \right) \right)^{p}\Vert \trainK^{*}\by - \bu_{0} \Vert_{A}
  \\
  &\leq 2 \left(\frac{1}{2 + \bigo{n e^{-kb/2} / \sigma}}\right)^{p}\Vert \trainK^{-1} y - \bu_{0} \Vert_{A}
\end{aligned}
\end{equation}
%
Thus, \autoref{thm:condition_number} implies that the pivoted Cholesky preconditioner exponentially improves the rate of convergence as $k$ increases.
