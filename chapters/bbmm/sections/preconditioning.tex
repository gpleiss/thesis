\section{Preconditioning}
\label{sec:preconditioning}

While each iteration of mBCG performs large parallel matrix-matrix operations that utilize hardware efficiently, the iterations themselves are sequential.
A natural goal for better utilizing hardware is to trade off fewer sequential steps for slightly more effort per step.
We accomplish this goal using \emph{preconditioning} \cite{golub2012matrix,saad2003iterative,demmel1997applied,van2003iterative}, which introduces a matrix $\bP$ to solve the related linear system
\begin{equation*}
  \bP^{-1} \trainK \bc = \bP^{-1}\by
\end{equation*}
instead of $\trainK^{-1} \by$.
Both systems are guaranteed to have the same solution, but the preconditioned system's convergence depends on the conditioning of $\bP^{-1}\trainK$ rather than that of $\trainK$.

\citet{cutajar2016preconditioning} explores using preconditioned conjugate gradients for exact GP training, where they use various sparse GP methods (as well as some classical methods) as preconditioners.
However, the methods in \citet{cutajar2016preconditioning} do not provide general purpose preconditioners.
For example, methods like Jacobi preconditioning have no effect when using a stationary kernel \cite{cutajar2016preconditioning,wilson2015thoughts}, and many other preconditioners have $\bigomega{N^{2}}$ complexity, which dominates the complexity of most scalable GP methods.

We observe two requirements of a preconditioner to be used in general for GP training.
First, in order to ensure that preconditioning operations do not dominate running time when using scalable GP methods, the preconditioner should afford roughly linear time solves and space.
Second, we should be able to efficiently compute the log determinant of the preconditioner matrix, $\log \vert \bP \vert$.
This is because the mBCG algorithm applied to the preconditioned system estimates $\log \vert \bP^{-1}\trainK \vert$ rather than $\log \vert \trainK \vert$. We must therefore compute
$
  \label{eq:logdet_adjusted}
  \log \vert \trainK \vert = \log \vert \bP^{-1}\trainK \vert + \log \vert \bP \vert.
$

For one possible preconditioner, we turn to the {\bf pivoted Cholesky decomposition} \cite{harbrecht2012low}.
The pivoted Cholesky algorithm allows us to compute a low-rank approximation of a positive definite matrix, $\bK_{\bX\bX} \approx \bL_{k} \bL_{k}^{\top}$.
We precondition mBCG with $(\bL_k \bL_k^\top + \sigma^2 \bI)^{-1}$, where $\sigma^2$ is the Gaussian likelihood's noise term.
Intuitively, if $\bP_{k}=\bL_{k}\bL_{k}^{\top}$ is a good approximation of $\bK_{\bX\bX}$, then $(\bP_{k} + \sigma^{2}\bI)^{-1}\trainK \approx \bI$.

Before describing the preconditioned and its effect on mBCG convergence, we will first introduce the pivoted Cholesky algorithm and its properties.


\subsection{The Pivoted Cholesky Decomposition}
\gp{Stopped here.}

\paragraph{The Pivoted Cholesky decomposition}
is an efficient algorithm for computing a low-rank decomposition of a positive definite matrix \cite{harbrecht2012low,bach2013sharp}, which we use in the context of preconditioning.
\citet{harbrecht2012low} explores the use of the pivoted Cholesky decomposition as a low rank approximation, although primarily in a scientific computing context.
In proving convergence bounds for our preconditioner we explicitly make use of some theoretical results from \cite{harbrecht2012low} (see \cref{app:theory}).
\citet{bach2013sharp} considers using random column sampling as well as the pivoted Cholesky decomposition as a low-rank approximation to kernel matrices.
However, \citet{bach2013sharp} treats this decomposition as an approximate training method, whereas we use the pivoted Cholesky decomposition primarily
as a preconditioner, which avoids any loss of accuracy from the low rank approximation as well as the complexity of computing derivatives.

\subsection{Pivoted Cholesky as a Preconditioner}

\paragraph{The Pivoted Cholesky Decomposition.}

We would like to emphasize three key properties of the pivoted Cholesky decomposition.
First, it can be computed in $\bigo{\row{\bK_{\bX\bX}}k^{2}}$ time, where $\row{\bK_{\bX\bX}}$ is the time to access a row (nominally this is $\bigo{N}$).
Second, linear solves with $\trainP = \bL_{k}\bL_{k}^{\top} + \sigma^{2}\bI$ can be performed in $\bigo{nk^{2}}$ time. Finally, the log determinant of $\trainP$ can be computed in $\bigo{nk^{2}}$ time.
In \cref{sec:results} we empirically show that this preconditioner dramatically accelerates CG convergence.
Further, in \cref{app:theory}, we prove the following lemma and theorem for univariate RBF kernels:

\begin{lemma}
  \label{thm:condition_number}
  Let $\bK_{\bX\bX} \in \reals^{n \times n}$ be a univariate RBF kernel matrix.
  Let $\bL_{k} \bL_k^\top$ be the rank $k$ pivoted Cholesky decomposition of $\bK_{\bX\bX}$, and let $\trainP_{k} = \bL_k \bL_k^\top + \sigma^{2}\bI$.
  Then there exists a constant $b>0$ so that the condition number $\kappa(\trainP^{-1}\trainK)$ satisfies the following inequality:
  \begin{equation}
    \kappa \left( \trainP_{k}^{-1}\trainK \right)
    \triangleq \left\Vert \trainP_{k}^{-1}\trainK \right\Vert_{2} \left\Vert \trainK^{-1}\trainP_{k} \right\Vert_{2}
    \leq \left( 1 + \bigo{n\exp(-bk)} \right)^2.
  \end{equation}
\end{lemma}
\begin{theorem}[Convergence of pivoted Cholesky-preconditioned CG]
  \label{thm:cg_convergence_rbf}
  Let $\bK_{\bX\bX} \in \reals^{n \times n}$ be a $n \times n$ univariate RBF kernel, and let $\bL_k \bL_k^\top$ be its rank $k$ pivoted Cholesky decomposition.
  Assume we are using preconditioned CG to solve the system $\trainK^{-1} \by = (\bK_{\bX\bX} + \sigma^2 \bI)^{-1} \by$ with preconditioner $\trainP = (\bL_k \bL_k^\top + \sigma^2 \bI)$.
  Let $\bc_J$ be the $J^\textrm{th}$ solution of CG, and let $\bc^{*} = \trainK^{-1} \by$ be the exact solution.
  Then there exists some $b > 0$ such that:
  \begin{equation}
    \Vert \bc^{*} - \bc_{J} \Vert_{\trainK}
    \leq 2 \left(1/(1 + \bigo{\exp(kb)/n}\right)^{J} \left\Vert \bc^{*} - \bc_{0} \right\Vert_{\trainK}.
  \end{equation}
\end{theorem}
%
\cref{thm:cg_convergence_rbf} implies that we should expect the convergence of conjugate gradients to improve \emph{exponentially} with the rank of the pivoted Cholesky decomposition used for RBF kernels. In our experiments we observe significantly improved convergence for other kernels as well (\cref{sec:results}). Furthermore, we can leverage \cref{thm:condition_number} and existing theory from \cite{ubaru2017fast} to argue that preconditioning improves our log determinant estimate. In particular, we restate Theorem 4.1 of \citet{ubaru2017fast} here:
\begin{theorem}[Theorem 4.1 of \citet{ubaru2017fast}]
  \label{thm:slq_convergence}
  Let $\bK_{\bX\bX} \in \reals^{n \times n}$, and let $\bL_k \bL_k^\top$ be its rank $k$ pivoted Cholesky decomposition.
  Suppose we run $p \geq \frac{1}{4} \sqrt{ \kappa \left( \trainP_{k}^{-1}\trainK \right) } \log \frac{D}{\epsilon}$ iterations of mBCG,
  where $D$ is a term involving this same condition number that vanishes as $k \to n$ (see \cite{ubaru2017fast}),
  and we use $t \geq \frac{24}{\epsilon^{2}}\log(2/\delta)$ vectors $\bz^{(i)}$ for the solves.
  Let $\Gamma$ be the log determinant estimate from \eqref{eq:slq}. Then:
  \begin{equation}
    \textrm{Pr}\left[\vert \log \vert \trainP^{-1}\trainK \vert - \Gamma \vert \leq \epsilon\vert \log \vert \trainP^{-1}\trainK \vert \vert \right] \geq 1 - \delta.
  \end{equation}
\end{theorem}
Because \cref{thm:condition_number} states that the condition number $\kappa \left( \trainP_{k}^{-1}\trainK \right)$ decays exponentially with the rank of $\bL_{k}$, \cref{thm:slq_convergence} implies that we should expect that the number of CG iterations required to accurately estimate $\log \vert \trainP^{-1}\trainK \vert$ decreases quickly as $k$ increases.
In addition, in the limit as $k \rightarrow n$ we have that $\log \vert \trainK \vert = \log \vert \trainP \vert$.
This is because $\log \vert \trainP^{-1}\trainK \vert \rightarrow 0$ (since $\trainP^{-1}\trainK$ converges to $\bI$) and we have that $\log \vert \trainK \vert = \log \vert \trainP^{-1}\trainK \vert + \log \vert \trainP \vert$.
Since our calculation of $\log \vert \trainP \vert$ is exact, our final estimate of $\log \vert \trainK \vert$ becomes more exact as $k$ increases.
In future work we hope to derive a more general result that covers multivariate settings and other kernels.
%

