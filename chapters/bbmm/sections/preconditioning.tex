\section{Preconditioning}
\label{sec:preconditioning}

While each iteration of mBCG performs large parallel matrix~\texttimes~matrix operations, the iterations themselves are sequential.
A natural goal for better utilizing hardware is to trade off fewer sequential steps for slightly more effort per step.
We accomplish this goal using \emph{preconditioning} \citep[e.g.][]{demmel1997applied,saad2003iterative,van2003iterative,golub2012matrix}, which introduces a matrix $\bP$ to solve the related linear system
\begin{equation}
  \left( \trainP^{- \frac 1 2} \trainK \trainP^{- \frac 1 2}\right) \bC = \trainP^{- \frac 1 2} \left[ \by, \:\:, \bz^{(1)}, \:\:, \ldots, \:\: \bz^{(T)} \right].
  \label{eqn:precond_system}
\end{equation}
instead of $\trainK^{-1} \left[  \by, \:\:, \bz^{(1)}, \:\:, \ldots, \:\: \bz^{(T)} \right].$.
Both systems are guaranteed to have the same solution, but the preconditioned system's convergence depends on the conditioning of $\trainP^{- \frac 1 2} \trainK \trainP^{- \frac 1 2}$ rather than that of $\trainK$.
Despite the matrix square roots in \cref{eqn:precond_system}, preconditioned CG/mBCG only need access to $\trainP$ and its inverse $\trainP^{-1}$ (see \cref{alg:std_pcg}).


\gp{CONTINUE HERE}

\subsection{Modifying mBCG for Preconditioning}
\label{sec:precond_requirements}
We have to make some special adjustments to our modified batch CG algorithm in order to use preconditioning.
While the preconditioned system simply returns $\trainK^{-1} \by$, we need to modify the random probe vectors $\bz^{(1)}$, $\ldots$, $\bz^{(T)}$ to get correct estimates of $\log \vert \trainK \vert$ and $\tr{ \trainK^{-1} ( \partial \trainK / \partial \btheta) }$.
In particular, we will perform the solves
%
\begin{equation}
  \label{eqn:mod_cg_call_precond}
  \trainK^{-1} \left[ \by, \:\: \bz^{(1)}, \:\: \cdots, \:\: \bz^{(T)} \right], \quad \bz^{(i)} \sim \normaldist{\bzero}{\trainP}.
\end{equation}

The only difference between \cref{eqn:mod_cg_call_precond} and the original set of solves in \cref{eqn:mod_cg_call}
is that the $\bz^{(i)}$ probe vectors have covariance $\Ev{ \bz^{(i)} \bz^{(i)^\top} } = \trainP$ (rather than unit covariance).
To understand why this is the case, recall from \cref{sec:bbmm_method} that our log determinant estimate is given by:
%
\begin{align*}
	\log \left\vert \trainK \right\vert
	&\approx \Evover{\bz^{(i)} \sim \normaldist{0}{\bI} }{\bz^{(i)^\top} \bQ^{(i)} \left( \log \bT^{(i)} \right) \bQ^{(i)^\top} \bz^{(i)}},
\end{align*}
%
where $\bT^{(1)}$, $\ldots$, $\bT^{(T)}$---the Lanczos tridiagonal matrices corresponding to $\trainK$ and probe vectors $\bz^{(i)}$, $\ldots$, $\bz^{(T)}$---are computed from byproducts of the CG iterations.
However, if we precondition mBCG with $\trainP$, then the $\bT^{(i)}$ matrices will instead correspond to the \emph{preconditioned system}
$(\trainP^{- \frac 1 2} \trainK \trainP^{- \frac 1 2})$ and \emph{preconditioned probe vectors} $(\trainP^{- \frac 1 2} \bz^{(i)})$.
Consequentially, the stochastic Lanczos quadrature estimate will actually return
%
\begin{align}
	\log \left\vert \trainP^{- \frac 1 2} \trainK \trainP^{- \frac 1 2} \right\vert
	&\approx \!
	\Evover{\bz^{(i)} \sim \normaldist{0}{\trainP} }{
		\left( \bz^{(i)^\top} \trainP^{-\frac 1 2} \right) \!
		\bQ^{(i)} \! \left( \log \bT^{(i)} \right) \! \bQ^{(i)^\top}
		\!\! \left( \trainP^{-\frac 1 2} \bz^{(i)} \right)
	}.
	\label{eqn:slq_precond}
\end{align}
%
\cref{eqn:slq_precond} motivates the use of $\bz^{(i)} \sim \normaldist{\bzero}{\trainP}$ as the probe vectors:
the resulting preconditioned vectors $\trainP^{- \frac 1 2} \bz^{(i)}$ will be samples from $\normaldist{\bzero}{\bI}$ and can therefore be used for a stochastic trace estimate.

\paragraph{Estimating $\log \vert \trainK \vert$ and $\tr{ \trainK^{-1} (\partial \trainK / \partial \btheta) }$ from preconditioned mBCG.}
To compute $\log \vert \trainK \vert$ from \cref{eqn:slq_precond}, we note that
\[
\log \vert \trainK \vert = \log \vert \trainP^{- \frac 1 2} \trainK \trainP^{- \frac 1 2} \vert + \log \vert \trainP \vert.
\]
We therefore estimate the first using stochastic Lanczos quadrature (\cref{eqn:slq_precond}) and then ``correct'' this estimate with the log determinant of the preconditioner.
Note that this will still be an unbiased estimate of $\log \vert \trainK \vert$ if we can exactly compute $\log \vert \trainP \vert$ (in an efficient manner).

To estimate $\tr{ \trainK^{-1} (\partial \trainK / \partial \btheta) }$ from the new probe vectors $\bz^{(i)} \sim \normaldist{\bzero}{\trainP}$,
we note that we can form a stochastic trace estimate from the following:
\begin{align}
	\tr{\trainK^{-1}\frac{\partial \trainK}{\partial \btheta}}
	&=
	\tr{
		\trainK^{-1}\frac{\partial \trainK}{\partial \btheta}
		\Evover{\bz^{(i)} \sim \normaldist{0}{\trainP} }{
			\trainP^{-1} \bz^{(i)} \bz^{(i)^\top}
		}
	}
  \nonumber \\
	&\approx
	\Evover{\bz^{(i)} \sim \normaldist{0}{\trainP} }{
		\left( \bz^{(i)} \trainK^{-1} \right)
		\left( \frac{\partial \trainK}{\partial \btheta} \: \trainP^{-1} \bz^{(i)} \right)
	}.
	\label{eqn:trace_est_precond}
\end{align}
%
The only differences between \cref{eqn:trace_est_precond} and the non-preconditioned trace estimate in \cref{eqn:trace_deriv_estimate} are
\begin{enumerate*}
	\item we use $\bz^{(i)} \sim \normaldist{\bzero}{\trainP}$ as probe vectors, and
	\item the derivative term $\partial \trainK / \partial \btheta$ is applied to the preconditioned vectors $\trainP^{-1} \bz^{(i)}$.
\end{enumerate*}

\paragraph{Requirements of mBCG preconditioners.}
Based on the above discussion, we observe three requirements of any preconditioner $\trainP$ that can be used with mBCG for GP training.
First, in order to ensure that preconditioning operations do not dominate the running time of \cref{alg:mod_pcg}, the preconditioner should afford roughly linear-time solves and linear space.
%\footnote{
	%Recall from \cref{alg:std_pcg} that preconditioned conjugate gradients only requires computing $\trainP^{-1}$ and $\trainP$, not $\trainP^{- \frac 1 2}.$
%}
Second, we should be able to efficiently compute the log determinant of the preconditioner $\log \vert \trainP \vert$ to ``correct'' the log determinant estimate in \cref{eqn:slq_precond}.
Finally, we should be able to efficiently sample probe vectors $\bz^{(i)}$ from the distribution $\normaldist{\bzero}{\trainP}$.



\subsection{The Partial Pivoted Cholesky Preconditioner for mBCG}
\label{sec:piv_chol_precond}

For one possible preconditioner, we turn to the {\bf partial pivoted Cholesky decomposition} (as introduced in \cref{sec:piv_chol}).
The pivoted Cholesky algorithm allows us to compute a rank-$R$ approximation ($R \ll N$) of the training kernel matrix $\bK_{\bX\bX} \approx \bar\bL_{R} \bar\bL_{R}^{\top}$.
Our mBCG preconditioner will be
\begin{equation}
  \trainP_R = \bar\bL_{R} \bar\bL_{R}^\top + \sigma^2_\text{obs} \bI,
\end{equation}
where $\sigma_\text{obs}^2$ is the Gaussian likelihood's noise term.
Intuitively, if $\bar\bL_{R}\bar\bL_{R}^{\top}$ is a good low-rank approximation of $\bK_{\bX\bX}$, then $(\bar\bL_{R} \bar\bL_R^\top + \sigma_\text{obs}^{2}\bI)^{-1}\trainK \approx \bI$.


Unlike the standard Cholesky decomposition, which computes an exact factorization in $N$ iterations,
the partial pivoted Cholesky decomposition produces a \emph{rank-R} factorization in $R \ll N$ iterations, and therefore does not share its asymptotic concerns.
Moreover, it meets the requirements outlined above:
%
\begin{observation}[Properties of the rank-$R$ pivoted Cholesky preconditioner]
  {\ }
  \begin{enumerate}
    \item $\bar\bL_{R}$ can be computed in $\bigo{\row{\trainK}R^{2}}$ time, where $\row{\trainK}$ is the time required to retrieve a single row of $\trainK$
      (see \cref{sec:piv_chol}).

    \item Storing $\bar\bL_{R}$ requires $\bigo{NR}$ memory.

    \item Linear solves with $\trainP_R = \bar\bL_{R} \bar\bL_{R}^{\top} + \sigma_\text{obs}^{2} \bI$ can be performed in $\bigo{NR^{2}}$ time using the Woodbury matrix formula.\footnote{
      The Woodbury matrix formula is a $\bigo{R^2 N}$ formula for ``rank-R plus diagonal'' solves:
      $$\left( \bar\bL_{R} \bar\bL_{R}^{\top} + \sigma^{2}_\text{obs} \bI \right)^{-1} \bb = {\sigma_\text{obs}^{-2}}\bb - {\sigma_\text{obs}^{-4}} \: \bar\bL_{R} \left( \bI - {\sigma^{-2}_\text{obs}} \bar\bL_{R}^{\top} \bar\bL_{R} \right)^{-1} \bar\bL_{R}^{\top}\bb.$$
    }

    \item The log determinant of $\trainP_R$ can be computed in $\bigo{NR^{2}}$ time using the matrix determinant lemma.\footnote{
      The matrix determinant lemma is an analog of the Woodbury formula for determinants:
      $$\log \vert \bar\bL_{R} \bar\bL_{R}^\top + \sigma^{2}_\text{obs} \bI \vert = \log \vert \bI - {\sigma_\text{obs}^{-2}} \bar\bL_{R}^{\top} \bar\bL_{R} \vert + 2 N \log \sigma_\text{obs}.$$
    }

  \item Samples from $\normaldist{ \bzero}{ \trainP_R }$ can be drawn with $\bigo{NR^2}$ computation using the reparameterization trick \cite{kingma2014auto}.\footnote{
      Draw standard normal vectors $\bepsilon_1' \in \reals^R$ and $\bepsilon_2' \in \reals^N$.
      By the reparameterization trick, $\left( \bar\bL_R \bepsilon_1' + \sigma_\text{obs} \bepsilon_2' \right)$ is a sample from $\normaldist{ \bzero}{( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI )}$.
    }
  \end{enumerate}
\end{observation}
%
\noindent
Assuming $R \ll N$ (for example, $R \approx 5$), computing and using $\trainP$ is less expensive than a single matrix multiplication with $\trainK$.
While the $R \approx 5$ iterations required to compute $\bar\bL_R$ are inherently sequential, we note that this is far fewer iterations than the standard Cholesky factorization.

Perhaps more important than its runtime are its convergence properties.
In general, the low-rank nature of $\trainP$ is an ideal choice for many kernel matrices with rapidly decaying spectra.
Such kernels tend to be horribly conditioned yet are well approximated by low rank matrices.
We empirically demonstrate in \cref{sec:bbmm_results} that $\trainP$ significantly improves the convergence of mBCG for several kernels.
Below we prove theoretical guarantees for certain classes of kernels.


\paragraph{Theoretical analysis.}
Kernels with rapidly decaying eigenvalues (i.e. kernels that are well approximated by low-rank matrices) will see the largest improvements from the pivoted Cholesky preconditioner.
Based on the work of \citet{harbrecht2012low}, we can prove the following lemma about kernel condition numbers:
%
\begin{lemma}
  \label{thm:condition_number}
  Let $\bar\bL_{R}$ be the rank-$R$ pivoted Cholesky factor of kernel matrix $\bK_{\bX\bX} \in \reals^{N \times N}$.
  If the first $R$ eigenvalues $\lambda_1$, $\ldots$, $\lambda_R$ of $\bK_{\bX\bX}$ satisfy
	\begin{equation}
		4^{i}\lambda_{i} \leq \bigo{e^{-Bi}}, \quad i \in \{ 1, \:\: \ldots, \:\: R \},
		\label{eqn:pcp_condition}
	\end{equation}
	for some $B>0$, then the condition number $\kappa(\trainP^{-1}\trainK) \triangleq \Vert \trainP_{k}^{-1}\trainK \Vert_{2} \Vert \trainK^{-1}\trainP_{k} \Vert_{2}$
	satisfies the following bound:
  \begin{align}
    \kappa \left( \trainP^{-1}\trainK \right)
    &\leq \Bigl( 1 + \bigo{\sigma^{-2}_\text{obs} N e^{-BR}} \Bigr)^2
		\nonumber
  \end{align}
	where $\trainP = \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)$ and $\trainK = \left( \bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)$.
\end{lemma}
%
\noindent
(See \cref{app:proofs} for a proof.)
It so happens that the exponentially-decaying eigenvalue assumption actually holds for certain classes of kernels.
For example, the eigenvalues of univariate RBF kernels are guaranteed to decay \emph{super-exponentially} (see \cref{app:univariate_rbf}).
In our experiments we observe improved conditioning for other kernels as well (\cref{sec:bbmm_results}).

Using \cref{thm:condition_number}, we can prove the following statements about the solves/log determinant estimates from preconditioned mBCG:
%
\begin{theorem}[Convergence of solves from preconditioned mBCG]
  \label{thm:precond_mbcg_solves}
  Let $\bK_{\bX\bX} \in \reals^{N \times N}$ be a $N \times N$ kernel that satisfies the eigenvalue condition of \cref{eqn:pcp_condition},
	and let $\bar\bL_R$ be its rank-$R$ pivoted Cholesky factor.
	After $J$ iterations of mBCG with preconditioner $\trainP = (\bar\bL_R \bar\bL_R^\top + \sigma_\text{obs}^2 \bI)$,
	the difference between $\bc_J$ and true solution $\trainK^{-1} \by$ is bounded by:
	%
  \begin{equation*}
    \left \Vert \trainK^{-1} \by - \bc_{J} \right \Vert_{\trainK}
    \leq \Bigg[ \frac 1 {1 + \bigo{\sigma^{2}_\text{obs} e^{RB}/N}} \Bigg]^{J}
    \left \Vert \trainK^{-1} \by \right \Vert_{\trainK},
		\nonumber
  \end{equation*}
	%
	where $\trainK = (\bK_{\bX\bX} + \sigma^2_\text{obs} \bI)$ and $B > 0$ is a constant.
\end{theorem}
%
\begin{theorem}[Convergence of log determinants from preconditioned mBCG]
  \label{thm:precond_mbcg_logdet}
  Assume $\bK_{\bX\bX} \in \reals^{N \times N}$ satisfies the eigenvalue condition of \cref{eqn:pcp_condition}.
	Suppose we estimate $\Gamma \approx \log \vert \trainP^{-1} \trainK \vert$ using \cref{eqn:slq_precond} with:
	\begin{itemize}
		\item $J \geq \mathcal{O} \left[ (1 + \sigma^{-2}_\text{obs} N e^{-BR}) \log \left( ( 1 + \sigma^{-2}_\text{obs} N e^{-BR} ) / \epsilon \right) \right]$ iterations of mBCG (for some constant $B > 0$), and
		\item $T \geq \frac{32}{\epsilon^2} \log \left( \frac 2 \delta \right)$ random $\bz^{(i)} \sim \normaldist{\bzero}{\trainP}$ vectors.
	\end{itemize}
  Then the error of the stochastic Lanczos quadrature estimate $\Gamma$ is probabilistically bounded by:
  \begin{equation*}
    \textrm{Pr}\left[\Bigl\vert \log \vert \trainP^{-1} \trainK \vert - \Gamma \Bigr\vert \leq \epsilon N \right] \geq \left( 1 - \delta \right).
  \end{equation*}
\end{theorem}
%
\noindent
(See \cref{app:proofs} for proofs.)
\cref{thm:precond_mbcg_solves} implies that the mBCG solves---used to compute both $\trainK^{-1} \by$ and $\tr{ \trainK^{-1} (\partial \trainK / \partial \btheta)}$---will converge \emph{exponentially} quicker as the rank of the pivoted Cholesky decomposition increases.
\cref{thm:precond_mbcg_logdet} implies that the number of iterations needed to accurately estimate $\log \vert \trainP^{-1}\trainK \vert$ also decreases quickly as $R$ increases.
Furthermore, in the limit as $R \rightarrow N$ we have that $\log \vert \trainK \vert = \log \vert \trainP \vert$.
%This is because $\log \vert \trainP^{-1}\trainK \vert \rightarrow 0$ (since $\trainP^{-1}\trainK$ converges to $\bI$) and we have that $\log \vert \trainK \vert = \log \vert \trainP^{-1}\trainK \vert + \log \vert \trainP \vert$.
Since our calculation of $\log \vert \trainP \vert$ is exact, the final estimate of $\log \vert \trainK \vert$ will have less stochasticity.

\paragraph{Related work.}
\citet{cutajar2016preconditioning} explore preconditioned conjugate gradients for GP training, using various sparse GP methods (as well as some classical methods) as preconditioners.
\citet{bach2013sharp} uses the pivoted Cholesky decomposition as a low-rank approximation to kernel matrices.
However, \citet{bach2013sharp} treats this decomposition as an approximate training method, whereas we use the decomposition primarily as a preconditioner and thus avoid any loss of accuracy from the low rank approximation.


