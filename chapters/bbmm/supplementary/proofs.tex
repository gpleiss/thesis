%!TEX root=../main.tex
\chapter{Convergence Analysis of Partial Pivoted Cholesky Preconditioned mBCG}
\label{app:theory}

\section{Proof of Theorems in \cref{sec:piv_chol_precond}}


\subsection{Proof of \cref{thm:condition_number}.}
\newtheorem*{thm:condition_number}{\cref{thm:condition_number} (Restated)}
\begin{thm:condition_number}
  Let $\bar\bL_{R}$ be the rank-$R$ pivoted Cholesky factor of kernel matrix $\bK_{\bX\bX} \in \reals^{N \times N}$.
  If the first $R$ eigenvalues $\lambda_1$, $\ldots$, $\lambda_R$ of $\bK_{\bX\bX}$ satisfy
	\begin{equation*}
		4^{i}\lambda_{i} \leq \bigo{e^{-Bi}}, \quad i \in \{ 1, \:\: \ldots, \:\: R \},
		\tag{\ref{eqn:pcp_condition}}
	\end{equation*}
	for some $B>0$, then the condition number $\kappa(\trainP^{-1}\trainK) \triangleq \Vert \trainP_{k}^{-1}\trainK \Vert_{2} \Vert \trainK^{-1}\trainP_{k} \Vert_{2}$
	satisfies the following bound:
  \begin{align}
    \kappa \left( \trainP^{-1}\trainK \right)
    &\leq \Bigl( 1 + \bigo{\sigma^{-2}_\text{obs} N e^{-BR}} \Bigr)^2
		\nonumber
  \end{align}
	where $\trainP = \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)$ and $\trainK = \left( \bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)$.
\end{thm:condition_number}

\begin{proof}
  Let $\bE$ be the difference between $\bK_{\bX\bX}$ and its rank-$R$ pivoted Cholesky approximation $\bar\bL_R \bar\bL_R^\top$:
  \[
    \bE = \bK_{\bX\bX} - \bar\bL_R \bar\bL_R^\top = \begin{bmatrix} \bzero & \bzero \\ \bzero & \bS_{R+1} \end{bmatrix}
  \]
  where $\bS_{R+1}$ is the Schur compliment that arises as the Cholesky error term (see \cref{eqn:chol_error}).
  Therefore, the error $\bE$ is a positive semi definite matrix.

  By definition, the condition number $\kappa ( \trainP^{-1}\trainK )$ is given by
  \begin{align*}
    \kappa \left( \trainP^{-1}\trainK \right)
    &\triangleq \left\Vert \trainP^{-1}\trainK \right\Vert_{2} \left\Vert \trainK^{-1}\trainP \right\Vert_{2}
  \end{align*}
  %
  The left term can be rewritten as:
  %
  \begin{align*}
    \\
    \left\Vert \trainP^{-1}\trainK \right\Vert_{2}
    &= \left\Vert \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)^{-1} \left(\bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right) \right\Vert_{2}
    \\
    &= \left\Vert \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)^{-1} \left(\bar\bL_R \bar\bL_R^\top + \bE + \sigma^2_\text{obs} \bI \right) \right\Vert_{2}
    \\
    &= \left\Vert \bI + \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)^{-1} \bE \right\Vert_{2}
  \end{align*}
  %
  Similarly, the right term is:
  %
  \begin{align*}
    \left\Vert \trainK^{-1}\trainP \right\Vert_{2}
    &= \left\Vert \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right) \left(\bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)^{-1} \right\Vert_{2}
    \\
    &= \left\Vert \left( \bK_{\bX\bX} - \bE + \sigma^2_\text{obs} \bI \right) \left(\bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)^{-1} \right\Vert_{2}
    \\
    &= \left\Vert \bI - \left(\bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)^{-1} \bE \right\Vert_{2}
  \end{align*}
  %
  Since $\bK_{\bX\bX}$ and $\bar\bL_R \bar\bL_R^\top$ are both positive (semi-)definite,
  $(\bK_{\bX\bX} + \sigma^2_\text{obs})$ and $(\bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs})$ will both have a minimum eigenvalue $\lambda_\text{min} \geq \sigma^2_\text{obs}$.
  Therefore,
  %
  \[
    \left\Vert \left(\bK_{\bX\bX} + \sigma^2_\text{obs} \right)^{-1} \right\Vert_2 \leq \sigma^{-2}_\text{obs},
    \quad
    \left\Vert \left(\bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \right)^{-1} \right\Vert_2 \leq \sigma^{-2}_\text{obs}.
  \]
  %
  Applying these bound, along with Cauchy-Schwarz and the triangle inequality, gives us
  %
  \begin{align}
    \kappa \left( \trainP^{-1}\trainK \right) \!
    &\leq \! \left( 1 + \left\Vert \left( \bar\bL_R \bar\bL_R^\top + \sigma^2_\text{obs} \bI \right)^{-1} \right\Vert_2 \left\Vert \bE \right\Vert_{2} \right)
      \!\!
      \left( 1 + \left\Vert \left( \bK_{\bX\bX} + \sigma^2_\text{obs} \bI \right)^{-1} \right\Vert_2 \left\Vert \bE \right\Vert_{2} \right)
    \nonumber \\
    &\leq
    \left( 1 + \sigma^{-2}_\text{obs} \Vert \bE \Vert_2 \right)
    \left( 1 + \sigma^{-2}_\text{obs} \Vert \bE \Vert_2 \right)
    \nonumber \\
    &= \left( 1 + \sigma^{-2}_\text{obs} \Vert \bE \Vert_2 \right)^2.
    \label{eqn:cond_number_bound}
  \end{align}

  Since $\bE$ is positive semi-definite, we have that $\Vert \bE \Vert_2 \leq \tr{\bE}$.
  The eigenvalue condition from \cref{eqn:pcp_condition} allows us to bound $\tr{\bE}$ using \cref{thm:harbrecht}:
  %
  \begin{equation}
    \Vert \bE \Vert_2 \leq \tr{\bE}
    =
    \tr{ \bK_{\bX\bX} - \bar\bL_R \bar\bL_R^\top } \leq \bigo{N e^{-BR}}.
    \label{eqn:error_trace_bound}
  \end{equation}
  %
  Plugging \cref{eqn:error_trace_bound} into \cref{eqn:cond_number_bound} completes the proof.
\end{proof}




\subsection{Proof of \cref{thm:precond_mbcg_solves}}
\newtheorem*{thm:precond_mbcg_solves}{\cref{thm:precond_mbcg_solves} (Restated)}
\begin{thm:precond_mbcg_solves}
  Let $\bK_{\bX\bX} \in \reals^{N \times N}$ be a $N \times N$ kernel that satisfies the eigenvalue condition of \cref{eqn:pcp_condition},
	and let $\bar\bL_R$ be its rank-$R$ pivoted Cholesky factor.
	After $J$ iterations of mBCG with preconditioner $\trainP = (\bar\bL_R \bar\bL_R^\top + \sigma_\text{obs}^2 \bI)$,
	the difference between $\bc_J$ and true solution $\trainK^{-1} \by$ is bounded by:
	%
  \begin{equation*}
    \left \Vert \trainK^{-1} \by - \bc_{J} \right \Vert_{\trainK}
    \leq \Bigg[ \frac 1 {1 + \bigo{\sigma^{2}_\text{obs} e^{RB}/N}} \Bigg]^{J}
    \left \Vert \trainK^{-1} \by \right \Vert_{\trainK},
		\nonumber
  \end{equation*}
	%
	where $\trainK = (\bK_{\bX\bX} + \sigma^2_\text{obs} \bI)$ and $B > 0$ is a constant.
\end{thm:precond_mbcg_solves}

\begin{proof}
Since \cref{eqn:pcp_condition} holds, we can simply plug \cref{thm:condition_number} into the standard CG convergence bound (\cref{thm:cg_convergence}):
%
\begin{align*}
  \left \Vert \trainK^{-1} \by - \bc_{J} \right \Vert_{\trainK}
  &\leq
  2 \left[ \frac{ \sqrt{ \kappa\left( \trainP^{-1} \trainK \right)}  - 1 }{\sqrt{ \kappa \left( \trainP^{-1} \trainK \right)} + 1} \right]^J
  \left \Vert \trainK^{-1} \by \right \Vert_{\trainK}
  \\
  &\leq
  2 \Bigg[ \frac{ 1 + \bigo{ \sigma^{-2}_\text{obs} N e^{-BR} } - 1 }{ 1 + \bigo{ \sigma^{-2}_\text{obs} N e^{-BR} } + 1} \Bigg]^J
  \left \Vert \trainK^{-1} \by \right \Vert_{\trainK}
  \\
  &=
  \Bigg[ \frac 1 {1 + \bigo{\sigma^{2}_\text{obs} e^{RB}/N}} \Bigg]^{J}
  \left \Vert \trainK^{-1} \by \right \Vert_{\trainK}.
\end{align*}
\end{proof}




\subsection{Proof of \cref{thm:precond_mbcg_logdet}}
\newtheorem*{thm:precond_mbcg_logdet}{\cref{thm:precond_mbcg_logdet} (Restated)}
\begin{thm:precond_mbcg_logdet}
  Assume $\bK_{\bX\bX} \in \reals^{N \times N}$ satisfies the eigenvalue condition of \cref{eqn:pcp_condition}.
	Suppose we estimate $\Gamma \approx \log \vert \trainP^{-1} \trainK \vert$ using \cref{eqn:slq_precond} with:
	\begin{itemize}
		\item $J \geq \mathcal{O} \left[ (1 + \sigma^{-2}_\text{obs} N e^{-BR}) \log \left( ( 1 + \sigma^{-2}_\text{obs} N e^{-BR} ) / \epsilon \right) \right]$ iterations of mBCG (for some constant $B > 0$), and
		\item $T \geq \frac{32}{\epsilon^2} \log \left( \frac 2 \delta \right)$ random $\bz^{(i)} \sim \normaldist{\bzero}{\trainP}$ vectors.
	\end{itemize}
  Then the error of the stochastic Lanczos quadrature estimate $\Gamma$ is probabilistically bounded by:
  \begin{equation*}
    \textrm{Pr}\left[\Bigl\vert \log \vert \trainP^{-1} \trainK \vert - \Gamma \Bigr\vert \leq \epsilon N \right] \geq \left( 1 - \delta \right).
  \end{equation*}
\end{thm:precond_mbcg_logdet}

\begin{proof}
Since \cref{eqn:pcp_condition} holds, we can simply plug \cref{thm:condition_number} into the stochastic Lanczos quadrature bound of \citet{ubaru2017fast} (\cref{thm:slq_convergence}).
\end{proof}









\section{Applying \cref{thm:precond_mbcg_solves,thm:precond_mbcg_logdet} to Univariate RBF Kernels}

\gp{REDO}
While there has been an enormous amount of work understanding the eigenvalue distributions of kernel functions (e.g., \cite{wathen2015spectral}),
in this paper we prove the following useful bound on the eigenvalue distribution of univariate RBF kernel matrices:
%
\begin{lemma}
\label{thm:eigenvalue_bound}
Given $x_1, \ldots, x_n \in [0, 1]$, the univariate RBF kernel matrix $K_{XX} \in \mathbb{R}^{n \times n}$ with $K_{ij} = \exp \left(-\gamma(x_i - x_j)^{2}\right)$ has eigenvalues bounded by:
\begin{equation*}
  \lambda_{2l+1} \leq
  2n e^{-\gamma/4} I_{l+1}(\gamma/4) \sim
  \frac{2n e^{-\gamma/4}}{\sqrt{\pi\gamma}}
  \left( \frac{e\gamma}{8(l+1)} \right)^{l+1}
\end{equation*}
where $I_j$ denotes the modified Bessel function of the first kind with parameter $j$.
\end{lemma}
%
In other words, the eigenvalues of an RBF kernel matrix $K_{XX}$ decay \emph{super-exponentially}, meeting the requirements of \cref{eqn:pcp_condition} in \cref{thm:condition_number}.
Therefore, the bounds given by \cref{thm:precond_mbcg_solves,thm:precond_mbcg_logdet} apply.

\begin{proof}
  We organize the proof into a series of lemmata.  First, we observe
  that if there is a degree $d$ polynomial that approximates
  $\exp(-\gamma r^2)$ to within some $\epsilon$ on $[-1,1]$,
  then $\lambda_{d+1}(K_{XX}) \leq n\epsilon$
  (\cref{lemma:interpbound}).
  Then in \cref{lemma:errbnd}, we show that if $p_l$ is a
  truncated Chebyshev expansions of degree $2l$, then
  $|p_l(r)-\exp(-\gamma r^2)| < 2 e^{-\gamma/4} I_{l+1}(\gamma/4)$;
  the argument involves a fact about sums of modified Bessel functions
  which we prove in \cref{lemma:Isum}.
  Combining these two lemmas yields the theorem.
\end{proof}
%
\begin{lemma}\label{lemma:interpbound}
  Given nodes $x_1, \ldots, x_n \in [0,1]$, define the kernel matrix
  $K \in \reals^{n \times n}$ with $k_{ij} = \phi(x_i-x_j)$.  Suppose
  the degree $d$ polynomial $q$ satisfies $|\phi(r)-q(r)| \leq
  \epsilon$ for $|r| \leq 1$.  Then
  \[
    \lambda_{d+1}(K) \leq n \epsilon.
  \]
\end{lemma}
\begin{proof}
  Define $\tilde{K} \in \reals^{n \times n}$ with $\tilde{k}_{ij} = q(x_i-x_j)$.
  Each column is a sampling at the $X$ grid of a $\deg(q)$ polynomial, so
  $\tilde{K}$ has rank at most $\deg(q)$.  The entries of the
  error matrix $E = K-\tilde{K}$ are bounded in magnitude by
  $\epsilon$, so $\|E\|_2 \leq n\epsilon$ (e.g.~by Gershgorin's circle theorem).
  Thus, $\lambda_{d+1}(K) \leq \lambda_{d+1}(\tilde{K}) + \|E\|_2 = n\epsilon$.
\end{proof}



\begin{lemma}~\label{lemma:errbnd}
  For $x \in [-1,1]$,
  \[
    |\exp(-\gamma x^2)-p_l(x)| \leq 2 e^{-\gamma/4} I_{l+1}(\gamma/4).
  \]
\end{lemma}
\begin{proof}
Given that
$|(-1)^j T_{2j}(x)| \leq 1$ for any $x \in [-1,1]$, the tail
admits the bound
\[
  |\exp(-\gamma x^2)-p_l(x)| \leq
  2 e^{-\gamma/2} \sum_{j=l+1} I_j(\gamma/2).
\]
Another computation (\cref{lemma:Isum}) bounds the sum of the
modified Bessel functions to yield
\[
  |\exp(-\gamma x^2)-p_l(x)| \leq
  2 e^{-\gamma/4} I_{l+1}(\gamma/4).
\]
\end{proof}


\begin{lemma}\label{lemma:Isum}
  \[
    \sum_{j=l+1}^\infty I_j(\eta) \leq \exp(\eta/2) I_{l+1}(\eta/2)
  \]
\end{lemma}
\begin{proof}
Take the power series expansion
\[
  I_j(\eta) =
    \sum_{m=0}^\infty \frac{1}{m! (m+j)!} \left(\frac{\eta}{2}\right)^{2m+j}
\]
and substitute to obtain
\[
  \sum_{j=l+1}^\infty I_j(\eta) =
    \sum_{j=l+1}^\infty \sum_{m=0}^\infty
    \frac{1}{m! (m+j)!} \left(\frac{\eta}{2}\right)^{2m+j}.
\]
All sums involved converge absolutely, and so we may reorder to obtain
\[
  \sum_{j=l+1}^\infty I_j(\eta) =
  \sum_{m=0}^\infty \frac{1}{m!} \left(\frac{\eta}{2}\right)^m
  \sum_{j=l+1}^\infty
    \frac{1}{(m+j)!} \left(\frac{\eta}{2}\right)^{m+j}.
\]
Because it is the tail of a series expansion for the exponential, we
can rewrite the inner sum as
\[
  \sum_{j=l+1}^\infty
  \frac{1}{(m+j)!} \left(\frac{\eta}{2}\right)^{m+j} =
  \frac{\exp(\xi_m/2)}{(m+l+1)!} \left(\frac{\xi_m}{2}\right)^{m+l+1}
\]
for some $\xi_m$ in $[0,\eta]$, and thus
\[
  \sum_{j=l+1}^\infty
  \frac{1}{(m+j)!} \left(\frac{\eta}{2}\right)^{m+j} \leq
  \frac{\exp(\eta/2)}{(m+l+1)!} \left(\frac{\eta}{2}\right)^{m+l+1}.
\]
Substituting into the previous expression gives
\begin{align*}
  \sum_{j=l+1}^\infty I_j(\eta)
  &\leq
  \sum_{m=0}^\infty \frac{1}{m!} \left(\frac{\eta}{2}\right)^m
  \frac{\exp(\eta/2)}{(m+l+1)!} \left(\frac{\eta}{2}\right)^{m+l+1} \\
  &=
  \exp\left(\frac{\eta}{2}\right)
  \sum_{m=0}^\infty \frac{1}{m! (m+l+1)!}
  \left(\frac{\eta}{2}\right)^{2m+l+1} \\
  &=
  \exp(\eta/2) I_{l+1}(\eta/2).
\end{align*}
\end{proof}
