% !TeX root = ../main.tex

%\item The validation set NLL never increases, which means that there can be no harm done.
%\item The entire process takes almost no computation or implementation effort compared to network training.
%\item Sec.\ref{results} empirically demonstrates our effectiveness on various state-of-the-art neural network models. We obtain much better NLL and calibration than the baselines introduced in Sec.\ref{related}, and even the model left of the gap.
% with the best validation NLL.
%\end{enumerate}
%Before going into the empirical results, we next offer two new perspectives on why our method could work well. Through our analysis, we also hope to bring more insights to neural network training in general.

Modern neural networks exhibit a strange phenomenon: probabilistic error and miscalibration worsen even as classification error is reduced.
We have demonstrated that recent advances in neural network architecture and training -- model capacity, normalization, and regularization -- have strong effects on network calibration.
It remains future work to understand why these trends affect calibration while improving accuracy.
Nevertheless, simple techniques can effectively remedy the miscalibration phenomenon in neural networks.
Temperature scaling is the simplest, fastest, and most straightforward of the methods, and surprisingly is often the most effective.
% We hope that the simplicity of this approach enables practitioners to easily produce well-calibrated neural networks.