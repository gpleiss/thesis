\section{Matrix-Vector Multiplication (MVM) Algorithms for Computing Linear Solves and Other Matrix Functions}
\label{sec:mvms}

The primary focus of this thesis is avoiding the Cholesky decomposition for GP learning and inference.
Recall that the Cholesky decomposition is primarily used to compute matrix solves and log determinants:
\[
  \bL \bL^\top = \trainK, \quad
  (\trainK^{-1}) \by = \bL^{-\top} \bL^{-1} \by, \quad
  \log \vert \trainK \vert = 2 \sum_{i=1}^N \log L^{(ii)}
\]
We will instead compute $(\trainK^{-1}) \by$ and $\log \vert \trainK \vert$ (and other non-linear terms) through algorithms that only utilize \emph{matrix-vector multiplication} ({\bf MVMs}) and other simple vector operations.
This approach, as we will demonstrate, has several advantages:
%
\begin{enumerate*}
  \item it effectively utilizes GPU acceleration,
  \item it reduces memory requirements, and
  \item it simplifies the implementation of specialty models.
\end{enumerate*}
%
We will rely on a family of iterative algorithms known as {\bf Krylov subspace methods} \citep[e.g.][]{saad2003iterative,van2003iterative}, which were originally developed for large sparse matrices.
Importantly, these algorithms can compute non-linear terms \emph{without explicitly computing the matrix} $\trainK$.
Instead, these methods only access $\trainK$ through MVMs.

The next three chapters propose GP-specific Krylov subspace algorithms for training and inference.
In this remainder of this section, we introduce three common Krylov methods that we use as building blocks: {\bf linear conjugate gradients} (CG), {\bf Lanczos tridiagonalization}, and {\bf MINRES}.

\paragraph{Intuition.}
At first glance, it may not be obvious how matrix-vector multiplications---a linear operation---can compute non-linear operations like $\trainK^{-1} \bb$ or $\log \vert \trainK \vert$.
To motivate the use of MVM-based algorithms, we will begin by reformulating matrix solves and log determinants to make their MVM connection obvious.
First, we will view $\trainK^{-1} \by$ through an optimization lens.
Since $\trainK^{-1}$ is positive definite, we note that any matrix solve is the solution to the following convex problem:
%
\begin{equation}
  \trainK^{-1} \by = \argmin_{\bc \in \reals^{N}} \Bigl[ \frac{1}{2} \: \bc^\top (\trainK) \: \bc - \bc^\top \by \Bigr],
  \label{eqn:solve_as_optim}
\end{equation}
%
The gradient of the objective function is $(\trainK) \: \bc - \by$.
Thus, one could compute $\trainK^{-1} \by$ with gradient descent, and each gradient step only accesses $\trainK$ through a MVM with the current solution $\bc$.
Rather than using vanilla gradient descent however, we will instead use a special decent method (conjugate gradients) that carefully chooses its updates for faster convergence.

The connection between log determinants and MVMs is less straightforward.
At a high-level, we note that the eigenvalues of $\trainK$ can be found through MVM iterations.
For example, the \emph{power method} computes the largest eigenvector by running the iteration $\bc_{j+1} = (\trainK) \bc_{j}$ until convergence.\footnote{
  Let $\lambda_i, \bv_i$ be the $i^\text{th}$ eigenvalue/vector of $\trainK$.
  If we write the initial vector as a linear combination of the eigenvectors $\bc_{0} = \sum_{i=1}^N w_i \bv_i$, then the iterations simply scale the weights $w_i$ by a factor $\lambda_i$:
  $\bc_{j} = \sum_{i=1}^N w_i \lambda_i^j \bv_i$.
  After enough iterations, the largest eigenvalue $\lambda_1^j$ will dominate all others and so $\bc_j \approx w_1 \lambda_1^j \bv_1$.
  See \citep[e.g.][Ch. 8]{golub2012matrix} for details.
}
In practice computing a complete eigendecomposition of $\trainK$ through power iteration would be a highly inefficient way to compute log determinants.
We will instead compute these terms through stochastic Lanczos quadrature \citep{ubaru2017fast} which similarly uses MVMs but avoids a full eigendecomposition.

With these intuitions in place, we will now introduce linear conjugate gradients, Lanczos tridiagonalization, and MINRES.
These methods expand upon the simple iterative procedures we outline above, yet converge to the true solutions in many fewer iterations.



\subsection{Linear Conjugate Gradients}
\label{sec:cg}

Linear conjugate gradients (CG) \cite{hestenes1952methods} is an iterative method for solving the optimization problem posed in \cref{eqn:solve_as_optim} if $\bK$ is positive definite.
At a high level, it is similar to gradient descent: each iteration updates an existing solution by taking a step in a ``search'' direction.
Unlike gradient descent, a good solution to $\bK^{-1} \bb$ is often found in $\ll N$ iterations, where $N$ is the size of the matrix $\bK$.
Moreover, each iteration requires only a single MVM with $\bK$.

Here we offer a high level overview of the algorithm and its convergence properties.
Because CG is notoriously confusing and difficult to intuit, we refer the reader to \citet{shewchuk1994introduction} for a gentler introduction.

\paragraph{Overview.}
Conjugate gradients differs from vanilla gradient descent in two ways: its' choice of the decent direction and its' choice of step size.
Standard gradient descent takes a constant step $\varphi$ in the direction of the gradient $\br_j$ at every iteration:
%
\begin{equation*}
  \bc_{j+1}^{\text(GD)} = \bc_{j} - \varphi \br_{j}, \quad
  \br_{j} = \bK \bc_{j} - \bb.
\end{equation*}
%
($\br_j$ is also referred to as the ``residual'' of the current solution.)
This approach is inefficient because multiple iterations might take steps in the same direction.

On the other hand, the descent directions taken by conjugate gradients $\bd_1, \ldots, \bd_J$ are \emph{mutually conjugate} to one another.
This means that all search directions are orthogonal with respect to the inner product defined by $\bK$---i.e. for all $j \ne j'$, we have $\langle \bd_j, \bd_{j'} \rangle_\bK = \bd_j^\top \bK \bd_{j'} = 0$.
A consequence of this is that no two iterations move the solution $\bc_{j+1}$ in the same direction.
Each iteration takes a variable step size $\alpha_j$ that corresponds to the optimal step in the $\bd_j$ direction.
The resulting iterations are:
%
\begin{equation*}
  \bc_{j+1}^{\text(CG)} = \bc_{j} - \alpha_j \bd_{j}.
\end{equation*}
%
where $\bd_j$ and $\alpha_j$ are determined using formulas described below.
The $N$ conjugate descent vectors $\bd_1, \ldots, \bd_N$ form a basis of $\reals^N$, and therefore we can describe the solution as $\bK^{-1} \bb = \bc_0 - \sum_{j=1}^N \alpha_j \bd_j.$
In other words, these iterations find the true solution $\bK^{-1} \bb$ in at most $N$ iterations, though often $\ll N$ iterations finds a very close approximation.
%(Again, the $\alpha_j$ steps are ``optimal'' with respect to $\bK^{-1} \bb$.)

\paragraph{The algorithm.}
By carefully choosing the conjugate descent directions $\bd_1, \ldots, \bd_N$, the CG iterations can be computed through an extremely efficient recurrence relation.
At iteration $j$, the descent direction $\bd_j$ is given by taking the residual $\br_j = \bK \bc_{j} - \bb$ and making it conjugate to all previous descent directions:
%
\begin{equation*}
  \bd_j = \br_j - \sum_{\ell = 1}^{j-1} \frac{ \bd_\ell^\top \bK \br_j }{ \bd_\ell^\top \bK \bd_\ell } \bd_\ell.
\end{equation*}
%
(The above equation is very similar to Gram-Schmidt orthogonalization, except that the resulting vectors are \emph{conjugate} rather than \emph{orthogonal}.)
In practice, the sum reduces down to a single term $( \bd_{j-1}^\top \bK \br_j ) / ( \bd_{j-1}^\top \bK \bd_{j-1} ) \bd_{j-1}$ since the residual already happens to be conjugate to all other search directions (see \citep[Sec. 7-8]{shewchuk1994introduction} for a complete derivation).
Moreover, the residual $\br_j$ and the step size $\alpha_j$ can be computed through inner products of existing terms.
As a result, each iteration of CG requires only a single MVM ($\bK \bd_{j-1}$) and a few additional vector inner products.
The entire algorithm is thus a recurrence relation involving 3 vectors (see \cref{alg:cg}), which makes CG very fast and memory efficient.

\input algorithms/cg

\paragraph{Convergence.}
While $N$ iterations of conjugate gradients finds the exact solution, often $J \ll N$ iterations find a solution \emph{within numerical precision} of the true solution.
Through some numerical manipulation \citep[][Ch. 9]{shewchuk1994introduction}, one can show that the solution $\bc_J$ at iteration $J$ is optimal with respect to norm induced by the matrix $\bK$:
%
\begin{align}
  \bc_J
  &= \argmin_{\bc \in \mathcal{K}_J (\bK, \bb)} \frac{1}{2} \bc^\top \bK \bc - \bc^\top \bb
  \nonumber \\
  &= \argmin_{\bc \in \mathcal{K}_J (\bK, \bb)} \Vert \bc - (\bK^{-1} \bb) \Vert_\bK,
  \label{eqn:cg_optimality}
\end{align}
%
where $\mathcal{K}_J (\bK, \bb)$---the domain that $\bc_J$ is optimal over---is referred to as the $J^\text{th}$ {\bf Krylov subpace} of $\bK$ and $\bb$.
%
\begin{equation}
  \mathcal{K}_J (\bK, \bb) = \left\{ \bb, \:\: \bK \bb, \:\: \bK^2 \bb, \:\: \ldots, \:\: \bK^{J-1} \bb \right\}.
  \label{eqn:krylov}
\end{equation}
%
(This is why CG is referred to as a ``Krylov Subspace Method''.)
Note the connection between \cref{eqn:krylov} and MVMs:
the vectors spanning $\mathcal{K}_J (\bK, \bb)$ can be formed by applying a MVM to the previous vector: $\bK^J \bb = \bK \left( \bK^{J-1} \bb \right)$.
The CG search directions $\bd_1, \ldots, \bd_J$ lie in the Krylov subspace, and so the matrix-vector multiplication $\bK \bd_{J-1}$ in \cref{alg:cg} expands $\mathcal{K}_{J}(\bK, \bb)$ to the next subspace $\mathcal{K}_{J+1} (\bK, \bb)$.

\cref{eqn:cg_optimality,eqn:krylov} explain why CG rapidly converges.
Since the Krylov subspace contains powers of $\bK$ applied to $\bb$, we can interpret \cref{eqn:cg_optimality} as an \emph{optimal polynomial approximation} of $\bK^{-1}$:
%
\begin{align*}
  \bc_J &= \argmin_{\text{poly}_J(\bK)} \Vert \text{poly}_J(\bK)\bb - (\bK^{-1} \bb) \Vert_\bK,
\end{align*}
%
where $\text{poly}_J(\bK)$ is a $J^\text{th}$-degree polynomial.
From this perspective, one can derive CG's famous (though loose) convergence bound (see \citep[e.g.][Ch. 9]{shewchuk1994introduction}):
%
\begin{theorem}[Convergence of CG]
  \label{thm:cg_convergence}
  Let $\bK$ be a $N \times N$ positive definite matrix and let $\bb$ be a vector in $\reals^N$.
  After $J$ iterations of CG, the difference between $\bc_J$ and the true solution $\bK^{-1} \bb$ is bounded by:
  \begin{equation*}
    \Vert \bc_J - (\bK^{-1} \bb) \Vert_\bK
    \leq
    2 \Bigg[ \frac{ \sqrt{ \kappa(\bK) } - 1 }{\sqrt{ \kappa(\bK) } + 1} \Biggr]^J \Vert \bb \Vert_\bK,
  \end{equation*}
  %
  where $\kappa(\bK) = \Vert \bK \Vert_2 / \Vert \bK^{-1} \Vert_2$ is the \emph{condition number} of $\bK$.
\end{theorem}
%
\noindent
Thus CG converges exponentially to the true solve $\bK \bb^{-1}$.
This bound depends on the condition number of $\bK$, which is often quite large for GP kernel matrices.
In practice however, convergence can be much faster if, for example, the eigenvalues of $\bK$ are clustered \cite{saad2003iterative}.
We find that a few hundred iterations of CG often produces solves within 3-5 decimal places for most kernel matrices, even when $N \geq 100,\!000$.

\paragraph{Preconditioning.}
A commonly-used approach to accelerate CG convergence is to lower the $\kappa(\bK)$ condition number in \cref{thm:cg_convergence}.
This can be accomplished through {\bf preconditioning}, which introduces a matrix $\bP$ to solve the related linear system
%
\[
  \bP^{-1} \bK \bc = \bP^{-1} \bb
\]
%
instead of $\bK^{-1} \bb$.
Both systems have the same solution $\bc$, but the preconditioned system's convergence depends on the conditioning of $\bP^{-1} \bK$ rather than that of $\bK$.
The algorithm for \emph{preconditioned conjugate gradients} (PCG) is essentially the same as vanilla CG, with the additional step of applying $\bP^{-1}$ to the MVMs (i.e. $\bP^{-1} \bK \bd_{j-1}$) and residuals (i.e. $\bP^{-1} \br_j$).
See \cref{alg:std_pcg} for details.

\input algorithms/pcg

Choosing a preconditioner $\bP^{-1}$ is a trade-off between computational efficiency and effectiveness.
Trivially, the best preconditioner is $\bP^{-1} = \bK^{-1}$ which would reduce the condition number in \cref{thm:cg_convergence} to $\kappa = 1$.
However this is obviously not a practical choice---if we already had a way to efficiently compute $\bK^{-1}$ there would be no need to run CG!
The most effective preconditioners have simple-to-compute inverses (e.g. diagonal matrices), yet are able to closely approximate $\bK^{-1}$.
A common (though often ineffective) choice of $\bP$ is the \emph{Jacobi preconditioner}, which is simply the diagonal of $\bK$ (i.e. $\bP = \text{diag}(\bK)$).

The design of effective preconditioners is an active area of research and is too extensive to adequately review here.
We refer the reader to \cite{saad2003iterative} which devotes two chapters to different preconditioning methods.

%\subsection{Overview}

%Given an $N \times N$ matrix $\bK$ and a vector $\bb$, {\bf Krylov subspace methods} are a broad family of \emph{iterative} algorithms for computing $f(\bK) \bb$, where $f(\cdot)$ is a matrix function.
%For example $f (\cdot)$ could be the matrix inverse (i.e. $\bK^{-1} \bb$) or the matrix logarithm (i.e. $\log ( \bK ) \bb$).
%%Importantly, Krylove these problems \emph{without explicitly computing the matrix $\bK$}.
%%Each iteration only requires a \emph{matrix-vector multiplication} (MVM)---i.e. $\bK \bv$ for some vector $\bv$---plus a few additional vector operations.
%Krylov subspace methods are desirable for two primary reasons.
%Firstly, these methods compute $f(\bK) \bb$ \emph{without explicitly computing the matrix $\bK$}.
%Instead, the methods simply access the matrix $\bK$ through a {\bf matrix-vector multiplication (MVM)} routine (i.e. $\bK \bv$ for some vector $\bv$),
%which is especially beneficial for sparse or structured matrices that afford efficient MVM routines.
%Secondly, Krylov subspace methods tend to converge rapidly.
%Though exact solutions to $f(\bK)\bb$ typically require $N$ iterations (and therefore $N$ MVMs), machine precision is often achieved in $\ll N$ iterations.
%The exact convergence rate depends on the conditioning of $\bK$ or the structure of its eigenvalues.

%In this thesis we will make use of two Krylov algorithms:
%The former can be applied to any symmetric matrix, whereas the latter is only for positive definite matrices.
%We will briefly introduce these two methods in this section and draw a connection between them.


\subsection{Lanczos Tridiagonalization}
\label{sec:lanczos}

The \citet{lanczos1950iteration} algorithm iteratively produces a partial tridiagonalization of the symmetric matrix $\bK \! \in \! \reals^{N \! \times \! N}$.
At iteration $J$, $\bK$ is factorized as
%
\begin{equation}
  \bK \bQ_J = \bQ_J \bT_J + \br_J \be_{J}^\top,
  \label{eqn:lanczos}
\end{equation}
%
where $\bQ_J \in \reals^{N \times J}$ has orthonormal columns, $\bT_J \in \reals^{J \times J}$ is symmetric tridiagonal, and $(\br_J \be_{J}^\top) \in \reals^{N \times J}$ is a residual term involving the unit vector $\be_{J}$.
After $N$ iterations, the residual term will be zero and we recover the complete tridiagonalization $\bK = \bQ_N \bT_N \bQ^\top_N$.
In practice however, the complete tridiagonalization is almost never computed.
The matrices $\bQ_J, \bT_J$ after $J \ll N$ iterations can approximate functions involving $\bK$ with high degrees of accuracy.
We will discuss how $\bQ_J$ and $\bT_J$ can approximate log determinants and matrix solves after introducing the algorithm itself.

\paragraph{The algorithm.}
Given an initial vector $\bb$, $J$ iterations of the Lanczos algorithm form an orthonormal basis of the $J^\text{th}$ Krylov subspace (\cref{eqn:krylov}).
%$\mathcal{K}_J (\bK, \bb) = \left\{ \bb, \:\: \bK \bb, \:\: \bK^2 \bb, \:\: \ldots, \:\: \bK^{J-1} \bb \right\}.$
%%
%\begin{equation}
  %\mathcal{K}_J (\bK, \bb) = \left\{ \bb, \:\: \bK \bb, \:\: \bK^2 \bb, \:\: \ldots, \:\: \bK^{J-1} \bb \right\}.
  %\label{eqn:krylov}
%\end{equation}
%%
%$\mathcal{K}_J (\bK, \bb)$ is known as the $J^\text{th}$ \emph{Krylov Subspace} of $\bK$ and $\bb$ (hence why this algorithm is classified as a ``Krylov Subspace Method''---see \citep{saad2003iterative} for more details).
Applying Gram-Schmidt orthogonalization to $\left[ \bb, \:\: \bK \bb, \:\: \ldots, \:\: \bK^{J-1} \bb \right]$ produces the columns of $\bQ_J = \left[ \bb/\Vert \bb \Vert, \:\: \bq^{(2)}, \:\: \ldots, \:\: \bq^{(J)} \right]$, and the orthogonalization coefficients are collected into $\bT_J$.
Because $\bK$ is symmetric, each vector only needs to be orthogonalized against the two preceding vectors, which results in the tridiagonal structure of $\bT$ \cite{golub2012matrix}.
In practice, the columns of $\bQ_J$ and the sub-diagonal/diagonal entries of $\mathbf T_j$ can be computed using a simple iterative procedure (\cref{alg:lanczos}).
%
\input algorithms/lanczos
%
Each iteration produces a new column of $\bQ_J$, requiring a single MVM with $\bK$.
In total $J$ iterations requires $J$ MVMs.

In practice, \cref{alg:lanczos} can be numerically unstable after many iterations of $J$ (e.g. $J > 50$).
This is because the columns of $\bQ_J$ lose orthogonality due to round-off errors.
We discuss ways to overcome these instabilities in \cref{sec:love_discussion}.


%Before introducing the algorithms, we will fist define Krylov subspaces.
%Formally, given an $n \times n$ matrix $\bK \in \reals^{n \times n}$ and a vector $\bb \in \reals^{n}$, the $J^\text{th}$ \emph{Krylov subspace} is defined as:
%\[
  %\mathcal{K}_{J}(\bK,\bb) = \text{span}\left\{ \bb, \:\: \bK \bb, \:\: \bK^2 \bb, \:\: \ldots, \:\: \bK^{J-1} \bb \right\}.
%\]
%Importantly, note that we can compute the $(J + 1)^\text{th}$ subspace from the $J^\text{th}$ subspace simply by performing an additional matrix-vector multiplication with $\bK$.
%If $\bK$ is full-rank and $\bb$ is not an eigenvector of $\bK$, then the $n^\text{th}$ Krylov subspace will span the entire vector space $\mathcal{K}_{n}(\bK,\bb) = \reals^n$.

%If $\bK$ is a symmetric matrix, the {\bf Lanczos algorithm} provides an efficient way to compute an orthonormal basis of $\mathcal{K}_J$: %%
%\[
  %\bQ_J = \begin{bmatrix} \bq_1, & \bq^{(2)}, & \cdots, & \bq^{(J)} \end{bmatrix}, \quad
  %\bq_1 = \bb / \Vert \bb \Vert^2, \quad
  %\text{span} \left\{ \bQ_J \right\} = \mathcal{K}_{J}(\bK, \bb).
%\]
%%
%The columns of $\bQ_J \in \reals^{N \times J}$ are computed by applying Gram-Schmidt orthogonalization to the MVMs $\{ \bb, \:\: \bK \bb, \:\: \cdots, \:\: \bK^{J-1} \bb \}$.
%Let $\bT_J$ be a $J \times J$ matrix where each column $\bt_i$ contains the orthogonalization coefficients necessary for computing $\bq_{i + 1}$.
%Because $\bK$ is symmetric, each vector $\bq_i$ needs only be orthogonalized against the two preceding vectors ($\bq_{i-1}$ and $\bq_{i-2}$), which results in $\bT_J$ having tridiagonal structure \cite{golub2012matrix}.
%Moreover, this orthogonalization process can be summarized through the following relation of $\bK$, $\bQ_J$, and $\bT_J$:
%%
%\begin{equation}
  %\bK \bQ_J = \bQ_J \bT_J + \br_J \be_{J}^\top,
  %\label{eqn:lanczos}
%\end{equation}
%%
%where $\br_J \in \reals^J$ is a residual vector given by $\br_J = (\bK - T_{[J,J]} \bI) \bq^{(J)} - (T_{[J,J-1]} \bq_{J-1})$
%and $\be_{J} \in \reals^J$ is the unit vector $[ 0, 0, \ldots, 1 ]$.
%If $J=N$, then $\bQ_N$ and $\bT_N$ form a complete tridiagonalization $\bK = \bQ_N \bT_N \bQ_N^\top$.

%The Lanczos matrices $\bQ_J$ and $\bT_J$ can be used to estimate a wide variety of matrix functions.
%We will first discuss these applications of the Lanczos algorithm before outlining how to compute it in practice.

\paragraph{Using Lanczos to estimate $f (\bK) \bb$.}
Lanczos tridiagonalization is a general-purpose algorithm, as it can be used to compute an approximation for any $f(\bK) \bb$, where $f( \cdot )$ is a matrix function (i.e. matrix inverse or matrix logarithm).
First, we note that, if $\bQ_N \bT_N \bQ_N^\top = \bK$ is a complete tridiagonalization, then $\bT_N$ and $\bK$ are similar matrices and therefore:
\begin{equation}
  f ( \bK ) = \bQ_N \bigl[ f ( \bT_N ) \bigr] \bQ_N^\top.
  \label{eqn:f_tridiagonal}
\end{equation}
%
%This can be a computationally advantageous way to compute $f(\bK)$ since we can often exploit the tridiagonal structure of $f (\bT_N)$.
%For example, consider $f(\cdot) = (\cdot)^{-1}$.
%We can invert tridiagonal matrices in $\bigo{N}$ time rather than $\bigo{N^3}$ time for general matrices.
%
Typically, the Lanczos algorithm is used only to compute a \emph{partial} tridiagonalization $\bQ_J$, $\bT_J$ with $J \ll N$.
However, this partial tridiagonalization can be used to approximate $f ( \bK )$ applied to the initial Lanczos vector $\bb$:
%
\begin{align}
  f ( \bK ) \bb
  &\approx \Bigl( \bQ_J \bigl[ f ( \bT_J ) \bigr] \bQ_J^\top \Bigr) \bb.
  \nonumber \\
  &= \Vert \bb \Vert_2 \Bigl( \bQ_J \bigl[ f ( \bT_J ) \bigr] \Bigr) \be_{1}
  \triangleq \bc_J,
  \label{eqn:f_lanczos}
\end{align}
%
where the second line holds because $\bq_1 = \bb / \Vert \bb \Vert$.
The estimate $\bc_J$ tends to converge exponentially as $J$ increases, though the rate of convergence depends on the conditioning of $\bK$.
Moreover, the computational complexity of \cref{eqn:f_lanczos} is quite small since we can take advantage of the tridiagonal structure of $\bT_J$.

For many functions of interest, there are special variants of \cref{eqn:f_lanczos} that offer more efficient computation.
We now outline two special cases that can be applied to Gaussian process inference.

\paragraph{Estimating log determinants.}

\citet{ubaru2017fast} introduce a method to produce unbiased estimates of log determinants using Lanczos tridiagonalization and \emph{stochastic trace estimation} \cite{hutchinson1990stochastic,avron2011randomized,fitzsimons2016improved}.
%First, given i.i.d random variables $\bz_1, \ldots, \bz_t$ with $\Ev{\bz^{(i)}} = 0$ and $\Ev{\bz^{(i)} \bz^{(i)}}^\top = \bI$ (e.g. $\bz^{(i)} \sim \normaldist{0}{\bI})$, note that the trace operation can be written as $\tr{ \cdot} = \Ev{ \bz^{(i)}^\top \left( \cdot \right) \bz^{(i)}^\top }.$
They note that the log determinant can be re-written as:
%
\begin{align}
  \log \vert \bK \vert &= \tr{ \log \bK }
  \nonumber \\
  &\approx \frac{1}{T} \sum_{i=1}^T \bz^{(i)^\top} \left( \log \bK \right) \bz^{(i)}.
  \label{eqn:logdet_sta}
\end{align}
%
where $\log \bK$ denotes the matrix logarithm, and $\bz^{(i)}$ are i.i.d random variables with zero mean and unit covariance (e.g. $\bz^{(i)} \sim \normaldist{0}{\bI}$).
It can be seen with a little mathematical manipulation that the second line is an unbiased estimate of the trace operator \cite{hutchinson1990stochastic}.

To compute each of the $\bz^{(i)} \left( \log \bK \right) \bz^{(i)}$ terms, we turn to \cref{eqn:f_tridiagonal}.
Let $\bQ_J^{(i)}$, $\bT_J^{(i)}$ correspond to the Lanczos matrices with initial vector $\bq^{(i)}_1 = \bz^{(i)} / \Vert \bz^{(i)} \Vert$.
Then we apply \cref{eqn:f_lanczos}:
%
\begin{align}
  \bz^{(i)^\top} \left( \log \bK \right) \bz^{(i)}
  &\approx \bz^{(i)^\top} \Bigl( \bQ^{(i)}_J \bigl[ \log \bT^{(i)}_J \bigr] \bQ^{(i)\top}_J \Bigr) \bz^{(i)}
  \nonumber
  \\
  &= \Vert \bz^{(i)} \Vert^2_2 \: \Bigl( \be_{1}^{\top} \bigl[ \log \bT^{(i)}_J \bigr] \be_{1} \Bigr).
  \label{eqn:slq}
\end{align}
%
Combining \cref{eqn:logdet_sta} and \cref{eqn:slq} gives us an unbiased estimate of $\log \vert \bK \vert$.
This estimate, referred to as the {\bf stochastic Lanczos quadrature} estimate of $\log \vert \bK \vert$, converges exponentially in $J$ (the number of Lanczos iterations):
%
\begin{theorem}[Corrolary 4.5 of \citet{ubaru2017fast}]
  \label{thm:slq_convergence}
  Let $\bK \in \reals^{N \times N}$ be a positive definite matrix with condition number $\kappa( \bK )$.
	Suppose we estimate $\Gamma \approx \log \vert \bK \vert$ using \cref{eqn:logdet_sta,eqn:slq} with:
	\begin{itemize}
		\item $J \geq \frac{1}{4} \sqrt{ 3 \kappa(\bK) } \log \left( \bigo{\kappa(\bK)} / \epsilon \right)$ iterations of Lanczos,\footnote{
			The exact value for the $\bigo{\kappa(\bK)}$ constant is $\left( 5 \kappa(\bK) \log ( 2 (\kappa(\bK) + 1)) / \sqrt{ 2(\kappa(\bK) + 1) } \right)$.
		} and
		\item $T \geq \frac{32}{\epsilon^2} \log \left( \frac 2 \delta \right)$ random $\bz^{(i)} \sim \normaldist{\bzero}{\bI}$ vectors.\footnote{
			The constant originally used by \citet{ubaru2017fast} assumes that the $\bz^{(i)}$ are Rademacher random variables rather than Gaussian random variables.
			The variance of stochastic trace estimation with Gaussian variables is bounded with a factor of $32$ rather than $24$ (see \citep[][Eqs. 4 and 5]{roosta2015improved}), and so we have adjusted the constant accordingly.
		}
	\end{itemize}
  Then the error of the stochastic Lanczos quadrature estimate $\Gamma$ is probabilistically bounded by:
  \begin{equation*}
    \textrm{Pr}\left[\Bigl\vert \log \vert \bK \vert - \Gamma \Bigr\vert \leq \epsilon N \right] \geq \left( 1 - \delta \right).
  \end{equation*}
\end{theorem}

\paragraph{Computing matrix solves.}
Another common application of Lanczos tridiagonalization is computing $\bK^{-1} \bb$.
Let $\bQ_J$ and $\bT_J$ be the Lanczos matrices using $\bb$ as the initial vector (i.e. $\bq_1 = \bb / \Vert \bb \Vert$).
We can approximate the matrix solve using \cref{eqn:f_lanczos}:
%
\begin{equation}
  \bK^{-1} \bb \approx \Vert \bb \Vert_2 \bQ_J \bT_J^{-1} \be_{1}
  \label{eqn:lanczos_solves}
\end{equation}
%
where again $\be_{1} \in \reals^J$ is the first unit vector $[1, 0, 0, \ldots, 0]$.




\subsection{Connection between CG and Lanczos}
\label{sec:cg_lanczos_connection}

The Lanczos and CG algorithms are very closely related, as they are both Krylov subspace methods.
Here we show that the two algorithms can be derived from one another.
We will exploit this fact in the next two chapters.

\paragraph{Deriving CG from Lanczos.}
It can be shown that, if $\bK$ is positive definite, then the solution in \cref{eqn:lanczos_solves} is \emph{exactly} the same as the CG solution after $J$ iterations.
In fact, one way to derive \cref{alg:cg} is by showing that \cref{eqn:lanczos_solves} can be reduced to the three-term CG recurrence.
In practice, the CG algorithm tends to be preferred over \cref{eqn:lanczos_solves} for computing $\bK^{-1} \bb$.
The advantages of CG are
\begin{enumerate}
  \item CG only stores 3 vectors at any given iteration, whereas \cref{eqn:lanczos_solves} requires storing the $\bQ_J \! \in \! \reals^{N \times J}$ matrix;
  \item CG is numerically stable, whereas the Lanczos vectors lose orthogonality (after say $J > 50$ iterations); and
  \item CG is more easily preconditioned.
\end{enumerate}
However, one advantage of Lanczos is that the $\bQ_J$ and $\bT_J$ matrices can be used to jump-start subsequent solves $\bK^{-1} \bb'$.
\citet{parlett1980new}, \citet{saad1987lanczos}, and \citet{schneider2001krylov} argue that subsequent solves can be approximated as
%
\begin{equation}
  \bK^{-1} \bb' \approx \left( \bQ_J \bT_J^{-1} \bQ_J^\top \right) \bb',
  \label{eqn:lanczos_solve}
\end{equation}
%
where $\bQ_J$ and $\bT_J$ come from a previous solve.\footnote{
  Alternatively, one could use $\left( \bQ_J \bT_J^{-1} \bQ_J^\top \right) \bb'$ as an ``initial guess'' to the solve $\bK^{-1} \bb'$ that can be refined with additional CG iterations.
}
We will use this in \cref{chapter:love}.

\paragraph{Deriving Lanczos from CG.}
Additionally, one can recover part of the Lanczos tridiagonalization from conjugate gradients.
\citet{saad2003iterative} and others show that it is possible to recover the $\bT_J$ tridigonal Lanczos matrix by {reusing the $\alpha_j$ and $\beta_j$ coefficients} generated in the CG iterations (see \cref{alg:std_pcg}).
%
\begin{observation}[Recovering Lanczos tridiagonal matrices from PCG \cite{saad2003iterative}]
  Assume we use $J$ iterations of standard preconditioned conjugate gradients to solve $\bK^{-1} \bb$ with preconditioner $\bP$.
  Let $\alpha_1, \ldots, \alpha_p$ and $\beta_1, \ldots, \beta_p$ be the scalar coefficients from each iteration (defined in \cref{alg:std_pcg}).
  The matrix
  %
  \begin{equation*}
    \left[\begin{array}{ccccc}
      \frac{1}{\alpha_1} & \frac{\sqrt{\beta_1}}{\alpha_1} &  & & 0 \\
      \frac{\sqrt{\beta_1}}{\alpha_1} & \frac{1}{\alpha_2} + \frac{\beta_1}{\alpha_1} & \frac{\sqrt{\beta_2}}{\alpha_2} &  &  \\
      & \frac{\sqrt{\beta_2}}{\alpha_2} & \frac{1}{\alpha_3} + \frac{\beta_2}{\alpha_2} & \frac{\sqrt{\beta_3}}{\alpha_3} &  \\
      &       & \ddots & \ddots & \frac{\sqrt{\beta_{m-1}}}{\alpha_{m-1}} \\
      0 &       &        & \frac{\sqrt{\beta_{m-1}}}{\alpha_{m-1}} & \frac{1}{\alpha_m} + \frac{\beta_{m-1}}{\alpha_{m-1}}
    \end{array}\right]
  \end{equation*}
  %
  is equal to the Lanczos tridiagonal matrix $\bT_J$, formed by running $J$ iterations of Lanczos to achieve $\left( \bP^{-1} \bK \right) \bQ_J = \bQ_J \bT_J + \br \be_{J}^{\top}$) with probe vector $\bb$.
  \label{obs:lanczos_cg}
\end{observation}
%
\noindent
(See \cite{saad2003iterative}, Section 6.7.3.)
In other words, we can recover the Lanczos tridiagonal matrix $\bT_J$ simply by running CG.
However, the orthonormal matrix $\bQ_J$ cannot be as easily derived as a CG byproduct.


\subsection{MINRES}
\label{sec:minres}

The method of minimum residuals (MINRES) \cite{paige1975solution} is an alternative to linear conjugate gradients, with the advantage that it can be applied to indefinite symmetric matrices.
\citet{paige1975solution} formulate MINRES to solve the least-squares problem $\argmin_{\bc} \Vert \bK \bc - \bb \Vert_2$.
Each iteration $J$ produces a solution $\bc_J$ which is optimal within the $J^\text{th}$ Krylov subspace:
%
\begin{equation}
	\bc_J^{(\text{MINRES})} = \argmin_{\bc \in \mathcal{K}_J(\bK, \bb)} \Vert \bK \bc - \bb \Vert_2.
	\label{eqn:minres_highlevel}
\end{equation}
%
Note the primary differences between \cref{eqn:minres_highlevel} and the CG optimality equation (\cref{eqn:cg_optimality}):
CG optimizes the \emph{error} with respect to the $\bK$ norm, while MINRES optimizes the \emph{residual} with respect to the $2$ norm.
This is why MINRES can be applied to indefinite matrices while CG cannot.

Through some mathematical manipulation, one can reformulate \cref{eqn:minres_highlevel} as an unconstrained optimization problem:
%
\begin{align}
  \bc_J^{(\text{MINRES})} = \Vert \bb \Vert_2 \bQ_J \bz_J,
  \quad
  \bz_J = \argmin_{\bz \in \reals^J} \left\Vert
		\left( \widetilde \bT_J \right)\bz - \be_1
	\right\Vert_2,
	\quad
  \widetilde \bT_J \begin{bmatrix} \bT_J \\ \Vert \br_J \Vert_2 \be_J^\top  \end{bmatrix},
  \label{eqn:minres_ols}
\end{align}
%
where $\be_1, \be_J$ are unit vectors, and $\bQ_J$, $\bT_J$, and $\br_J$ are the outputs from the Lanczos algorithm.
Since \cref{eqn:minres_ols} is a least-squares problem, we can write its analytic solution using the QR factorization of $ \widetilde \bT_J = \bQU_J \bR_J$:
%
\begin{align}
  \bc_J^{(\text{MINRES})} = \Vert \bb \Vert_2 \: \bQ_J \left( \bR_J^{-1} \bQU_J^\top \right) \be_1.
	\label{eqn:minres_qr}
\end{align}
%
One way to perform MINRES is first running $J$ iterations of the Lanczos algorithm, computing $\widetilde \bT_J = \bQU_J \bR_J$, and then plugging the resulting $\bQ_J$, $\bQU_J$, and $\bR_J$ into \cref{eqn:minres_qr}.
\citeauthor{paige1975solution} instead introduce a vector recurrence to iteratively compute $\bc_J^\text{(MINRES)}$.
This recurrence relation, which is given by \cref{alg:minres} and broadly described below, is exactly equivalent to \cref{eqn:minres_qr}; however it uses careful bookkeeping to avoid storing any $N \times J$ terms.

First we note that the $\widetilde \bT_J$ matrices are formed recursively, and thus their QR factorizations are also recursive:
%
\[
  \bQU^\top \widetilde \bT_J = \begin{bmatrix}
    \bQU_{J-1}^\top & \bqu^{\top(J,1:J-1)} \\
    \bqu^{\top{(1:J-1,J+1)}} & \mathcal{Q}^{(J,J+1)}
  \end{bmatrix} \begin{bmatrix}
    \widetilde \bT_{J-1} & \bt^{(J)} \\ \bzero^\top & \Vert \br_J \Vert
  \end{bmatrix}
  = \begin{bmatrix}
    \bR_{J-1} & {\br^{(J,1:J-1)}} \\
    \bzero & R^{(J,J)}
  \end{bmatrix} = \bR_J
\]
%
where $\bt^{(J)}$ and $[\br^{(J,1:J-1)}; R^{(J,J)}]$ are the last columns of $\bT_J$ and $\bR_J$ respectively.
Moreover, if we recursively form $\bR_J^{-1}$ as
%
\[
  \bR_J^{-1} = \begin{bmatrix}
    \bR_{J-1} & {\br^{(J,1:J-1)}} \\
    \bzero & R^{(J,J)}
  \end{bmatrix}^{-1}
  = \begin{bmatrix}
    \bR_{J-1}^{-1} & \left( \bR_{J-1}^{-1} {\br^{(J,1:J-1)}} \right) / R^{(J,J)} \\
    \bzero & 1 / R^{(J,J)}
  \end{bmatrix},
\]
%
then \cref{eqn:minres_qr} can be re-written in a decent-style update:
%
\begin{align}
  \bc_J^{(\text{MINRES})} &=
  \Vert \bb \Vert_2 \begin{bmatrix}
    \bQ_{J-1} \bq^{(J)}
  \end{bmatrix} \begin{bmatrix}
    \bR_{J-1}^{-1} & \frac{ \bR_{J-1}^{-1} {\br^{(J,1:J-1)}} }{ R^{(J,J)} } \\
    \bzero & 1 / R^{(J,J)}
  \end{bmatrix} \begin{bmatrix}
    \bQU_{J-1}^\top & \bqu^{\top(J,1:J-1)} \\
    \bqu^{\top{(1:J-1,J+1)}} & \mathcal{Q}^{(J,J+1)}
  \end{bmatrix} \be_1
  \nonumber \\
  &=
  \Vert \bb \Vert_2
  \begin{bmatrix}
    \bQ_{J-1} \bR_{J-1}^{-1} & \frac{ \bQ_{J-1} \bR_{J-1}^{-1} {\br^{(J,1:J-1)}} }{ R^{(J,J)} } \\
    \bzero & 1 / R^{(J,J)} \bq_{J-1}
  \end{bmatrix} \begin{bmatrix}
    \bQU_{J-1}^\top \be_1
    \\
    \mathcal{Q}^{\top{(1,J+1)}}
  \end{bmatrix}
  \nonumber \\
  &=
  \underbracket{\left( \Vert \bb \Vert_2 \bQ_{J-1} \bR_{J-1}^{-1} \bQU_{J-1} \be_1 \right)}_{\bc_{J-1}^{(\text{MINRES})}}
  \:\: + \:\:
  \underbracket{\frac{\Vert \bb \Vert_2 \mathcal{Q}^{\top{(1,J+1)}}}{ R^{(J,J) }}}_{\varphi_J}
  \underbracket{\begin{bmatrix}
    \bQ_{J-1} \bR_{J-1}^{-1} {\br^{(J,1:J-1)}}  \\
    \bq_{J-1}
  \end{bmatrix}}_{\bd_J}.
  \label{eqn:minres_descent}
\end{align}
%
Thus $\bc^{(\text{MINRES})}_J = \bc^{(\text{MINRES})}_{J-1} + \varphi_J \bd_J$.
%The only seemingly expensive part of this update is computing $\bd_J$, as we need to compute $\bQ_{J-1} \bR^{-1}_{J-1} \br^{(J,1:J-1)}$.
We note that $\br^{(J,1:J-1)}$, which is the next entry in the QR factorization of $\widetilde \bT_J$, can be cheaply computed using Givens rotations (see \citep[e.g.][Ch. 11.4.1]{golub2012matrix}).
Moreover, only the last two entries of $\br^{(J,1:J-1)}$ will be non-zero (due to the tridiagonal structure of $\widetilde \bT_J$.
Consequentially, we only need to store the last two vectors of $\bQ_{J-1} \bR^{-1}_{J-1}$, which again can be computed recursively.

The recurrence requires the storage of $\approx 6$ vectors.
As with CG, each iteration requires a single MVM with $\bK$ (to form the next Lanczos vector $\bq_J$); all subsequent operations are $\bigo{N}$.
The entire procedure is given by \cref{alg:minres}.

\input{algorithms/minres}
