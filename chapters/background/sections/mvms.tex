%\section{Computing $\trainK^{-1} \bb$ and $\log \vert \trainK \vert$ with Matrix Multiplication}
\section{Matrix-Vector Multiplication (MVM) Algorithms for Computing Linear Solves and Other Matrix Functions}
\label{sec:mvms}

Gaussian process training and inference requires expensive computations involving the kernel matrix $\trainK = \bK_{\bX\bX} + \sigma_\text{obs} \bI$.
For example, in \cref{eqn:log_lik} we must compute the linear solve $(\trainK^{-1}) \by$ and the log determinant $\log \vert \trainK \vert$.
Since $\trainK$ is positive definite, both of these operations can computed using the Cholesky factorization:
%
\[
  \bL \bL^\top = \trainK, \quad
  (\trainK^{-1}) \by = \bL^{-\top} \bL^{-1} \by, \quad
  \log \vert \trainK \vert = 2 \sum_{i=1}^N \log L^{(ii)}
\]
%
where $\bL$ is the lower-triangular Cholesky factor of $\trainK$.
Though this is a common way to compute these terms, it has drawbacks that were discussed in \cref{chapter:introduction}:
%
\begin{enumerate*}
  \item its $\bigo{N^3}$ asymptotic complexity,
  \item its $\bigo{N^2}$ storage requirement, and
  \item its minimal use of GPU acceleration.
\end{enumerate*}

A primary focus of this thesis is alternatives for GP learning and inference.
We will instead compute the $(\trainK^{-1}) \by$ and $\log \vert \trainK \vert$ terms (and other non-linear terms) through algorithms that only utilize \emph{matrix-vector multiplication} ({\bf MVMs}) and other simple vector operations.
This approach, as we will demonstrate, has several advantages:
%
\begin{enumerate*}
  \item it effectively utilizes GPU acceleration,
  \item it reduces memory requirements, and
  \item it simplifies the implementation of specialty models.
\end{enumerate*}
%
We will rely on a family of iterative algorithms known as {\bf Krylov subspace methods} \citep[e.g.][]{saad2003iterative,van2003iterative}, which were originally developed for large linear algebra problems involving sparse matrices.
These algorithms can compute matrix solves, log determinants, and other operations \emph{without explicitly computing computing the matrix} $\trainK$.
Instead, these methods only access $\trainK$ through MVMs.

The next two chapters propose GP-specific Krylov algorithms for training and inference.
In this remainder of this section, we introduce two common Krylov subspace methods that we use as building blocks: {\bf linear conjugate gradients} (CG) and {\bf Lanczos tridiagonalization}.

\paragraph{Intuition.}
At first glance, it may not be obvious how a matrix-vector multiplications---a linear operation---could compute non-linear operations like $\trainK^{-1} \bv$ or $\log \vert \trainK \vert$.
To motivate the use of MVM-based algorithms, we will begin by reformulating the matrix solve and log determinant operations to make their MVM connection obvious.
First, we will view $\trainK^{-1} \by$ through an optimization lens.
Since $\trainK^{-1}$ is positive definite, we note that any matrix solve is the solution to the following convex problem:
%
\begin{equation}
  \trainK^{-1} \by = \argmin_{\bc \in \reals^{N}} \Bigl[ \frac{1}{2} \: \bc^\top (\trainK) \: \bc - \bc^\top \by \Bigr],
  \label{eqn:solve_as_optim}
\end{equation}
%
where the gradient of the objective function is $(\trainK) \: \bc - \by$.
Thus, one could compute $\trainK^{-1} \by$ with gradient descent, and each gradient step only accesses $\trainK$ through an MVM with the current solution $\bc$.
Rather than using vanilla gradient descent however, we will instead use a decent method (conjugate gradients) that carefully chooses its updates for faster convergence.

The connection between log determinants and MVMs is less straightforward.
At a high-level, we note that the eigenvalues of $\trainK$ can be found through MVM iterations.
For example, the \emph{power method} computes the largest eigenvector by running the iteration $\bc_{j+1} = (\trainK) \bc_{j}$ until convergence.\footnote{
  Let $e_i, \bv_i$ be the $i^\text{th}$ eigenvalue/vector of $\trainK$.
  If we write the initial vector as a linear combination of the eigenvectors $\bc_{0} = \sum_{i=1}^N w_i \bv_i$, then the iterations simply scale the weights $w_i$ by a factor $e_i$:
  $\bc_{j} = \sum_{i=1}^N w_i e_i^j \bv_i$.
  After enough iterations, the largest eigenvalue $e_1^j$ will dominate all others and so $\bv_{j} \approx w_1 e_1^j \bv_1$.
  In other words, $\bc_{j} / \vert \bc_{j} \vert_2 \approx \bv_1$.
  See \citep[e.g.][Ch. 8]{golub2012matrix} for more details.
}
In practice computing a complete eigendecomposition of $\trainK$ through the power iteration would be a highly inefficient way to compute log determinants.
We will instead compute these terms through stochastic Lanczos quadrature \citep{ubaru2017fast} which similarly uses MVMs but avoids a full eigendecomposition.

With these intuitions in place, we will now introduce linear conjugate gradients and Lanczos tridiagonalization.
These methods expand upon the simple iterative procedures we outline above, yet converge to the true solutions in many fewer iterations.



\subsection{Linear Conjugate Gradients}
\label{sec:cg}

Linear conjugate gradients (CG) \cite{hestenes1952methods} is an iterative method for solving the optimization problem posed in \cref{eqn:solve_as_optim} if $\bA$ is positive definite.
At a high level, it is similar to gradient descent: each iteration updates an existing solution by taking a step in a ``search'' direction.
Unlike gradient descent, a good solution to $\bA^{-1} \bb$ is often found in $\ll N$ iterations, where $N$ is the size of the matrix $\bA$.
Moreover, each iteration requires only a single MVM with $\bA$.

Here we offer a high level overview of the algorithm and its convergence properties.
Because CG is notoriously confusing and difficult to intuit, we refer the reader to \citet{shewchuk1994introduction} for a gentler introduction.

\paragraph{Overview.}
Conjugate gradients differs from vanilla gradient descent in two ways: its' choice of the decent direction and its' choice of step size.
Standard gradient descent takes a constant step $\gamma$ in the direction of the gradient $\br_j$ at every iteration:
%
\begin{equation*}
  \bc_{j+1}^{\text(GD)} = \bc_{j} - \gamma \br_{j}, \quad
  \br_{j} = \bA \bc_{j} - \bb.
\end{equation*}
%
($\br_j$ is also referred to as the ``residual'' of the current solution.)
This approach is inefficient because multiple iterations might take steps in the same direction.

On the other hand, the descent directions taken by conjugate gradients $\bd_1, \ldots, \bd_J$ are \emph{mutually conjugate} to one another.
This means that all search directions are orthogonal with respect to the inner product defined by $\bA$---i.e. for all $j \ne j'$, we have $\langle \bd_j, \bd_j' \rangle_\bA = \bd_j^\top \bA \bd_{j'} = 0$.
A consequence of this is that no two iterations move the solution $\bc_{j+1}$ in the same direction.
Moreover, each iteration takes a variable step size $\alpha_j$ which corresponds to the optimal step in the $\bd_j$ direction.
The resulting iterations are:
%
\begin{equation*}
  \bc_{j+1}^{\text(CG)} = \bc_{j} - \alpha_j \bd_{j}.
\end{equation*}
%
where $\bd_j$ and $\alpha_j$ are determined using formulas described below.
The $N$ conjugate descent vectors $\bd_1, \ldots, \bd_N$ form a basis of $\reals^N$, and therefore we can describe the solution as $\bA^{-1} \bb = \bc_0 - \sum_{j=1}^N \alpha_j \bd_j.$
In other words, these iterations find the true solution $\bA^{-1} \bb$ in at most $N$ iterations, though often $\ll N$ iterations finds a very close approximation.
%(Again, the $\alpha_j$ steps are ``optimal'' with respect to $\bA^{-1} \bb$.)

\paragraph{The algorithm.}
By carefully choosing the conjugate descent directions $\bd_1, \ldots, \bd_N$, the CG iterations can be computed through an extremely efficient recurrence relation.
At iteration $j$, the descent direction $\bd_j$ is given by taking the residual $\br_j = \bA \bc_{j} - \bb$ and making it conjugate to all previous descent directions:
%
\begin{equation*}
  \bd_j = \br_j - \sum_{\ell = 1}^{j-1} \frac{ \bd_\ell^\top \bA \br_j }{ \bd_\ell^\top \bA \bd_\ell } \bd_\ell.
\end{equation*}
%
(The above equation is very similar to Gram-Schmidt orthogonalization, except that the resulting vectors are conjugate rather than orthogonal.)
In practice, the sum reduces down to a single term $( \bd_{j-1}^\top \bA \br_j ) / ( \bd_{j-1}^\top \bA \bd_{j-1} ) \bd_{j-1}$ since the residual already happens to be conjugate to all other search directions (see \citep[Sec. 7-8]{shewchuk1994introduction} for a complete derivation).
Moreover, the residual $\br_j$ and the step size $\alpha_j$ can be computed through inner products of existing terms.
As a result, each iteration of CG requires only a single MVM ($\bA \bd_{j-1}$) and a few vector inner products.
The entire algorithm can be written through a recurrence relation involving 3 vectors, which makes CG very fast and memory efficient (see \cref{alg:cg}).

\input algorithms/cg

\paragraph{Convergence.}
This choice of descent directions has an additional advantage: rapid convergence to the true solution.
While $N$ iterations finds the exact arithmetic, often $J \ll N$ solutions find a solution \emph{within numerical precision} of the true solution.
Through some numerical manipulation \citep[see][]{golub2012matrix}, one can show that the solution $\bc_J$ at iteration $J$ is optimal with respect to norm induced by the matrix $\bA$:
%
\begin{align}
  \bc_J
  &= \argmin_{\bc \in \mathcal{K}_J (\bA, \bb)} \frac{1}{2} \bc^\top \bA \bc - \bc^\top \bb
  \nonumber \\
  &= \argmin_{\bc \in \mathcal{K}_J (\bA, \bb)} \Vert \bc - (\bA^{-1} \bb) \Vert_\bA,
  \label{eqn:cg_optimality}
\end{align}
%
where $\mathcal{K}_J (\bA, \bb)$, the domain that $\bc_J$ is optimal over, is referred to as the $J^\text{th}$ {\bf Krylov subpace} of $\bA$ and $\bb$.
%
\begin{equation}
  \mathcal{K}_J (\bA, \bb) = \left\{ \bb, \:\: \bA \bb, \:\: \bA^2 \bb, \:\: \ldots, \:\: \bA^{J-1} \bb \right\}.
  \label{eqn:krylov}
\end{equation}
%
(This is why CG is referred to as a ``Krylov Subspace Method''.)
Note the connection between \cref{eqn:krylov} and MVMs:
the vectors spanning $\mathcal{K}_J (\bA, \bb)$ can be formed by applying an MVM to the previous vector: $\bA^J \bb = \bA \left( \bA^{J-1} \bb \right)$.
The CG search directions $\bd_1, \ldots, \bd_J$ lie in the Krylov subspace, and so the matrix-vector multiplication $\bA \bd_{J-1}$ in \cref{alg:cg} expands $\mathcal{K}_{J}(\bA, \bb)$ to the next subspace $\mathcal{K}_{J+1} (\bA, \bb)$.

\cref{eqn:cg_optimality,eqn:krylov} explain why CG rapidly converges.
Since the Krylov subspace contains powers of $\bA$ applied to $\bb$, we can interpret of \cref{eqn:cg_optimality} as an \emph{optimal polynomial approximation} of $\bA^{-1}$:
%
\begin{align*}
  \bc_J &= \argmin_{\text{poly}_J(\bA)} \Vert \text{poly}_J(\bA)\bb - (\bA^{-1} \bb) \Vert_\bA,
  \\
  \text{poly}_J(\bA) &= w_0 \bb + w_1 \bA \bb + \ldots + w_{J-1} \bA^{J-1} \bb.
\end{align*}
%
($\text{poly}_J(\bA)$ is a $J^\text{th}$-degree polynomial with coefficients $w_0, \ldots, w_{J+1}$.)
From this perspective, one can derive CG's famous (though loose) convergence bound (see \citep{shewchuk1994introduction} for a derivation):
%
\begin{theorem}[Convergence of CG]
  \label{thm:cg_convergence}
  Let $\bA$ be an $N \times N$ positive definite matrix and let $\bb$ be a vector in $\reals^N$.
  After $J$ iterations of CG, the difference between $\bc_J$ and the true solution $\bA^{-1} \bb$ is bounded by:
  \begin{equation*}
    \Vert \bc_J - (\bA^{-1} \bb) \Vert_\bA
    \leq
    2 \Bigg[ \frac{ \sqrt{ \kappa(\bA) } - 1 }{\sqrt{ \kappa(\bA) } + 1} \Biggr]^J \Vert \bb \Vert_\bA,
  \end{equation*}
  %
  where $\kappa(\bA) = \Vert \bA \Vert_2 / \Vert \bA^{-1} \Vert_2$ is the \emph{condition number} of $\bA$.
\end{theorem}
%
Thus CG converges exponentially to the true solve $\bA \bb^{-1}$.
This bound does depends on the condition number of $\bA$, which is often quite large for GP kernel matrices.
In practice however, the exact convergence can be much faster if, for example, the eigenvalues of $\bA$ are clustered \cite{saad2003iterative}.
We find that a few hundred iterations of CG often produces solves within 5 decimal places for most kernel matrices, even when $N \geq 100,\!000$.

\paragraph{Preconditioning.}
A commonly-used approach to accelerate CG convergence is to lower the $\kappa$ condition number in \cref{thm:cg_convergence}.
This can be accomplished through {\bf preconditioning}, which introduces a matrix $\bP$ to solve the related linear system
%
\[
  \bP^{-1} \bA \bc = \bP^{-1} \bb
\]
%
instead of $\bA^{-1} \bb$.
Both systems have the same solution $\bc$, but the preconditioned system's convergence depends on the conditioning of $\bP^{-1} \bA$ rather than that of $\bA$.
The algorithm for \emph{preconditioned conjugate gradients} (PCG) is essentially the same as vanilla CG, with the additional step of applying $\bP^{-1}$ to the MVMs (i.e. $\bP^{-1} \bA \bd_{j-1}$) and residuals (i.e. $\bP^{-1} \br_j$).
See \cref{alg:std_pcg} for details.

\input algorithms/pcg

Choosing a preconditioner $\bP^{-1}$ is a trade-off between computational efficiency and effectiveness.
Trivially, the most effective preconditioner is $\bP^{-1} = \bA^{-1}$ which would reduce the condition number in \cref{thm:cg_convergence} to $\kappa = 1$.
However this is obviously not a practical choice---if we already had a way to efficiently compute $\bA^{-1}$ there would be no need to run CG!
The most effective preconditioners have simple-to-compute inverses (e.g. diagonal matrices, low rank matrices, etc.), yet are able to closely approximate $\bA^{-1}$.
A common (though often ineffective) choice of $\bP$ is the \emph{Jacobi preconditioner}, which is simply the diagonal of $\bA$ (i.e. $\bP = \text{diag}(\bA)$).

The design of effective preconditioners is an active area of research and is too extensive to adequately review here.
We refer the reader to \cite{saad2003iterative} which devotes two chapters to different preconditioning methods.

%\subsection{Overview}

%Given an $N \times N$ matrix $\bA$ and a vector $\bb$, {\bf Krylov subspace methods} are a broad family of \emph{iterative} algorithms for computing $\bF(\bA) \bb$, where $\bF(\cdot)$ is a matrix function.
%For example $\bF (\cdot)$ could be the matrix inverse (i.e. $\bA^{-1} \bb$) or the matrix logarithm (i.e. $\log ( \bA ) \bb$).
%%Importantly, Krylove these problems \emph{without explicitly computing the matrix $\bA$}.
%%Each iteration only requires a \emph{matrix-vector multiplication} (MVM)---i.e. $\bA \bv$ for some vector $\bv$---plus a few additional vector operations.
%Krylov subspace methods are desirable for two primary reasons.
%Firstly, these methods compute $\bF(\bA) \bb$ \emph{without explicitly computing the matrix $\bA$}.
%Instead, the methods simply access the matrix $\bA$ through a {\bf matrix-vector multiplication (MVM)} routine (i.e. $\bA \bv$ for some vector $\bv$),
%which is especially beneficial for sparse or structured matrices that afford efficient MVM routines.
%Secondly, Krylov subspace methods tend to converge rapidly.
%Though exact solutions to $\bF(\bA)\bb$ typically require $N$ iterations (and therefore $N$ MVMs), machine precision is often achieved in $\ll N$ iterations.
%The exact convergence rate depends on the conditioning of $\bA$ or the structure of its eigenvalues.

%In this thesis we will make use of two Krylov algorithms:
%The former can be applied to any symmetric matrix, whereas the latter is only for positive definite matrices.
%We will briefly introduce these two methods in this section and draw a connection between them.


\subsection{Lanczos Tridiagonalization}
\label{sec:lanczos}

The Lanczos algorithm \cite{lanczos1950iteration} iteratively produces a partial tridiagonalization of symmetric matrix $\bA \in \reals^{N \times N}$.
At iteration $J$, $\bA$ is factorized as
%
\begin{equation}
  \bA \bQ_J = \bQ_J \bT_J + \br_J \be^{(J)^\top},
  \label{eqn:lanczos}
\end{equation}
%
where $\bQ_J \in \reals^{N \times J}$ has orthonormal columns, $\bT_J \in \reals^{J \times J}$ is symmetric tridiagonal, and $(\br_J \be^{(J)^\top}) \in \reals^{N \times J}$ is a residual term involving the unit vector $\be^{(J)}$.
After $N$ iterations, the residual term will be zero and we recover the complete tridiagonalization $\bA = \bQ_N \bT_N \bQ^\top_N$.
In practice however, the complete tridiagonalization is almost never computed, and the matrices $\bQ_J, \bT_J$ after $J \ll N$ iterations can approximate functions involving $\bA$ with high degrees of accuracy.
We will discuss how $\bQ_J$ and $\bT_J$ can approximate log determinants and matrix solves after briefly describing the algorithm itself.

\paragraph{The algorithm.}
Given an initial vector $\bb$, $J$ iterations of the Lanczos algorithm form an orthonormal basis of the $J^\text{th}$ Krylov subspace, given by \cref{eqn:krylov}.
%$\mathcal{K}_J (\bA, \bb) = \left\{ \bb, \:\: \bA \bb, \:\: \bA^2 \bb, \:\: \ldots, \:\: \bA^{J-1} \bb \right\}.$
%%
%\begin{equation}
  %\mathcal{K}_J (\bA, \bb) = \left\{ \bb, \:\: \bA \bb, \:\: \bA^2 \bb, \:\: \ldots, \:\: \bA^{J-1} \bb \right\}.
  %\label{eqn:krylov}
%\end{equation}
%%
%$\mathcal{K}_J (\bA, \bb)$ is known as the $J^\text{th}$ \emph{Krylov Subspace} of $\bA$ and $\bb$ (hence why this algorithm is classified as a ``Krylov Subspace Method''---see \citep{saad2003iterative} for more details).
Applying Gram-Schmidt orthogonalization to $\left[ \bb, \:\: \bA \bb, \:\: \ldots, \:\: \bA^{J-1} \bb \right]$ produces the columns of $\bQ_J = \left[ \bb/\Vert \bb \Vert, \:\: \bq^{(2)}, \:\: \ldots, \:\: \bq^{(J)} \right]$, and the orthogonalization coefficients are collected into $\bT_J$.
Because $\bA$ is symmetric, each vector needs only be orthogonalized against the two preceding vectors, which results in the tridiagonal structure of $\bT$ \cite{golub2012matrix}.
In practice, the columns of $\bQ_J$ and the sub-diagonal/diagonal entries of $T_j$ can be computed using a simple iterative procedure (\cref{alg:lanczos}).
%
\input algorithms/lanczos
%
Each iteration produces a new column of $\bQ_J$ and requires only a single MVM with $\bA$.
In total $J$ iterations requires $J$ MVMs.

In practice, \cref{alg:lanczos} can be numerically unstable after many iterations of $J$ (e.g. $J > 50$).
This is because the columns of $\bQ_J$ lose orthogonality due to round-off errors.
We list some solutions to overcome these numerical instabilities in \cref{sec:love_discussion}.


%Before introducing the algorithms, we will fist define Krylov subspaces.
%Formally, given an $n \times n$ matrix $\bA \in \reals^{n \times n}$ and a vector $\bb \in \reals^{n}$, the $J^\text{th}$ \emph{Krylov subspace} is defined as:
%\[
  %\mathcal{K}_{J}(\bA,\bb) = \text{span}\left\{ \bb, \:\: \bA \bb, \:\: \bA^2 \bb, \:\: \ldots, \:\: \bA^{J-1} \bb \right\}.
%\]
%Importantly, note that we can compute the $(J + 1)^\text{th}$ subspace from the $J^\text{th}$ subspace simply by performing an additional matrix-vector multiplication with $\bA$.
%If $\bA$ is full-rank and $\bb$ is not an eigenvector of $\bA$, then the $n^\text{th}$ Krylov subspace will span the entire vector space $\mathcal{K}_{n}(\bA,\bb) = \reals^n$.

%If $\bA$ is a symmetric matrix, the {\bf Lanczos algorithm} provides an efficient way to compute an orthonormal basis of $\mathcal{K}_J$: %%
%\[
  %\bQ_J = \begin{bmatrix} \bq_1, & \bq^{(2)}, & \cdots, & \bq^{(J)} \end{bmatrix}, \quad
  %\bq_1 = \bb / \Vert \bb \Vert^2, \quad
  %\text{span} \left\{ \bQ_J \right\} = \mathcal{K}_{J}(\bA, \bb).
%\]
%%
%The columns of $\bQ_J \in \reals^{N \times J}$ are computed by applying Gram-Schmidt orthogonalization to the MVMs $\{ \bb, \:\: \bA \bb, \:\: \cdots, \:\: \bA^{J-1} \bb \}$.
%Let $\bT_J$ be a $J \times J$ matrix where each column $\bt_i$ contains the orthogonalization coefficients necessary for computing $\bq_{i + 1}$.
%Because $\bA$ is symmetric, each vector $\bq_i$ needs only be orthogonalized against the two preceding vectors ($\bq_{i-1}$ and $\bq_{i-2}$), which results in $\bT_J$ having tridiagonal structure \cite{golub2012matrix}.
%Moreover, this orthogonalization process can be summarized through the following relation of $\bA$, $\bQ_J$, and $\bT_J$:
%%
%\begin{equation}
  %\bA \bQ_J = \bQ_J \bT_J + \br_J \be^{(J)}^\top,
  %\label{eqn:lanczos}
%\end{equation}
%%
%where $\br_J \in \reals^J$ is a residual vector given by $\br_J = (\bA - T_{[J,J]} \bI) \bq^{(J)} - (T_{[J,J-1]} \bq_{J-1})$
%and $\be^{(J)} \in \reals^J$ is the unit vector $[ 0, 0, \ldots, 1 ]$.
%If $J=N$, then $\bQ_N$ and $\bT_N$ form a complete tridiagonalization $\bA = \bQ_N \bT_N \bQ_N^\top$.

%The Lanczos matrices $\bQ_J$ and $\bT_J$ can be used to estimate a wide variety of matrix functions.
%We will first discuss these applications of the Lanczos algorithm before outlining how to compute it in practice.

\paragraph{Using Lanczos to estimate $\bF (\bA) \bb$.}
Lanczos tridiagonalization is general-purpose algorithm, as it can be used to compute an approximation for any $\bF(\bA) \bb$, where $\bF( \cdot )$ is a matrix function (i.e. matrix inverse or matrix logarithm).
First, we note that, if $\bQ_N \bT_N \bQ_N^\top = \bA$ is a complete tridiagonalization, then $\bT_N$ and $\bA$ are similar matrices and therefore:
\begin{equation}
  \bF ( \bA ) = \bQ_N \bigl[ \bF ( \bT_N ) \bigr] \bQ_N^\top.
  \label{eqn:f_tridiagonal}
\end{equation}
%
%This can be a computationally advantageous way to compute $\bF(\bA)$ since we can often exploit the tridiagonal structure of $\bF (\bT_N)$.
%For example, consider $\bF(\cdot) = (\cdot)^{-1}$.
%We can invert tridiagonal matrices in $\bigo{N}$ time rather than $\bigo{N^3}$ time for general matrices.
%
Typically, the Lanczos algorithm is used to only compute a \emph{partial} tridiagonalization $\bQ_J$, $\bT_J$ with $J \ll N$.
However, this partial tridiagonalization can be used to approximate $\bF ( \bA )$ applied to the vector $\bb$:
%
\begin{align}
  \bF ( \bA ) \bb
  &\approx \Bigl( \bQ_J \bigl[ \bF ( \bT_J ) \bigr] \bQ_J^\top \Bigr) \bb.
  \nonumber \\
  &= \Vert \bb \Vert_2 \Bigl( \bQ_J \bigl[ \bF ( \bT_J ) \bigr] \Bigr) \be^{(1)}
  \triangleq \bc_J,
  \label{eqn:f_lanczos}
\end{align}
%
where the second line holds because $\bq_1 = \bb / \Vert \bb \Vert$.
The estimate $\bc_J$ tend to converge exponentially as $J$ increases, though the rate of convergence depends on the conditioning of $\bA$.
Moreover, the computational complexity of \cref{eqn:f_lanczos} is quite small since we can take advantage of the tridiagonal structure of $\bT_J$.

For many functions of interest, there are special variants of \cref{eqn:f_lanczos} that offer more efficient computation.
We now outline two special cases that can be applied to Gaussian process inference.

\paragraph{Estimating log determinants.}

\citet{ubaru2017fast} introduce a method to produce unbiased estimates log determinants using Lanczos tridiagonalization, and \emph{stochastic trace estimation} \cite{hutchinson1990stochastic,avron2011randomized,fitzsimons2016improved}.
%First, given i.i.d random variables $\bz_1, \ldots, \bz_t$ with $\Ev{\bz^{(i)}} = 0$ and $\Ev{\bz^{(i)} \bz^{(i)}}^\top = \bI$ (e.g. $\bz^{(i)} \sim \normaldist{0}{\bI})$, note that the trace operation can be written as $\tr{ \cdot} = \Ev{ \bz^{(i)}^\top \left( \cdot \right) \bz^{(i)}^\top }.$
They note that the log determinant can be re-written as:
%
\begin{align}
  \log \vert \bA \vert &= \tr{ \log \bA }
  \nonumber \\
  &\approx \frac{1}{T} \sum_{i=1}^T \bz^{(i)^\top} \left( \log \bA \right) \bz^{(i)}.
  \label{eqn:logdet_sta}
\end{align}
%
where $\log \bA$ denotes the matrix logarithm, and $\bz^{(i)}$ are i.i.d random variables with zero mean and unit covariance (e.g. $\bz^{(i)} \sim \normaldist{0}{\bI}$).
It can be seen with a little mathematical manipulation that the second line is an unbiased estimate of the trace operator \cite{hutchinson1990stochastic}.

To compute each of the $\bz^{(i)} \left( \log \bA \right) \bz^{(i)}$ terms, we turn to \cref{eqn:f_tridiagonal}.
Let $\bQ_J^{(i)}$, $\bT_J^{(i)}$ correspond to the Lanczos matrices with initial vector $\bq^{(i)}_1 = \bz^{(i)} / \Vert \bz^{(i)} \Vert$.
Then we apply \cref{eqn:f_lanczos}:
%
\begin{align}
  \bz^{(i)^\top} \left( \log \bA \right) \bz^{(i)}
  &\approx \bz^{(i)^\top} \Bigl( \bQ^{(i)}_J \bigl[ \log \bT^{(i)}_J \bigr] \bQ^{(i)\top}_J \Bigr) \bz^{(i)}
  \nonumber
  \\
  &= \Vert \bz^{(i)} \Vert^2_2 \: \Bigl( \be^{(1)^\top} \bigl[ \log \bT^{(i)}_J \bigr] \be^{(1)} \Bigr).
  \label{eqn:slq}
\end{align}
%
Combining \cref{eqn:logdet_sta} and \cref{eqn:slq} gives us an unbiased estimate of $\log \vert \bA \vert$.
This estimate, referred to as the {\bf stochastic Lanczos quadrature} estimate of $\log \vert \bA \vert$, converges exponentially in $J$ (the number of Lanczos iterations):
%
\begin{theorem}[Corrolary 4.5 of \citet{ubaru2017fast}]
  \label{thm:slq_convergence}
  Let $\bA \in \reals^{N \times N}$ be a positive definite matrix with condition number $\kappa( \bA )$.
	Suppose we estimate $\Gamma \approx \log \vert \bA \vert$ using \cref{eqn:logdet_sta,eqn:slq} with:
	\begin{itemize}
		\item $J \geq \frac{1}{4} \sqrt{ 3 \kappa(\bA) } \log \left( \bigo{\kappa(\bA)} / \epsilon \right)$ iterations of Lanczos,\footnote{
			The exact value for the $\bigo{\kappa(\bA)}$ constant is $\left( 5 \kappa(\bA) \log ( 2 (\kappa(\bA) + 1)) / \sqrt{ 2(\kappa(\bA) + 1) } \right)$.
		} and
		\item $T \geq \frac{32}{\epsilon^2} \log \left( \frac 2 \delta \right)$ random $\bz^{(i)} \sim \normaldist{\bzero}{\bI}$ vectors.\footnote{
			The constant originally used by \citet{ubaru2017fast} assumes that the $\bz^{(i)}$ are Rademacher random variables rather than Gaussian random variables.
			The variance of stochastic trace estimation with Gaussian variables is bounded with a factor of $32$ rather than $24$ (see \citep[][Eqs. 4 and 5]{roosta2015improved}), and so we have adjusted the constant accordingly.
		}
	\end{itemize}
  Then the error of the stochastic Lanczos quadrature estimate $\Gamma$ is probabilistically bounded by:
  \begin{equation*}
    \textrm{Pr}\left[\Bigl\vert \log \vert \bA \vert - \Gamma \Bigr\vert \leq \epsilon N \right] \geq \left( 1 - \delta \right).
  \end{equation*}
\end{theorem}

\paragraph{Computing matrix solves.}
Another common application of Lanczos tridiagonalization is computing $\bA^{-1} \bb$.
Let $\bQ_J$ and $\bT_J$ be the Lanczos matrices using $\bb$ as the initial vector (i.e. $\bq_1 = \bb / \vert \bb \vert$).
We can approximate the matrix solve using \cref{eqn:f_lanczos}:
%
\begin{equation}
  \bA^{-1} \bb \approx \Vert \bb \Vert_2 \bQ_J \bT_J^{-1} \be^{(1)}
  \label{eqn:lanczos_solves}
\end{equation}
%
where again $\be^{(1)} \in \reals^J$ is the first unit vector $[1, 0, 0, \ldots, 0]$.




\subsection{Connection between CG and Lanczos}
\label{sec:cg_lanczos_connection}

The Lanczos and CG algorithms are very closely related, as they are both Krylov subspace methods.
Here we show that the two algorithms can essentially be derived from one another.
We will exploit this fact in the next two chapters.

\paragraph{Deriving CG from Lanczos.}
It can be shown that, if $\bA$ is positive definite, then the solution in \cref{eqn:lanczos_solves} is \emph{exactly} the same as the CG solution after $J$ iterations.
In fact, one way to derive \cref{alg:cg} is from Lanczos tridiagonalization and showing that \cref{eqn:lanczos_solves} can be reduced to the three-term CG recurrence.

In practice, the CG algorithm tends to be preferred over \cref{eqn:lanczos_solves} for computing $\bA^{-1} \bb$.
The advantages of CG are
\begin{enumerate}
  \item CG only stores 3 vectors at any given iteration, whereas \cref{eqn:lanczos_solves} requires storing the $\bQ_J \! \in \! \reals^{N \times J}$ matrix;
  \item CG is numerically stable, whereas the Lanczos vectors lose orthogonality (after say $J > 50$ iterations); and
  \item CG is more easily preconditioned.
\end{enumerate}
However, one advantage of Lanczos is that the $\bQ_J$ and $\bT_J$ matrices can be used to jump-start subsequent solves $\bA^{-1} \bb'$.
\citet{parlett1980new}, \citet{saad1987lanczos}, and \citet{schneider2001krylov} argue that subsequent solves can be approximated as
%
\begin{equation}
  \bA^{-1} \bb' \approx \left( \bQ_J \bT_J^{-1} \bQ_J^\top \right) \bb',
  \label{eqn:lanczos_solve}
\end{equation}
%
where $\bQ_J$ and $\bT_J$ come from a previous solve $\bA^{-1} \bb$.
We will use this fact in \cref{chapter:love}.

\paragraph{Deriving Lanczos from CG.}
Additionally, one can recover part of the Lanczos tridiagonalization from conjugate gradients.
\citet{saad2003iterative} and others show that it is possible to recover the $\bT_J$ tridigonal Lanczos matrix by \emph{reusing coefficients} generated in CG iterations.
In particular, we will store the $\alpha_j$ and $\beta_j$ coefficients from \cref{alg:std_pcg}.
%
\begin{observation}[Recovering Lanczos tridiagonal matrices from PCG \cite{saad2003iterative}]
  Assume we use $J$ iterations of standard preconditioned conjugate gradients to solve $\bA^{-1} \bb$ with preconditioner $\bP$.
  Let $\alpha_1, \ldots, \alpha_p$ and $\beta_1, \ldots, \beta_p$ be the scalar coefficients from each iteration (defined in \cref{alg:std_pcg}).
  The matrix
  %
  \begin{equation*}
    \left[\begin{array}{ccccc}
      \frac{1}{\alpha_1} & \frac{\sqrt{\beta_1}}{\alpha_1} &  & & 0 \\
      \frac{\sqrt{\beta_1}}{\alpha_1} & \frac{1}{\alpha_2} + \frac{\beta_1}{\alpha_1} & \frac{\sqrt{\beta_2}}{\alpha_2} &  &  \\
      & \frac{\sqrt{\beta_2}}{\alpha_2} & \frac{1}{\alpha_3} + \frac{\beta_2}{\alpha_2} & \frac{\sqrt{\beta_3}}{\alpha_3} &  \\
      &       & \ddots & \ddots & \frac{\sqrt{\beta_{m-1}}}{\alpha_{m-1}} \\
      0 &       &        & \frac{\sqrt{\beta_{m-1}}}{\alpha_{m-1}} & \frac{1}{\alpha_m} + \frac{\beta_{m-1}}{\alpha_{m-1}}
    \end{array}\right]
  \end{equation*}
  %
  is equal to the Lanczos tridiagonal matrix $\bT_J$, formed by running $J$ iterations of Lanczos to achieve $\left( \bP^{-1} \bA \right) \bQ_J = \bQ_J \bT_J + \br \be^{(J)^\top}$) with probe vector $\bb$.
  \label{obs:lanczos_cg}
\end{observation}
(See \cite{saad2003iterative}, Section 6.7.3.)
In other words, we can recover the Lanczos tridiagonal matrix $\bT_J$ simply by running CG.
However, the orthonormal matrix $\bQ_J$ cannot be as easily derived as a CG byproduct.
