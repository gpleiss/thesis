\section{Gaussian Process Regression}
\label{sec:gps}

%For someone without much prior knowledge (pun intended) about Gaussian processes, the terminology can appear confusing and overloaded.
In the machine learning literature the term \emph{Gaussian process} describes two different objects.
In one context it refers to a distribution over functions that can define a prior over $f(\cdot)$.
It can also refer the class of predictive models that make use of Gaussian process priors on $f(\cdot)$.
As we will soon see, these two classes of objects are effectively the same.
However, for clarity in this section we will explicitly distinguish when ``Gaussian process'' is describing a prior distribution over functions and when it is describing a class of predictive models.

\subsection{Gaussian Process Distributions}
\label{sec:gp_dist}
A {\bf Gaussian process distribution} $f(\cdot) \sim \mathcal{GP}$ extends the multivariate-Gaussian distribution from finite-dimensional vectors to (infinite-dimensional) functions.
It is defined by a {\bf mean function} $\mu(\cdot)$ and a {\bf covariance function} or {\bf kernel function} $k(\cdot, \cdot)$.\footnote{
  Throughout this thesis we will use the two terms interchangeably.
}
%It can only be realized by applying $f$ to some finite set of points.
Given $N$ data points $\bX = [ \bx^{(1)}, \ldots, \bx^{(N)} ] \in \reals^{\numdata \times \numdim}$,
the distribution over $\bfn = [ f(\bx^{(1)}), \ldots, f(\bx^{(N)}) ] \in \reals^\numdata$ is a multivariate Normal distribution:
\begin{equation}
 p \left( \bfn \right) = \normaldist{ \bfn ; \bmu_\bX}{ \bK_{\bX\bX} },
 \label{eqn:prior_finite}
\end{equation}
where $f^{(i)} = f(\bx^{(i)})$, $\mu^{(i)}_\bX = \mu( \bx^{(i)} )$ and $K_{\bX\bX}^{(i,j)} = k(\bx^{(i)}, \bx^{(j)})$.
(We will be using the above short-hand notations $\bfn$, $\bmu_\bX$, and $\bK_{\bX\bX}$ throughout the remainder of this thesis.)
The matrix $\bK_{\bX\bX} \in \reals^{\numdata \times \numdata}$ is often referred to as the {\bf kernel matrix} of $\bX$.

%Analogously to its finite-dimensional counterpart, a Gaussian process is completely defined by its mean and covariance functions $\mu(\cdot)$ and $k(\cdot, \cdot)$.
$\mu(\cdot)$ can be any real-valued function, though it is common simply to choose the zero-function (i.e. $\mu(\cdot) = 0$).
The covariance function $k(\cdot, \cdot)$ must be a valid kernel function, which means that the kernel matrix $\bK$ must be positive definite.
See \cref{sec:common_kernels} for common choices of $k(\cdot, \cdot)$.

\subsection{Gaussian Process Regression Models}
\label{sec:gp_models}

Recall our high-level regression model from \cref{eqn:regression}: $y = f \left( \bx \right) + \epsilon$.
{\bf Gaussian process regression models} are a class of predictive models where
%
\begin{enumerate}
  \item $f ( \cdot )$ is modelled by a Gaussian process prior: $f (\cdot) \sim \GP{\mu(\cdot)}{k(\cdot, \cdot)}$, and
  \item $\epsilon$ is modelled by a Gaussian noise distribution: $\epsilon \sim \normaldist{0}{\sigma_\text{obs}^2}$.
\end{enumerate}
%
Together, these two items define the prior distribution and the likelihood, respectively.
To make predictions on previously-unseen test points $\bxtest, \ytest$ (e.g. predict the future weight of a child), we \emph{condition} the prior model on a set of previously-seen training data $\bX, \by$ (e.g. the weights/features of other children).
In total the predictive model is fully defined by:
\begin{enumerate}
  \item the mean function $\mu(\cdot)$ and covariance function $k(\cdot, \cdot)$ of the GP prior,
  \item the amount of observational noise $\sigma_\text{obs}^2$, and
  \item training data $\dset_\text{train} = (\bX, \by)$.
\end{enumerate}
%
Note that the only learnable parameters of this model are $\sigma_\text{obs}^2$ and whatever parameters are required by the mean/covariance functions.

\paragraph{The predictive distribution.}
In many supervised regression paradigms (e.g. neural networks, ridge regression, etc.), it is common to learn a single latent function $f^*(\cdot)$ that best fits the training data $\dset_\text{train}$.
Under such a setup, the predictive distribution for a test point $(\bxtest, \ytest)$ is given by
$ p(\ytest \vert f^*(\bxtest)) = \normaldist{ \ytest; f^*(\bxtest) }{ \sigma^2_\text{obs} }. $
However, Gaussian process regression models \emph{marginalize out} the choice of $f(\cdot)$.
Under this setup, the predictive distribution is given by:
\begin{equation*}
  p \left( \ytest \mid \bxtest, \dset_\text{train} \right)
  = \int_{f(\cdot)} p\left( y \mid f(\bxtest) \right) \: p\left( f(\bxtest) \mid \bxtest, \dset_\text{train} \right)
  \intd f(\cdot).
  %\label{eqn:marginalize}
\end{equation*}
%
This predictive distribution happens to be computable in closed form.
%\footnote{
  %Note that this is rare; in general, exact Bayesian inference is intractable for most ``interesting'' machine learning models.
%}
Given the Gaussian process prior on $f(\cdot)$ and the Gaussian noise observation model for $p(\ytest \vert f(\bxtest))$, the prediction $p(\ytest \vert \bxtest, \dset_\text{train})$ is a Gaussian distribution:
%
\begin{equation}
  p \left(
    \ytest \mid \bxtest, \dset_\text{train}
  \right)
  = \normaldist{ \ytest ; \meantest{\bxtest}}{ \vartest{\bxtest} }
  \label{eqn:predictive}
\end{equation}
%
where $\meantest{\cdot}$ and $\vartest{\cdot}$ are given by:
%
\begin{align}
  \meantest{\bxtest}
  &= \mu\left( \bxtest \right) + \bk_{\bX \bxtest}^\top \left( \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI \right)^{-1} \left( \by - \bmu \right)
  \label{eqn:predictive_mean}
  \\
  \vartest{\bxtest}
  &= k ( \bxtest, \bxtest) + \sigma^2_\text{obs} - \bk_{\bX \bxtest}^\top \left( \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI \right)^{-1} \bk_{\bX \bxtest}.
  \label{eqn:predictive_var}
\end{align}
%
Here the shorthand $\bk_{\bX \bxtest} \in \reals^N$ is the vector of covariances between test point $\bxtest$ and all training points $\bX = [\bx^{(1)}, \ldots, \bx^{(N)}].$
Note that, under \cref{eqn:predictive_mean,eqn:predictive_var}, the Gaussian process' prediction depends on
\begin{enumerate*}
  \item the labels of the training data
  \item the similarities between $\bx*$ and other training data (as determined by their prior covariances $\bk_{\bX \bxtest}$).
\end{enumerate*}

\paragraph{Short derivation of the predictive distribution.}
To understand where \cref{eqn:predictive_mean} and \cref{eqn:predictive_var} come from, we start by writing the joint prior distribution $p[\bfn, f(\bxtest)]$.
Under the Gaussian process prior of $f(\cdot)$ and \cref{eqn:prior_finite}, this joint distribution will be a multivariate-Gaussian.
%
\begin{align*}
  p \left(
    \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} \middle|
    \begin{bmatrix} \bX \\ \bxtest \end{bmatrix}
  \right)
  &= \normaldist{
    \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} ;
    \begin{bmatrix} \bmu \\ \mu(\bxtest) \end{bmatrix}
  }{
    \begin{bmatrix}
      \bK_{\bX \bX}    & \bk_{\bX \bxtest} \\
      \bk_{\bX \bxtest}^\top   & k(\bxtest, \bxtest)
    \end{bmatrix}
  }.
\end{align*}
%
We then compute the joint marginal likelihood $p( [ \by, \ytest ] \mid [ \bX, \bxtest ] )$ by integrating out the dependence on $p( [\bfn, f(\bxtest)] )$.
Under the Gaussian noise observation model of \cref{eqn:regression}, it happens that the marginal likelihood is also multivariate Gaussian and can be computed in closed form.
%
\begin{align*}
  p \left(
    \begin{bmatrix} \by \\ \ytest \end{bmatrix} \: \middle| \:
    \begin{bmatrix} \bX \\ \bxtest \end{bmatrix}
  \right)
  &= \int
    p \left(
      \begin{bmatrix} \by \\ \ytest \end{bmatrix} \: \middle| \:
      \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix}
    \right)
    p \left(
      \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} \: \middle| \:
      \begin{bmatrix} \bX \\ \bxtest \end{bmatrix}
    \right)
    \intd { \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} }
  \\
  \vspace{0.5em}
  &= \normaldist{
    \begin{bmatrix} \by \\ \ytest \end{bmatrix} ;
    \begin{bmatrix} \bmu \\ \mu(\bxtest) \end{bmatrix}
  }{
    \begin{bmatrix}
      \bK_{\bX \bX} + \sigma_\text{obs}^2 \mathbf I   & \bk_{\bX \bxtest} \\
      \bk_{\bX \bxtest}^\top         & k(\bxtest, \bxtest) + \sigma_\text{obs}^2
    \end{bmatrix}
  }.
\end{align*}
where the $\sigma_\text{obs}^2$ terms come from the observational noise (which we assume is independent for all data points).
%
Therefore, the predictive distribution $p(\ytest \: \vert \: \bxtest, \dset_\text{train})$ is simply the conditional of a multivariate Gaussian.
Applying standard Gaussian conditioning rules \citep[see e.g.][]{bishop2006pattern,rasmussen2006gaussian} results in \cref{eqn:predictive_mean} and \cref{eqn:predictive_var}.

\paragraph{Predictions on multiple test points.}
If there are multiple test points $\bxtest$, $\bxtestprime$ of interest, the predictions $p([\ytest, \ytestprime] \mid [\bxtest, \bxtestprime], \dset_\text{train})$ are jointly Gaussian distributed.
The mean is given by \cref{eqn:predictive_mean} and the covariance between the two points is given as
\[ \covtest{\bxtest}{\bxtestprime} = k(\bxtest, \bxtestprime) - \bk_{\bX\bxtest}^\top ( \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI )^{-1} \bk_{\bX\bxtestprime}. \]
In fact, any (finite) set of test points will be jointly Gaussian with the above covariance.
Recalling our definition from \cref{sec:gp_dist}, this implies that the predictive distribution is a Gaussian process distribution.

\subsection{Training Gaussian Process Models}
\label{sec:gp_training}

Gaussian process regression models are non-parametric, which leaves us few terms that need to be learned.
However, we wish to pick the mean function, kernel function, and observational noise that best fit our training data $\dset_\text{train} = (\bX, \by)$.
Let $\btheta$ be the set of learnable {\bf hyperparameters} of these functions.
For example, $\btheta$ might include the lengthscale $\ell$ and outputscale $o^2$ of the kernel function (see \cref{sec:common_kernels}) as well as the observational noise parameter $\sigma_\text{obs}$ of the likelihood.
In more complex Gaussian process models, $\btheta$ may also include inducing point locations \cite{titsias2009variational} or neural network parameters for deep kernel learning \cite{wilson2016deep}.
We can measure the fit of these parameters through the {\bf marginal log likelihood} of the Gaussian process applied to the training data $\bX, \by$:
%
\begin{align}
  -\log p( \by \mid \bX, \btheta)
  &= -\log \normaldist{ \by ; \bmu }{ \trainK }
  \nonumber{} \\
  &\propto \log \left \vert \trainK \right \vert + (\by - \bmu)^{\top} \trainK^{-1} (\by - \bmu),
  \label{eqn:log_lik}
\end{align}
%
where $\trainK$ is shorthand for the training kernel matrix with added observational noise: $\trainK = \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI$.
Note that the dependence on $\btheta$ is implicitly absorbed into $\bmu$ and $\trainK$.
We can use \cref{eqn:log_lik} to learn the parameters $\btheta$ via gradient-based optimization or sampling.
Its derivative is given by
%
\begin{align}
  \frac{\partial - \log p( \by \mid \bX, \btheta )}{\partial \btheta} &\propto
   \tr{\trainK^{-1}\frac{\partial \trainK}{\partial \btheta}} -
	 (\by - \bmu)^{\top} \trainK^{-1}\frac{\partial \trainK}{\partial \btheta}\trainK^{-1} (\by - \bmu).
  \label{eqn:log_lik_deriv}
\end{align}
%
We will discuss how to efficiently compute these terms in \cref{chapter:bbmm}.






\subsection{Common Covariance Functions}
\label{sec:common_kernels}

Of the three objects that define a Gaussian process regression model---the prior mean function, the prior covariance function, and the observed noise parameter---the covariance function arguably has the largest impact on the predictive distribution.
While the parameters of any particular kernel function can most often be learned through gradient descent, the practitioner must first choose the class of covariance functions to optimize over.
Choosing the right covariance function class will largely determine how the Gaussian process regression model generalizes to unseen data points.
This is especially crucial in the small-data regime: if the covariance function properly encodes prior information, then the model can generalize well even when there are few training points.

In the running example of predicting a child's weight, there are several known functional properties that we can encode a priori.
For example, a child's weight will (in many cases) be roughly monotonic with respect to time.
Additionally, children who grow up in similar household settings will be more likely to have similar weights.
We will discuss how both of these properties can be encoded by choosing an appropriate kernel function.

\paragraph{Common kernel functions.}
Here we list some common and simple kernel functions that will be used throughout this thesis.
We will describe their functional form and their properties:
%
\begin{itemize}
  \item The {\bf Radial Basis Function (RBF)} kernel, or squared exponential kernel is one of the most commonly-used covariance functions.
    It's functional form is given by
    %
    \[ k_\text{RBF}(\bx, \bx') = o^2 \exp \left( \sum_{d=1}^D \frac{\left( x^{(d)} - x^{(d)\prime} \right)^2}{\ell_d^2}  \right) \]
    %
    where $o^2$ is referred to as the {\bf outputscale} parameter and the $\ell_1, \ldots, \ell_D$ are the {\bf lengthscales} for each dimension $d \in [1, D]$.
    When the $\ell_d$ parameters differ across all dimensions, the kernel is referred to as the {\bf RBF-ARD } kernel, where ARD stands for {\bf automatic relevance determination }.

    The RBF kernel is commonly considered to be a ``general'' kernel, since it is able to universally approximate any function given enough training data \cite{micchelli2006universal}.
		However, functions drawn from $f \sim \GP{\mu}{k_\text{RBF}}$ are infinitely differentiable, which may impose an unreasonable smoothness constraint in some applications \cite{stein2012interpolation}.

	\item The {\bf Mat\'{e}rn} kernel is another popular kernel which is arguably as general as the RBF kernel yet without the strict smoothness properties.
		The kernel introduces a parameter $\nu$ that determines how differentiable its sampled functions are.
		More concretely, the covariance between any two points is given as:
    %
    \[
      k_{\text{Mat}}^{(\nu)} (\bx, \bx') = o^2 \frac{2^{1 - \nu}}{\gamma(\nu)}
			\left( \sqrt{2 \nu} \sum_{d=1}^D \frac{\vert x^{(d)} - x^{(d)\prime} \vert}{\ell_d}  \right)^\nu
			K_\nu \left( \sqrt{2 \nu} \sum_{d=1}^D \frac{\vert x^{(d)} - x^{(d)\prime} \vert}{\ell_d}  \right),
		\]
		%
    where $o^2$ and $\ell_d$ are analogous to the parameters from the RBF kernel, and where $K_\nu$ is a modified Bessel function \cite{rasmussen2006gaussian}.
    Functions sampled from $f \sim \GP{\mu}{k_{\text{mat}}^{(\nu)}}$ are $\lceil \nu \rceil - 1$-times differentiable.
    As $\nu \rightarrow \infty$ we recover the RBF kernel.

    Typically, $o^2$ and $\ell_d$ are learned through gradient descent whereas $\nu$ is pre-selected and fixed.
    Common values for $\nu$ are $3/2$ and $5/2$, both of which have more succinct closed-form expressions.
    These kernels are often referred to as {\bf Mat\'ern-3/2} and {\bf Mat\'ern-5/2}, respectively.

  \item The {\bf Linear} kernel is one of the least expressive kernels, as it only has support for linear functions.
    However, it is commonly used as a building block for more complex kernels, as we will discuss shortly.
    It is given by:
    %
    \[ k_\text{Lin} ( \bx, \bx' ) = b^2 + o^2 (\bx - \bc)^\top (\bx - \bc), \]
    %
    where $b$, $\bc$, and $o$ are learnable parameters.
\end{itemize}
%
\noindent
There are many other kernels that can encode other functional structures such as periodicity (using the periodic kernel), multiple lengthscales (using the rational quadratic kernel), or erratic/noisy dynamics (using Ornstein-Uhlenbeck kernel).
See \cite{rasmussen2006gaussian} for more discussion on basic kernel types.

\paragraph{Composing kernels.}
Kernels can be composed together to combine functional properties.
The two most common methods for composition are addition: $k(\bx, \bx') = k_1(\bx, \bx') + k_2(\bx, \bx')$
and multiplication: $k(\bx, \bx') = k_1(\bx, \bx') k_2(\bx, \bx')$.
The sub-kernels $k_1$ and $k_2$ can operate on any subset of the dimensions of $\bx$.
In our running example, we may wish to model our data with the kernel
%
\[ k(\bx, \bx') = k_\text{Lin}( \bx_\text{time}, \bx_\text{time}' ) k_\text{RBF}( \bx_\text{time}, \bx_\text{time}' ) + k_\text{RBF}( \bx_\text{household}, \bx_\text{household}' ) \]
%
where $\bx_\text{time}$ and $\bx_\text{household}$ are the time and household features of $\bx$ respectively.
This would encode our first prior assumption (a child's weight increases near-monotonically with time, but is not strictly linear)
and our second prior assumption (children from a similar household environments should have similar development).


\paragraph{Stationary kernels.}
The RBF and Mat\'ern kernels belong to an important class of kernels known as {\bf stationary kernels}.
This class more generally includes all kernels that can be written as function of the difference between two points:
\begin{equation}
  k_\text{stationary} (\bx, \bx') = g( \vert \bx - \bx' \vert ),
  \label{eqn:stationary}
\end{equation}
for some function $g$.\footnote{
  Note that the linear kernel cannot be written in this way and is therefore non-stationary.
}
This class of kernels has many important properties; see \citep[e.g.][]{rasmussen2006gaussian,wilson2013gaussian} for more details.
For the purpose of this thesis, we draw attention to stationary kernels as they allow for potential computational savings.
For example, the computational complexity of GPs with stationary kernels is reduced when the data lie on a regularly-spaced grid \cite{cunningham2008fast,saatcci2012scalable} or can be approximated by grid points \cite{wilson2014thesis,wilson2015kernel}.


\paragraph{Spectral and Deep kernels.}
%For datasets with well-understood properties, it can be advantageous to hand-choose or construct kernels that best encode prior knowledge.
With complex datasets, it may be too difficult to manually encode a specific functional structure.
In such settings, it is better to automatically learn functional structure directly from the data using a highly-parametric kernel, such as the {\bf spectral-mixture} kernel \cite{wilson2013gaussian} or a {\bf deep kernel} \cite{wilson2016deep}.
At a high level, these kernels learn the $g$ function in \cref{eqn:stationary} using a mixture model or a neural network.
%Though such learning often requires larger datasets, it can result in kernel functions that more accurately model the data.

The spectral-mixture kernel (in one dimension) is defined by a mixture of $Q$ functions:
%
\[
  k_\text{SM} (x, x') = \sum_{q=1}^Q w_q
    \exp \left( -2 \pi^2 (x - x')^2 v_q \right)
    \cos \left( -2 \pi (x - x')^2 \mu_q \right),
\]
%
where $w_q$, $v_q$, and $\mu_q$ are learnable parameters.
It is well-suited for interpolation and extrapolation tasks of time series or spatial data \cite{wilson2013gaussian}.
Deep kernels \cite{wilson2016deep,wilson2016stochastic,calandra2016manifold} are kernels where the inputs $\bx$, $\bx'$ are first transformed using a deep neural network $\Phi$:
%
\[
  k_\text{deep} (x, x') = k_\text{base} \left( \Phi(\bx), \Phi(\bx') \right),
\]
where $k_\text{base}(\cdot, \cdot)$ is a simpler base kernel (usually the RBF kernel).
For both cases, the parameters of the mixture model/neural network are learned by optimizing the marginal log likelihood in \cref{eqn:log_lik}.
%It is well suited to high-dimensional datasets or datasets with complex features.







\subsection{Scalable Gaussian Processes}
\label{sec:approx_gps}

As we will discuss in detail in the next chapters, computing the marginal log likelihood (\cref{eqn:log_lik}) and the predictive distribution (\cref{eqn:predictive_mean,eqn:predictive_var}) has historically been computationally prohibitive for large datasets.
This motivates the development of several approximate Gaussian process models \citep[e.g.][]{quinonero2005unifying,snelson2006sparse,rahimi2008random,titsias2009variational,hensman2013gaussian,wilson2015kernel,izmailov2018scalable,gardner2018product,evans2018scalable} that trade off exactness for computational scalability.
Many of these methods approximate the training kernel matrix $\bK_{\bX\bX}$ with a structured or low-rank matrix $\approxK$ that affords faster matrix operations and requires less storage.
Here we highlight some notable variants, though it is by no means an exhaustive list---see \cite{liu2018gaussian} for a more thorough review.


\paragraph{Inducing Points.}
A common mechanism for scalable models is to introduce a set of pseudo-inputs, or {\bf inducing points}.
The kernel function for (non-inducing) inputs $\bx$, $\bx'$ is approximated by interpolating against the kernel matrix for the pseudo-inputs $\bZ = [\bz^{(1)}, \ldots, \bz^{(M)}]$.
%
\begin{align}
  k(\bx, \bx') \approx \widetilde k(\bx, \bx') &= \:\:
		\overbracket{ \left( \bk_{\bZ\bx}^\top \bK_{\bZ\bZ}^{-1} \right) }^{\text{interpolation}} \:\: \bK_{\bZ\bZ} \:\:
		\overbracket{ \left( \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx'} 		 \right) }^{\text{interpolation}}
	\nonumber{} \\
	&= \:\:
	\bk_{\bZ\bx}^\top \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx'}.
	\label{eqn:inducing_interpolation}
\end{align}
%
In its simplest form, it is common to choose $M \ll N$ inducing points.
The approximate training kernel matrix $\widetilde \bK_{\bX\bX} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX}$ will then have low-rank structure, allowing for efficient computation of \cref{eqn:predictive_mean,eqn:predictive_var,eqn:log_lik,eqn:log_lik_deriv}.
The locations of the inducing points $\bZ$ are additional parameters to optimize via \cref{eqn:log_lik} (typically with gradient descent).

Here we have presented inducing points as a mechanism for approximating kernel matrices.
Under this view, we can use the same training/prediction equations as presented in \cref{sec:gp_models,sec:gp_training}---replacing the kernel matrices with their corresponding approximations.
%As we will discuss in \cref{chapter:bbmm}, this allows us to create a simple unifying framework to Gaussian process training and inference.
We do note it is also possible to motivate/derive inducing point methods in other ways---e.g. through a greedy approximation \cite{smola2001sparse} or a variational bound \cite{titsias2009variational,hensman2013gaussian}.
However, under these interpretations it is less straightforward to derive connections to \cref{eqn:predictive_mean,eqn:predictive_var,eqn:log_lik,eqn:log_lik_deriv}.

\paragraph{Variants.}
The basic formula of \cref{eqn:inducing_interpolation} can be extended to create a variety of approximate Gaussian process regression methods.
We briefly present three common variants here---all of which use $M \ll N$ inducing points and learn their locations through gradient descent of \cref{eqn:log_lik}.
In the next section we will discuss Kernel Interpolation for Scalable Structured GPs (KISS-GP) \cite{wilson2015kernel}---a variant which utilizes inducing points in a unique way.

\begin{itemize}
	\item {\bf Subset of Regressors (SoR)} \cite{silverman1985some,smola2001sparse} is arguably the most standard variant.
    Here, the kernel matrix $\bK_{\bX\bX}$ and all kernel vectors $\bk_{\bX\bxtest}$ are replaced with their approximate variants:
    \[ \widetilde \bK_{\bX\bX} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX}, \quad
       \widetilde \bk_{\bX\bxtest} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bk_{\bX\bxtest}. \]

	\item {\bf Fully Independent Training Conditional (FITC) } \cite{snelson2006sparse} can be interpreted as an extension of SoR
		where the approximate training kernel matrix has a diagonal correction term $\bLambda$.
    \[ \widetilde \bK_{\bX\bX} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX} + \bLambda, \quad
       \Lambda^{(i,i)} = k(\bx^{(i)}, \bx^{(i)}) - \bk_{\bZ\bx^{(i)}}^\top \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx^{(i)}}. \]
    The diagonal correction term is motivated through a connection to approximate Bayesian inference.

	\item {\bf Sparse Gaussian Process Regression (SGPR) } \cite{titsias2009variational} uses the same kernel approximation as SoR.
		However, when learning the inducing point locations and other parameters, SGPR augments the marginal log likelihood of \cref{eqn:log_lik} with a diagonal correction term:
    \[ \loglik(\by \mid \bX, \btheta) = -\log p(\by \mid \bX, \btheta) + \sum_{i=1}^N \left( k(\bx^{(i)}, \bx^{(i)}) - \bk_{\bZ\bx^{(i)}}^\top \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx^{(i)}} \right). \]
    The diagonal correction term can be derived through a connection to variational methods.
\end{itemize}
%
We will discuss how to exploit this low-rank kernel matrix structure for faster inference in the next chapter.


\subsection{Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)}
\label{sec:kissgp}

Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP) \cite{wilson2015kernel} utilizes inducing points in a fundamentally different way than SoR, FITC, or SGPR.
While the above methods tend to use $M \ll N$ inducing points, \citet{wilson2015kernel} demonstrate that it is possible to set $M \gg N$ by making the following restrictions:
\begin{enumerate}
  \item the interpolation terms in \cref{eqn:inducing_interpolation} are approximated by \emph{sparse interpolation vectors} $\bw_\bx \approx \bK^{-1}_{\bZ\bZ} \bk_{\bZ\bx};$
  \item the inducing points $\bZ = [\bz^{(1)}, \ldots, \bz^{(M)}]$ must lie on a \emph{regularly-spaced grid}; and
  \item the kernel function is \emph{stationary} (e.g. RBF, Mat\'ern, etc.).
\end{enumerate}
%
\noindent
The resulting approximate kernel matrix has nice algebraic structure that can be exploited for efficient storage and fast computations.
In particular, the approximate kernel matrix $\widetilde \bK_{\bX\bX}$ affords fast \emph{matrix-vector multiplications} (MVMs), which will translate to fast Gaussian process training and inference (using the methods described in \cref{chapter:bbmm,chapter:love}).

In more detail, KISS-GP assumes that a data point $\bx$ is well-approximated as a \emph{local interpolation} of $\bZ$.
Using cubic interpolation \cite{keys1981cubic}, $\bx$ is expressed in terms of its 4 closest inducing points, and the interpolation weights are captured in the sparse vector $\bw_\bx$.
The $\bw_\bx$ vectors approximate the training kernel matrix $\bK_{\bX\bX} \approx \tilde \bK_{\bX\bX}$ via \cref{eqn:inducing_interpolation}:
%
\begin{align}
  \widetilde \bK_{\bX\bX} &= \:\:
    \left( \overbracket{ \bK_{\bZ\bX}^\top \bK_{\bZ\bZ}^{-1} }^{ \approx \bW_{\bX}^\top} \right)
    \bK_{\bZ\bZ}
    \left( \overbracket{ \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX} }^{ \approx \bW_{\bX}} \right)
  \nonumber{} \\
  &\approx \:\: \bW_{\bX}^\top \: \bK_{\bZ\bZ} \: \bW_{\bX}.
  \label{eqn:ski}
\end{align}
%
Here, $\bW_{\bX} = [\bw_{\bx^{(1)}}, \ldots, \bw_{\bx^{(N)}}]$ contains the interpolation vectors for all $\bx^{(i)}$.
Performing a matrix-vector multiplication with $\widetilde \bK_{\bX\bX}$ (i.e. $( \bW_\bX^\top \bK_{\bZ\bZ} \bW_{\bX}) \bb$ for any vector $\bb$) takes \emph{near-linear} time.
To see why, a MVM with the matrix $\bW_\bX$ requires $\bigo{N}$ time due to the $\bigo{N}$ sparsity of $\bW_\bX$.
Moreover, restrictions 2 and 3 (stationary kernel/grided inducing points) endow the inducing kernel matrix $\bK_{\bZ\bZ}$ with Toeplitz structure, which can be exploited for $\bigo{M}$ storage and $\bigo{M \log M}$ MVMs \citep[see][for details]{wilson2015thoughts}.
Thus, if we perform $( \bW_\bX^\top \bK_{\bZ\bZ} \bW_{\bX}) \bb$ from right-to-left, the entire multiplication takes $\bigo{N + M \log M}$ time.
As we will demonstrate in the next section, exploiting these fasts MVMs results in a near-linear training time complexity for KISS-GP.

%
\paragraph{Computing predictive means.}
One advantage of KISS-GP's structure is the ability to perform constant time predictive mean calculations \cite{wilson2015thoughts}.
Substituting the KISS-GP approximate kernel into \cref{eqn:predictive_mean} and assuming a prior mean of 0 for notational brevity, the predictive mean is given by
\begin{equation}
  \meantest{\bxtest} = \bw_{\bxtest}^\top {\blue{\underbracket{\bK_{\bZ\bZ} \bW_{\bX}(\bW_{\bX}^{\top}\bK_{\bZ\bZ}\bW_{\bX} + \sigma_\text{obs}^{2} \bI)^{-1} \by}_{\ba'}}}.
  \label{eqn:pred_mean_ski}
\end{equation}
(Here {\color{blue} blue} highlights computations that don't depend on test data.)
Because $\bw_{\bx^{*}}$ is the only term in \cref{eqn:pred_mean_ski} that depends on $\bx^{*}$, the remainder of the equation (denoted as $\blue \ba'$) can be pre-computed: $\meantest{\bxtest} = \bw_{\bxtest}^\top \blue \ba'$.
After pre-computing $\blue \ba'$, the multiplication $\bw_{\bxtest}^\top \blue \ba'$ requires $\bigo{1}$ time, as $\bw_{\bxtest}$ is sparse and has only four nonzero elements.



\subsection{Stochastic Variational Inference for Non-Conjugate/Large-Scale GPs}
\label{sec:variational}
For most of this thesis we will focus on Gaussian process regression with a (conjugate) Gaussian observation model---$p(y \mid f(x)) = \normaldist{y; f(x)}{\sigma_\text{obs}^2}$.
Under this setting, exact Bayesian inference is tractable, which results in the closed-form predictive distribution given by \cref{eqn:predictive_mean,eqn:predictive_var}.
Here we will briefly examine the case where exact inference is intractable, which occurs when
%
\begin{enumerate}
  \item using a non-Gaussian observation model (e.g. a Bernioulli likelihood for GP classification, a Student-T likelihood for heavy-tailed regression); or
  \item using a dataset that is too large (or cumbersome) to fit into memory.
\end{enumerate}
%
These settings necessitate \emph{approximate Bayesian inference methods} to estimate the predictive distribution.
Here, we would note a subtle but important distinction between ``approximate inference'' and the ``scalable approximations'' presented in \cref{sec:approx_gps}.
Scalable GP approximations like KISS-GP perform exact GP inference but approximate the kernel matrix operations.\footnote{
  We note that the SGPR method \cite{titsias2009variational} is in some sense a ``hybrid'' method.
  It is motivated and derived via variational inference, and therefore can be considered an approximate inference method.
  However, as demonstrated in \cref{sec:approx_gps}, it can also be viewed through the lens of exact GP inference with a SoR approximated kernel.
  In this thesis we consider it under the lens of ``scalable exact GPs.''
  We reserve the ``approximate inference'' label for models that can handle non-conjugate observation models.
}
Approximate inference GPs on the other hand approximate the intractable predictive distribution with a simple (often Gaussian) distribution.

There are several approaches to approximate non-conjugate GP predictive distributions \citep{minka2001family,rasmussen2006gaussian,hensman2015mcmc,li2015stochastic}.
Here we introduce a family of methods that rely on stochastic variational approximations.

\paragraph{Stochastic variational Gaussian processes (SVGP)}
\cite{hensman2013gaussian,hensman2015scalable,matthews2016sparse} are a popular class of models that approximate GP posteriors via stochastic variational inference.
This method has several compelling properties:
\begin{enumerate*}
  \item it uses $M \ll N$ inducing points to easily trade off speed for predictive fidelity; and
  \item it can be used in conjunction with stochastic gradient optimization for memory savings.
\end{enumerate*}

Similarly to SoR/FITC/SGPR, these models introduce a set of $M \ll N$ inducing points $\bZ = [\bz_1, \:\: \ldots, \:\: \bz_M]$.
Let the random variable $\bu = [f(\bz_1, \:\: \ldots, \:\: f(\bz_M)]$ represent the function evaluated at each inducing point.
SGPR approximates the joint posterior distribution $p( f(\cdot), \bu, \mid \bX, \by)$ with a Gaussian process \emph{variational distribution} $q(f(\cdot), \bu)$:
%
\begin{align*}
  p(f(\cdot), \bu \mid \bX, \by) &\approx q(f(\cdot), \bu)
  \\
  &= p(f(\cdot) \mid \bu) q(\bu),
  \quad
  q(\bu) = \normaldist{\bm}{\bS},
\end{align*}
%
where $\bm \in \reals^M$ and $\bS \in \reals^{M \times M}$ are learnable \emph{variational parameters}.
Under this approximation, note that the conditional variational distribution $q(f(\cdot) \mid \bu)$ matches the prior conditional distribution.
Marginalizing out $\bu$ gives us the approximate Gaussian predictive distribution $q( f(\bx) )$:
%
\begin{align}
  q \left( f(\bx) \right) &= \Evover{q(\bu)}{ q( f(\bx), \bu) }
  \triangleq \Evover{q(\bu)}{ \: p\left( f(\bx) \mid \bu \right) \: }
  \nonumber \\
  &= \normaldist{ \bxtest; \ameantest{ \bx }} { \avartest{ \bx }}
  %\label{eqn:approx_pred}
  \nonumber
  \\
  \ameantest{\bx} &= \bk_{\bZ\bx}^\top \bK_{\bZ\bZ}^{-1} \bm
  \label{eqn:approx_pred_mean} \\
  \avartest{\bx} &= k(\bx, \bx) -
    \bk_{\bZ\bx}^\top \bK_{\bZ\bZ}^{-1} \left( \bK_{\bZ\bZ} - \bS \right) \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx}
  \label{eqn:approx_pred_covar}
\end{align}
%
where $\ameantest{\cdot}$ and $\avartest{\cdot}$ are the (approximate) posterior mean and covariance functions.
The parameters $\bm$ and $\bS$ (as well as the inducing locations $\bZ$ and kernel/likelihood hyperparameters) are optimized to minimize the KL-divergence between the approximate posterior and the true posterior $p(f(\cdot), \bu \mid \bX, \by)$.
This objective function is captured by the evidence-lower bound (ELBO):
%
\begin{align}
	-\loglik_\text{ELBO} &= -\sum_{i=1}^N \Evover{q(f(\bx^{(i)}))}{  \: \log p( y^{(i)} \mid f(\bx^{(i)}) ) \: } + \kl{ q(\bu) }{ p(\bu) }.
	\label{eqn:elbo}
\end{align}
%
Here, $p(y^{(i)} \mid f(\bx^{(i)})$ is the (potentially) non-conjugate likelihood function.
The expectation $\Evover{q(f(\bx^{(i)}))}{  \: \log p( y^{(i)} \mid f(\bx^{(i)}) ) \: }$ can be approximated with Monte Carlo integration \citep{wilson2016stochastic} or with Gauss-Hermite quadrature \cite{hensman2015scalable}.
The KL divergence term is given by
%
\[
	\kl{ q(\bu) }{ p(\bu) } = \frac{1}{2} \Bigl( \bm^\top \bK_{\bZ\bZ}^{-1} \bm + \tr{ \bK_{\bZ\bZ}^{-1} \bS } - \log \vert \bK_{\bZ\bZ}^{-1} \bS \vert - M \Bigr).
\]

\cref{eqn:elbo} is typically optimized using gradient descent or natural gradient descent methods \cite{hensman2012fast,salimbeni2018natural}.
Note that, because the ELBO factorizes as a sum over data points, we can approximate the sum over a minibatch of data.
This ability to use stochastic optimization is an advantage of SVGP, as it does not require large datasets to be stored in memory.

To form predictions at test time, we simply use \cref{eqn:approx_pred_mean} and \cref{eqn:approx_pred_covar} to compute $q(f(\bxtest))$ for a test point $\bxtest$.
The predictive distribution $q(\by \mid \bxtest) = \Evover{q(f(\bxtest))}{ p(\ytest \mid f(\bxtest)) }$ can again be approximated via sampling or quadrature.
Closed-form predictive distributions $q(\ytest \mid \bxtest)$ exist for Gaussian and Bernoulli observation models \citep{rasmussen2006gaussian}.

\subsection{Summary of Notation}
The notation of this section is summarized in \cref{tab:gp_notation}.
It will be used throughout the remaining chapters.

\begin{table}[!b]
  \centerfloat
  \caption{Summary of Gaussian process notation.}
  \label{tab:gp_notation}
	\resizebox{1.\textwidth}{!}{%
		\begin{tabular}{ ccc }
			\toprule
			{\bf Notation} & {\bf Domain} & {\bf Description} \\
			\midrule
			\midrule
			$\numdata$ & $\ints$ & Number of training data points \\
			$\numdim$ & $\ints$ & Dimensionality of inputs \\
			$\numinduc$ & $\ints$ & Number of inducing points \\
			\midrule
			$\bX$ & $\reals^{\numdata \times \numdim}$ & Training features \\
			$\by$ & $\reals^{\numdata}$ & Training targets \\
			$\bxtest$ & $\reals^{\numdim}$ & Features of a test data point \\
			$\ytest$ & $\reals$ & Target of a test data point \\
			\midrule
			$\mu( \cdot )$ & $\reals^\numdim \rightarrow \reals$ & The GP (prior) mean function \\
			$k( \cdot, \cdot )$ & $\reals^\numdim \times \reals^\numdim \rightarrow \reals$ & The GP (prior) covariance/kernel function \\
			$\sigma_\text{obs}^2$ & $\reals$ & Observational variance of the Gaussian likelihood \\
			$f( \cdot )$ & $\reals^\numdim \rightarrow \reals$ & Latent function (modelled with a GP prior) \\
			$\btheta$ &  & Parameters of kernel, likelihood, etc. \\
			\midrule
			$\bmu_\bX$ & $\reals^\numdata$ & (Prior) mean vector for training data $\bX$ \\
			$\bK_{\bX\bX}$ & $\reals^{\numdata \times \numdata}$ & (Prior) kernel matrix for training data $\bX$ \\
			$\trainK$ & $\reals^{\numdata \times \numdata}$ & (Prior) kernel matrix $\bK_{\bX\bX}$ plus observational noise $\sigma^2_\text{obs} \bI$ \\
			$\bk_{\bX\bxtest}$ & $\reals^{\numdata}$ & Prior covariance between training data $\bX$ and point $\bxtest$\\
			$\bfn$ & $\reals^\numdata$ & Latent function evaluated on training data $\bX$ \\
			\midrule
			$\bZ$ & $\reals^{\numinduc \times \numdim}$ & Locations of inducing points \\
			$\bK_{\bZ\bZ}$ & $\reals^{\numinduc \times \numinduc}$ & (Prior) kernel matrix for inducing points $\bZ$ \\
			$\bk_{\bZ\bx}$ & $\reals^{\numinduc}$ & Prior covariance between inducing points $\bZ$ and point $\bx$ \\
			$\bW_{\bX}$ & $\reals^{\numdata \times \numinduc}$ & Sparse inducing-interpolation matrix for training data $\bX$ (using \cref{eqn:ski}) \\
			$\bw_{\bx}$ & $\reals^{\numinduc}$ & Sparse inducing-interpolation vector for point $\bx$ (using \cref{eqn:ski}) \\
			\midrule
			$\widetilde k(\cdot, \cdot)$ & $\reals^\numdata \times \reals^\numdata \rightarrow \reals$ & Approximate kernel function (using \cref{eqn:inducing_interpolation}) \\
			$\widetilde \bK_{\bX\bX}$ & $\reals^{\numdata \times \numdata}$ & Approximate training kernel matrix (using \cref{eqn:inducing_interpolation}) \\
			\midrule
			$\meantest{ \cdot }$ & $\reals^\numdim \rightarrow \reals$ & Predictive mean function of the GP model \\
			$\vartest{ \cdot }$ & $\reals^\numdim \rightarrow \reals$ & Predictive variance function of the GP model\\
      $\covtest{ \cdot }{ \cdot }$ & $\reals^\numdim \rightarrow \reals$ & Predictive co-variance function of the GP model\\
			\bottomrule
		\end{tabular}
	}
\end{table}

\clearpage
