\section{Gaussian Process Regression}
\label{sec:gps}

%For someone without much prior knowledge (pun intended) about Gaussian processes, the terminology can appear confusing and overloaded.
In the machine learning literature the term \emph{Gaussian process} describes two different objects.
In one context it refers to a distribution over functions that can define a prior over $f(\cdot)$.
It can also refer the class of predictive models that make use of Gaussian process priors on $f(\cdot)$.
As we will soon see, these two uses are relatively interchangeable.
However, for clarity in this section we will explicitly distinguish when ``Gaussian process'' is describing a prior distribution over functions and when it is describing a class of predictive models.

\subsection{Gaussian Process Distributions}
A {\bf Gaussian process distribution} $f(\cdot) \sim \mathcal{GP}$ extends the multivariate-Gaussian distribution from finite-dimensional vectors to (infinite-dimensional) functions.
It is defined by a \emph{mean function} $\mu(\cdot)$ and a {\bf covariance function} or {\bf kernel function} $k(\cdot, \cdot)$.\footnote{

  Throughout this thesis we will use the two terms interchangeably.
}
%It can only be realized by applying $f$ to some finite set of points.
Given $N$ data points $\bX = [ \bx^{(1)}, \ldots, \bx^{(N)} ] \in \reals^{\numdata \times \numdim}$ (e.g. the $D$-dimensional features of $N$ different children in our running example),
the distribution over $\bfn = [ f(\bx^{(1)}), \ldots, f(\bx^{(N)}) ] \in \reals^\numdata$ is a multivariate Normal distribution:
\begin{equation}
 p \left( \bfn \right) = \normaldist{ \bfn ; \bmu_\bX}{ \bK_{\bX\bX} },
 \label{eqn:prior_finite}
\end{equation}
where $f^{(i)} = f(\bx^{(i)})$, $\mu^{(i)}_\bX = \mu( \bx^{(i)} )$ and $K_{\bX\bX}^{(i,j)} = k(\bx^{(i)}, \bx^{(j)})$.
(We will be using the above short-hand notations $\bfn$, $\bmu_\bX$, and $\bK_{\bX\bX}$ throughout the remainder of this thesis.)
The matrix $\bK_{\bX\bX} \in \reals^{\numdata \times \numdata}$ is often referred to as the {\bf kernel matrix} of $\bX$.

%Analogously to its finite-dimensional counterpart, a Gaussian process is completely defined by its mean and covariance functions $\mu(\cdot)$ and $k(\cdot, \cdot)$.
$\mu(\cdot)$ can be any real-valued function, though it is common simply to choose the zero-function (i.e. $\mu(\cdot) = 0$).
The covariance function $k(\cdot, \cdot)$ must be a valid kernel function, which means that all kernel matrices $\bK$ must be positive definite.
See \cref{sec:common_kernels} for common choices of $k(\cdot, \cdot)$.

\subsection{Gaussian Process Regression Models}
\label{sec:gp_models}

Recall our high-level regression model from \cref{eqn:regression}: $y = f \left( \bx \right) + \epsilon$.
{\bf Gaussian process regression models} are a class of predictive models where
%
\begin{enumerate}
  \item $f ( \cdot )$ is modelled by a Gaussian process prior: $f (\cdot) \sim \GP{\mu(\cdot)}{k(\cdot, \cdot)}$, and
  \item $\epsilon$ is modelled by a Gaussian noise distribution: $\epsilon \sim \normaldist{0}{\sigma_\text{obs}^2}$.
\end{enumerate}
%
Together, these two items define the prior distribution of the data.
To make predictions on previously-unseen test points $\bxtest, \ytest$ (e.g. predict the future weight of a child), we \emph{condition} the prior model on a set of previously-seen training data $\bX, \by$ (e.g. the weights and features of other children).
In total the predictive model is fully defined by:
\begin{enumerate}
  \item the mean function $\mu(\cdot)$ and covariance function $k(\cdot, \cdot)$ of the GP prior,
  \item the amount of observational noise $\sigma_\text{obs}^2$, and
  \item training data $\dset_\text{train} = (\bX, \by)$.
\end{enumerate}
%
Note that the only learnable parameters of this model are $\sigma_\text{obs}^2$ and whatever parameters are required by the mean/covariance functions.

\paragraph{The predictive distribution.}
In many supervised regression paradigms (e.g. neural networks, ridge regression, etc.), it is common to learn a single latent function $f^*(\cdot)$ that best fits the training data $\dset_\text{train}$.
Under such a setup, the predictive distribution for a test point $(\bxtest, \ytest)$ is given by
$ p(\ytest \vert f^*(\bxtest)) = \normaldist{ \ytest; f^*(\bxtest) }{ \sigma^2_\text{obs} }. $
However, Gaussian process regression models \emph{marginalize out} the choice of $f(\cdot)$.
Under this setup, the predictive distribution is given by:
\begin{equation*}
  p \left( \ytest \mid \bxtest, \dset_\text{train} \right)
  = \int_{f(\cdot)} p\left( y \mid f(\bxtest) \right) \: p\left( f(\bxtest) \mid \bxtest, \dset_\text{train} \right)
  \intd f(\cdot).
  %\label{eqn:marginalize}
\end{equation*}
%
This predictive distribution happens to be computable in closed form.\footnote{
  Note that this is rare; in general, exact Bayesian inference is intractable for most ``interesting'' machine learning models.
}
Given the Gaussian process prior on $f(\cdot)$ and the Gaussian noise observation model for $p(\ytest \vert f(\bxtest))$, the prediction $p(\ytest \vert \bxtest, \dset_\text{train})$ for a new child's future weight is given by a Gaussian distribution:
%
\begin{equation}
  p \left(
    \ytest \mid \bxtest, \dset_\text{train}
  \right)
  = \normaldist{ \ytest ; \meantest{\bxtest}}{ \vartest{\bxtest} }
  \label{eqn:predictive}
\end{equation}
%
where $\meantest{\cdot}$ and $\vartest{\cdot}$ are given by:
%
\begin{align}
  \meantest{\bxtest}
  &= \mu\left( \bxtest \right) + \bk_{\bX \bxtest}^\top \left( \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI \right)^{-1} \left( \by - \bmu \right)
  \label{eqn:predictive_mean}
  \\
  \vartest{\bxtest}
  &= k ( \bxtest, \bxtest) + \sigma^2_\text{obs} - \bk_{\bX \bxtest}^\top \left( \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI \right)^{-1} \bk_{\bX \bxtest}.
  \label{eqn:predictive_var}
\end{align}
%
Here the shorthand $\bk_{\bX \bxtest} \in \reals^N$ is the vector of covariances between $\bxtest$ and all training points $\bX = [\bx^{(1)}, \ldots, \bx^{(N)}].$
Note that, under \cref{eqn:predictive_mean,eqn:predictive_var}, the Gaussian process' prediction for a child's weight depends on
\begin{enumerate*}
  \item the weights of previously-observed children and
  \item the similarities between the children's features (as determined by their prior covariances $\bk_{\bX \bxtest}$).
\end{enumerate*}
We will discuss a framework for efficiently computing the predictive distribution in \cref{chapter:bbmm,chapter:love}.

\paragraph{Short derivation of the predictive distribution.}
To understand where \cref{eqn:predictive_mean} and \cref{eqn:predictive_var} come from, we start by writing the joint prior distribution for $[\bfn, f(\bxtest)]$.
Under the Gaussian process prior of $f(\cdot)$ and \cref{eqn:prior_finite}, this joint distribution will be a multivariate-Gaussian.
%
\begin{align*}
  p \left(
    \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} \mid
    \begin{bmatrix} \bX \\ \bxtest \end{bmatrix}
  \right)
  &= \normaldist{
    \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} ;
    \begin{bmatrix} \bmu \\ \mu(\bxtest) \end{bmatrix}
  }{
    \begin{bmatrix}
      \bK_{\bX \bX}    & \bk_{\bX \bxtest} \\
      \bk_{\bX \bxtest}^\top   & k(\bxtest, \bxtest)
    \end{bmatrix}
  }.
\end{align*}
%
We then compute the joint marginal likelihood $p( [ \by, \ytest ] \mid [ \bX, \bxtest ] )$ by integrating out the dependence on $p( [\bfn, f(\bxtest)] )$.
Under the Gaussian noise observation model of \cref{eqn:regression}, it happens that the marginal likelihood is also multivariate Gaussian and can be computed in closed form.
%
\begin{align*}
  p \left(
    \begin{bmatrix} \by \\ \ytest \end{bmatrix} \: \middle| \:
    \begin{bmatrix} \bX \\ \bxtest \end{bmatrix}
  \right)
  &= \int
    p \left(
      \begin{bmatrix} \by \\ \ytest \end{bmatrix} \: \middle| \:
      \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix}
    \right)
    p \left(
      \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} \: \middle| \:
      \begin{bmatrix} \bX \\ \bxtest \end{bmatrix}
    \right)
    \intd { \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} }
  \\
  \vspace{0.5em}
  &= \normaldist{
    \begin{bmatrix} \by \\ \ytest \end{bmatrix} ;
    \begin{bmatrix} \bmu \\ \mu(\bxtest) \end{bmatrix}
  }{
    \begin{bmatrix}
      \bK_{\bX \bX} + \sigma_\text{obs}^2 \mathbf I   & \bk_{\bX \bxtest} \\
      \bk_{\bX \bxtest}^\top         & k(\bxtest, \bxtest) + \sigma_\text{obs}^2
    \end{bmatrix}
  }.
\end{align*}
where the $\sigma_\text{obs}^2$ terms come from the observational noise (which we assume is independent for all data points).
%
Therefore, the predictive distribution $p(\ytest \: \vert \: \bxtest, \bX, \by)$ is simply the conditional of a multivariate Gaussian.
Applying standard Gaussian conditioning rules \citep[see e.g.][]{bishop2006pattern,rasmussen2006gaussian} results in \cref{eqn:predictive_mean} and \cref{eqn:predictive_var}.

\paragraph{Predictions on multiple test points.}
If there are multiple test points $\bxtest$, $\bxtestprime$ of interest, the predictions $p([\ytest, \ytestprime] \mid [\bxtest, \bxtestprime], \dset_\text{train})$ are jointly Gaussian distributed.
The mean is given by \cref{eqn:predictive_mean} and the covariance between the two points is given as
\[ \covtest{\bxtest}{\bxtestprime} = k(\bxtest, \bxtestprime) - \bk_{\bX\bxtest}^\top ( \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI )^{-1} \bk_{\bX\bxtestprime}. \]
In fact, any (finite) set of test points will be jointly Gaussian with the above covariance, meaning that the predictive distribution is also a Gaussian process.

\subsection{Training Gaussian Process Models}
\label{sec:gp_training}

The predictive distribution of Gaussian process regression model is non-parametric, which leaves us few terms that need to be learned.
However, we wish to pick the best mean function, kernel function, and observational noise that define a Gaussian process which best fits our training data $\dset_\text{train} = (\bX, \by)$.
Let $\btheta$ be the set of learnable {\bf hyperparameters} of these functions.
For example, $\btheta$ might include the lengthscale $\ell$ and outputscale $o^2$ of the kernel function (see \cref{sec:common_kernels}) as well as the observational noise parameter $\sigma_\text{obs}$.
In more complex Gaussian process models, $\btheta$ may also include inducing point locations \cite{titsias2009variational} or neural network parameters for deep kernel learning \cite{wilson2016deep}.
We can measure the fit of these parameters through the {\bf marginal log likelihood} of the Gaussian process applied to the training data $\bX, \by$:
%
\begin{align}
  -\log p( \by \mid \bX, \btheta)
  &= -\log \normaldist{ \by ; \bmu }{ \trainK }
  \nonumber{} \\
  &\propto \log \left \vert \trainK \right \vert + (\by - \bmu)^{\top} \trainK^{-1} (\by - \bmu),
  \label{eqn:log_lik}
\end{align}
%
where $\trainK$ is shorthand for the training kernel matrix with added observational noise: $\trainK = \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI$.
Note that the dependence on $\btheta$ is implicitly absorbed into $\bmu$ and $\trainK$.
We can use \cref{eqn:log_lik} to learn the parameters $\btheta$ via gradient-based optimization or sampling.
Its derivative is given by
%
\begin{align}
  \frac{\partial - \log p( \by \mid \bX, \btheta )}{\partial \btheta} &\propto
   \tr{\trainK^{-1}\frac{\partial \trainK}{\partial \btheta}} -
	 (\by - \bmu)^{\top} \trainK^{-1}\frac{\partial \trainK}{\partial \btheta}\trainK^{-1} (\by - \bmu).
  \label{eqn:log_lik_deriv}
\end{align}
%
We will discuss how to efficiently compute these terms in \cref{chapter:bbmm}.

%We formalize this assumption by modelling $y$ with a \emph{Gaussian likelihood}:
%%
%\begin{equation}
  %p(y \mid f(\bx)) = \normaldist{ y; f(\bx)}{\sigma^2_\text{obs}}.
  %\label{eqn:likelihood}
%\end{equation}
%%
%where again $\sigma^2_\text{obs}$ is a parameter that defines the observational noise variance.

%In a \emph{Gaussian process model}, we assume assume a priori that the latent function $f(\cdot)$ is drawn from a Gaussian process distribution $\normaldist{\mu(\cdot)}{k(\cdot, \cdot)}$.

%%%%%%%%%%%

%In general, we assume that the observational noise $\epsilon$ is independent for all data points.
%%
%\begin{equation}
  %p(\by \mid \bfn) = \normaldist{ \by; \bfn}{\sigma^2_\text{obs} \bI}.
  %\label{eqn:likelihood_all}
%\end{equation}

%Note that the likelihood is conditioned on our choice of latent function $f(\cdot)$.
%In a \emph{Gaussian process model}, we \emph{marginalizing} over all possible functions $f(\cdot)$.
%More formally, we assume a Gaussian process prior distribution over the model's function $f(\cdot) \sim \GP{\mu(\cdot)}{k(\cdot, \cdot)}$.
%We can then marginalize away any dependence on the function $f(\cdot)$ using the product rule of probability:
%%
%\begin{equation}
  %p( \by \mid \bX, \sigma^2_\text{obs}, \mu, k ) = \int_\bfn \: p( y \mid \bfn, \sigma^2_\text{obs} ) \: p( \bfn \mid \bX, \mu, k ) \intd \bfn.
  %\label{eqn:ml_full}
%\end{equation}
%%
%where $p( y \mid \bfn, \sigma^2_\text{obs})$ is given by \cref{eqn:likelihood_all} and $p( \bfn \mid \bX, \mu, k )$ is given by \cref{eqn:prior_finite}.
%Here we explicitly note the marginal likelihood's dependence on $\mu$, $k$, and $\sigma^2_\text{obs}$, though typically for brevity we will simply write it as $p(\by \mid \bX)$.
%Because $p( \bfn \mid \bX)$ and $p( y \mid \bfn)$ are both multivariate Gaussians, we can actually compute the integral in \cref{eqn:ml_full} in closed-form using common Gaussian identities (see e.g. \cite{bishop2006pattern,rasmussen2006gaussian}):
%%
%\begin{equation}
  %p( \by \mid \bX) = \normaldist{ \by; \bmu_\bX}{\trainK}, \quad
  %\trainK = \bK_{\bX\bX} + \sigma^2 \bI.
  %\label{eqn:ml_exact}
%\end{equation}







\subsection{Common Covariance Functions}
\label{sec:common_kernels}

Of the three objects that define a Gaussian process---the prior mean function, the prior covariance function, and the observed noise parameter---the covariance function arguably has the largest impact on the properties of the predictive distribution.
While the parameters of any particular kernel function can most often be learned through gradient descent, the practitioner must first choose the class of covariance functions to optimize over.
Choosing the right covariance function class will largely determine how the Gaussian process generalizes to unseen data points.
This is especially crucial in the small-data regime, where the predictive distribution closely follows the prior distribution.
If the covariance function properly encodes prior information about the function's properties, then the model can generalize well even when there are few training points.

In the running example of predicting a child's weight, there are several known functional properties that we can encode a priori.
For example, a child's weight will (in many cases) be roughly monotonic with respect to time.
Additionally, children who grow up in similar socioeconomic conditions will be more likely to have similar weights.
We will discuss how both of these properties can be encoded by choosing the appropriate kernel function.

\paragraph{Common kernel functions.}
Here we list some common and simple kernel functions that will be used throughout this thesis.
We will describe their functional form and their properties:
%
\begin{itemize}
  \item The {\bf Radial Basis Function (RBF)} kernel, or squared exponential kernel is one of the most commonly-used covariance functions.
    It's functional form is given by
    %
    \[ k_\text{RBF}(\bx, \bx') = o^2 \exp \left( \sum_{d=1}^D \frac{\left( x^{(d)} - x^{(d)\prime} \right)^2}{\ell_d^2}  \right) \]
    %
    where $o^2$ is referred to as the {\bf outputscale} parameter and the $\ell_1, \ldots, \ell_D$ are the {\bf lengthscales} for each dimension $d \in [1, D]$.
    When the $\ell_d$ parameter is different across all dimensions, the kernel is referred to as the {\bf RBF-ARD } kernel, where ARD stands for {\bf automatic relevance determination }.
    To reduce the number of learned parameters, it is sometimes common to share the same $\ell$ parameter across all dimensions.

    The RBF kernel is commonly considered to be a ``general'' kernel, since it is able to universally approximate any function given enough training data \cite{micchelli2006universal}.
		However, functions drawn from $f \sim \GP{\mu}{k_\text{RBF}}$ are infinitely differentiable, which may impose an unreasonable smoothness constraint in some applications \cite{stein2012interpolation}.

	\item The {\bf Mat\'{e}rn} kernel is another popular kernel which is arguably as general as the RBF kernel yet without the strict smoothness properties.
		The kernel introduces a parameter $\nu$ that determines how differentiable its sampled functions are.
		More concretely, the covariance between any two points is given as:
    %
    \[
			k_{\text{Mat}-\nu} (\bx, \bx') = o^2 \frac{2^{1 - \nu}}{\gamma(\nu)}
			\left( \sqrt{2 \nu} \sum_{d=1}^D \frac{\vert x^{(d)} - x^{(d)\prime} \vert}{\ell_d}  \right)^\nu
			K_\nu \left( \sqrt{2 \nu} \sum_{d=1}^D \frac{\vert x^{(d)} - x^{(d)\prime} \vert}{\ell_d}  \right),
		\]
		%
    where $o^2$ and $\ell_d$ are analogous to the parameters from the RBF kernel, and where $K_\nu$ is a modified Bessel function \cite{rasmussen2006gaussian}.
    Functions sampled from $f \sim \GP{\mu}{k_{\text{mat}-\nu}}$ are $\lceil \nu \rceil - 1$-times differentiable.
    As $\nu \rightarrow \infty$ we recover the RBF kernel.

    Typically, $o^2$ and $\ell_d$ are learned through gradient descent whereas $\nu$ is pre-selected and fixed.
    Common values for $\nu$ and $3/2$ and $5/2$, both of which have more succinct closed-form expressions.

  \item The {\bf Linear} kernel is one of the least expressive kernels, as it only has support for linear functions.
    However, it is commonly used as a building block for more complex kernels, as we will discuss shortly.
    It is given by:
    %
    \[ k_\text{Lin} ( \bx, \bx' ) = b^2 + o^2 (\bx - \bc)^\top (\bx - \bc), \]
    %
    where $b$, $\bc$, and $o$ are learnable parameters.
\end{itemize}

There are many other kernels that can encode different types of functional structure such as periodicity (using the periodic kernel), multiple lengthscales (using the rational quadratic kernel), or erratic/noisy dynamics (using Ornstein-Uhlenbeck kernel).
See \cite{rasmussen2006gaussian} for more discussion on basic kernel types.

\paragraph{Composing kernels.}
Kernels can be composed together to combine the functional properties.
The two most common methods for composition are addition: $k(\bx, \bx') = k_1(\bx, \bx') + k_2(\bx, \bx')$
and multiplication: $k(\bx, \bx') = k_1(\bx, \bx') k_2(\bx, \bx')$.
The sub-kernels $k_1$ and $k_2$ can operate on any subset of the dimensions of $\bx$.
In our running example, we may wish to model our data with the kernel
%
\[ k(\bx, \bx') = k_\text{Lin}( \bx_\text{time}, \bx_\text{time}' ) + k_\text{RBF}( \bx_\text{econ}, \bx_\text{econ}' ) \]
%
where $\bx_\text{time}$ and $\bx_\text{econ}$ are the time and socioeconomic features of $\bx$ respectively.
This would encode our first prior assumption (a child's weight increases near-monotonically with time)
and our second prior assumption (children from a similar socioeconomic background should have similar development).


\paragraph{Stationary kernels.}
The RBF and Mat\'ern kernels belong to an important class of kernels known as {\bf stationary kernels}.
This class more generally includes all kernels that are simply a function of the distance between two points:
\begin{equation}
  k_\text{stationary} (\bx, \bx') = \phi( \vert \bx - \bx' \vert ),
  \label{eqn:stationary}
\end{equation}
for some function $\phi$.\footnote{
  Note that the linear kernel cannot be written in this way and is therefore non-stationary.
}
This class of kernels has many important properties; see \citep[e.g.][]{rasmussen2006gaussian,wilson2013gaussian} for more details.
For the purpose of this thesis, we draw attention to stationary kernels as they allow for potential computational savings.
For example, the computational complexity of GPs with stationary kernels is reduced when the data lie on a regularly-spaced grid \cite{cunningham2008fast,saatcci2012scalable} or can be approximated by grid points \cite{wilson2014thesis,wilson2015kernel}.


\paragraph{Spectral and Deep kernels.}
For datasets with well-understood properties, it can be advantageous to hand-choose or construct kernels that best encode prior knowledge.
With more complex datasets, it may be too difficult to manually encode a specific functional structure.
In such settings, it is better to automatically learn functional structure directly from the data using a highly-parametric kernel, such as the {\bf spectral-mixture} kernel \cite{wilson2013gaussian} or a {\bf deep kernel} \cite{wilson2016deep}.
At a high level, these kernels learn the $\phi$ function in \cref{eqn:stationary} using a mixture model or a neural network.
Though such learning often requires larger datasets, it can result in kernel functions that more accurately model the data.

The spectral-mixture kernel (in one dimension) is defined by a mixture of $Q$ functions:
%
\[
  k_\text{SM} (x, x') = \sum_{q=1}^Q w_q
    \exp \left( -2 \pi^2 (x - x')^2 v_q \right)
    \cos \left( -2 \pi (x - x')^2 \mu_q \right),
\]
%
where $w_q$, $v_q$, and $\mu_q$ are learnable parameters.
It is well-suited for interpolation and extrapolation tasks of time series or spatial data \cite{wilson2013gaussian}.
Deep kernels \cite{wilson2016deep,wilson2016stochastic,calandra2016manifold} are kernels where the inputs $\bx$, $\bx'$ are first transformed using a deep neural network $\phi$:
%
\[
  k_\text{deep} (x, x') = k_\text{base} \left( \phi(\bx), \phi(\bx') \right),
\]
where $k_\text{base}(\cdot, \cdot)$ is a simpler base kernel (usually the RBF kernel).
The parameters of $\phi$ are learned by optimizing the marginal log likelihood in \cref{eqn:log_lik} \cite{wilson2016deep}.
It is well suited to high-dimensional datasets or datasets with complex features.







\subsection{Approximate Gaussian Processes}
\label{sec:approx_gps}

As we will discuss in detail in the next chapters, computing the marginal log likelihood (\cref{eqn:log_lik}) and the predictive distribution (\cref{eqn:predictive_mean,eqn:predictive_var}) has historically been computationally prohibitive for large datasets.
This has motivated the development of several approximate Gaussian process regression methods \citep[e.g.][]{quinonero2005unifying,snelson2006sparse,rahimi2008random,titsias2009variational,hensman2013gaussian,wilson2015kernel,izmailov2018scalable,gardner2018product,evans2018scalable} that trade off exactness for computational scalability.
Many of these methods approximate the training kernel matrix $\bK_{\bX\bX}$ with a structured or low-rank matrix $\approxK$ that affords faster matrix operations and requires less storage.
Here we highlight some notable variants of the basic approach, though it is by no means an exhaustive list---see \cite{liu2018gaussian} for a more thorough review.


\paragraph{Inducing Points.}
A common mechanism for scalable methods is to introduce a set of pseudo-inputs, or {\bf inducing points}, which function as a sufficient statistic for the predictive distribution.
The kernel function for \emph{real}-inputs $\bx$, $\bx'$ is approximated by interpolating against the kernel matrix for the \emph{pseudo}-inputs $\bZ = [\bz^{(1)}, \ldots, \bz^{(M)}]$.
%
\begin{align}
  k(\bx, \bx') \approx \widetilde k(\bx, \bx') &= \:\:
		\overbracket{ \left( \bk_{\bZ\bx}^\top \bK_{\bZ\bZ}^{-1} \right) }^{\text{interpolation}} \:\: \bK_{\bZ\bZ} \:\:
		\overbracket{ \left( \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx'} 		 \right) }^{\text{interpolation}}
	\nonumber{} \\
	&= \:\:
	\bk_{\bZ\bx}^\top \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx'}.
	\label{eqn:inducing_interpolation}
\end{align}
%
In its simplest form, it is common to choose $M \ll N$ inducing points.
The approximate training kernel matrix $\widetilde \bK_{\bX\bX} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX}$ will then have low-rank structure, allowing for efficient computation of \cref{eqn:predictive_mean,eqn:predictive_var,eqn:log_lik,eqn:log_lik_deriv}.
The locations of the inducing points $\bZ$ are considered to be additional parameters to optimize via \cref{eqn:log_lik} (typically with gradient descent).

Here we have presented inducing points as a mechanism for approximating kernel matrices.
Under this view, we can use the same training/prediction equations as presented in \cref{sec:gp_models,sec:gp_training}---replacing the kernel matrices/vectors with their approximations.
As we will discuss in \cref{chapter:bbmm}, this allows us to create a simple unifying framework to Gaussian process training and inference.
We do note it is also possible to motivate/derive inducing point methods in other ways---e.g. through a greedy approximation \cite{smola2001sparse} or a variational bound \cite{titsias2009variational,hensman2013gaussian}.
However, under these interpretations it is less straightforward to derive connections to \cref{eqn:predictive_mean,eqn:predictive_var,eqn:log_lik,eqn:log_lik_deriv}.

\paragraph{Variants.}
The basic formula of \cref{eqn:inducing_interpolation} can be extended to create a variety of approximate Gaussian process regression methods.
We briefly present three common variants here---all of which use $M \ll N$ inducing points and learn their locations through gradient descent of \cref{eqn:log_lik}.
In the next section we will discuss Kernel Interpolation for Scalable Structured GPs (KISS-GP) \cite{wilson2015kernel}---a variant which utilizes inducing points in a unique way.

\begin{itemize}
	\item {\bf Subset of Regressors (SoR)} \cite{silverman1985some,smola2001sparse} is arguably the most standard variant.
    Here, the kernel matrix $\bK_{\bX\bX}$ and all kernel vectors $\bk_{\bX\bxtest}$ are replaced with their approximate variants:
    \[ \widetilde \bK_{\bX\bX} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX}, \quad
       \widetilde \bk_{\bX\bxtest} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bk_{\bX\bxtest}. \]

	\item {\bf Fully Independent Training Conditional (FITC) } \cite{snelson2006sparse} can be interpreted as an extension of SoR
		where the approximate training kernel matrix has a diagonal correction term $\bLambda$.
    \[ \widetilde \bK_{\bX\bX} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX} + \bLambda, \quad
       \Lambda^{(i,i)} = k(\bx^{(i)}, \bx^{(i)}) - \bk_{\bZ\bx^{(i)}}^\top \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx^{(i)}}. \]
    The diagonal correction term is motivated through a connection to approximate inference.

	\item {\bf Sparse Gaussian Process Regression (SGPR) } \cite{titsias2009variational} uses the same kernel approximations as SoR.
		However, when learning the inducing point locations and other parameters, SGPR uses an alternative objective function:
		the marginal log likelihood of \cref{eqn:log_lik} augmented with a diagonal correction term:
    \[ \loglik(\by \mid \bX, \btheta) = -\log p(\by \mid \bX, \btheta) + \sum_{i=1}^N \left( k(\bx^{(i)}, \bx^{(i)}) - \bk_{\bZ\bx^{(i)}}^\top \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx^{(i)}} \right). \]
    The diagonal correction term can be derived by optimizing a variational lower bound on the function values at inducing point locations.
\end{itemize}
%
We will discuss how to exploit this low-rank kernel matrix structure for faster inference in the next chapter.


\subsection{Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)}
\label{sec:kissgp}

Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP) \cite{wilson2015kernel} utilizes inducing points in a fundamentally different way.
While other methods tend to use $M \ll N$ inducing points, \citet{wilson2015kernel} demonstrate that it is possible to set $M \gg N$ by making the following restrictions:
\begin{enumerate}
  \item the interpolation terms in \cref{eqn:inducing_interpolation} are approximated by a \emph{sparse interpolation vector} $\bw_\bx \approx \bK^{-1}_{\bZ\bZ} \bk_{\bZ\bx};$
  \item the inducing points $\bZ = [\bz^{(1)}, \ldots, \bz^{(M)}]$ must lie on a \emph{regularly-spaced grid};
  \item the kernel function is \emph{stationary} (e.g. RBF, Mat\'ern, etc.).
\end{enumerate}
The resulting approximate kernel matrix has nice algebraic structure that can be exploited for efficient storage and fast computations.
In particular, the approximate kernel matrix $\widetilde \bK_{\bX\bX}$ affords fast \emph{matrix-vector multiplications} (MVMs), which will translate to fast Gaussian process training and inference (using the methods described in \cref{chapter:bbmm,chapter:love}).

In more detail, KISS-GP assumes that a data point $\bx$ is well-approximated as a \emph{local interpolation} of $\bZ$.
Using cubic interpolation \cite{keys1981cubic}, $\bx$ is expressed in terms of its 4 closest inducing points, and the interpolation weights are captured in the sparse vector $\bw_\bx$.
The $\bw_\bx$ vectors approximate the training kernel matrix $\bK_{\bX\bX} \approx \tilde \bK_{\bX\bX}$ via \cref{eqn:inducing_interpolation}:
%
\begin{align}
  \widetilde \bK_{\bX\bX} &= \:\:
    \left( \overbracket{ \bK_{\bZ\bX}^\top \bK_{\bZ\bZ}^{-1} }^{ \approx \bW_{\bX}^\top} \right)
    \bK_{\bZ\bZ}
    \left( \overbracket{ \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX} }^{ \approx \bW_{\bX}} \right)
  \nonumber{} \\
  &\approx \:\: \bW_{\bX}^\top \: \bK_{\bZ\bZ} \: \bW_{\bX}.
  \label{eqn:ski}
\end{align}
%
Here, $\bW_{\bX} = [\bw_{\bx^{(1)}}, \ldots, \bw_{\bx^{(N)}}]$ contains the interpolation vectors for all $\bx^{(i)}$.
Performing a matrix-vector multiplication with $\widetilde \bK_{\bX\bX}$ (i.e. $( \bW_\bX^\top \bK_{\bZ\bZ} \bW_{\bX}) \bv$ for any vector $\bv$) takes \emph{near-linear} time.
To see why, a MVM with the matrix $\bW_\bX$ require $\bigo{N}$ time due to the $\bigo{N}$ sparsity of $\bW_\bX$.
Moreover, restrictions 2 and 3 (stationary kernel/grided inducing points) endow the inducing kernel matrix $\bK_{\bZ\bZ}$ with Toeplitz structure (or Kronecker/Toeplitz structure if the inputs $\bx^{(i)}$ are multi-dimensional).
By exploiting this structure, $\bK_{\bZ\bZ}$ requires $\bigo{M}$ storage and MVMs with $\bK_{\bZ\bZ}$ require $\bigo{M \log M}$ time \citep[see][for details]{cunningham2008fast,saatcci2012scalable}.
Thus, if we perform $( \bW_\bX^\top \bK_{\bZ\bZ} \bW_{\bX}) \bv$ from right-to-left, the entire multiplication takes $\bigo{N + M \log M}$ time.
As we will demonstrate in the next section, exploiting these fasts MVMs results in a near-linear training time complexity for KISS-GP.

%
\paragraph{Computing predictive means.}
One advantage of KISS-GP's structure is the ability to perform constant time predictive mean calculations \cite{wilson2015thoughts}.
Substituting the KISS-GP approximate kernel into \cref{eqn:predictive_mean} and assuming a prior mean of 0 for notational brevity, the predictive mean is given by
\begin{equation}
  \meantest{\bxtest} = \bw_{\bxtest}^\top {\blue{\underbracket{\bK_{\bZ\bZ} \bW_{\bX}(\bW_{\bX}^{\top}\bK_{\bZ\bZ}\bW_{\bX} + \sigma_\text{obs}^{2} \bI)^{-1} \by}_{\ba'}}}.
  \label{eqn:pred_mean_ski}
\end{equation}
(Here {\color{blue} blue} highlights computations that don't depend on test data.)
Because $\bw_{\bx^{*}}$ is the only term in \cref{eqn:pred_mean_ski} that depends on $\bx^{*}$, the remainder of the equation (denoted as $\blue \ba'$) can be pre-computed: $\meantest{\bxtest} = \bw_{\bxtest}^\top \blue \ba'$.
After pre-computing $\blue \ba'$, the multiplication $\bw_{\bxtest}^\top \blue \ba'$ requires $\bigo{1}$ time, as $\bw_{\bxtest}$ is sparse and has only four nonzero elements.

\subsection{Summary of Notation}
All the notation of this section is summarized in \cref{tab:gp_notation}.
It will be used throughout the remaining chapters.

\begin{table}[!b]
  \centerfloat
  \caption{Summary of Gaussian process notation.}
  \label{tab:gp_notation}
	\resizebox{1.\textwidth}{!}{%
		\begin{tabular}{ ccc }
			\toprule
			{\bf Notation} & {\bf Domain} & {\bf Description} \\
			\midrule
			\midrule
			$\numdata$ & $\ints$ & Number of training data points \\
			$\numdim$ & $\ints$ & Dimensionality of inputs \\
			$\numinduc$ & $\ints$ & Number of inducing data points \\
			\midrule
			$\bX$ & $\reals^{\numdata \times \numdim}$ & Training features \\
			$\by$ & $\reals^{\numdata}$ & Training targets \\
			$\bxtest$ & $\reals^{\numdim}$ & Features of a test data point \\
			$\ytest$ & $\reals$ & Target of a test data point \\
			\midrule
			$\mu( \cdot )$ & $\reals^\numdim \rightarrow \reals$ & The GP (prior) mean function \\
			$k( \cdot, \cdot )$ & $\reals^\numdim \times \reals^\numdim \rightarrow \reals$ & The GP (prior) covariance/kernel function \\
			$\sigma_\text{obs}^2$ & $\reals$ & Observational variance of the Gaussian likelihood \\
			$f( \cdot )$ & $\reals^\numdim \rightarrow \reals$ & Latent function (modelled with a GP prior) \\
			$\btheta$ &  & Parameters of kernel, likelihood, etc. \\
			\midrule
			$\bmu_\bX$ & $\reals^\numdata$ & (Prior) mean vector for training data $\bX$ \\
			$\bK_{\bX\bX}$ & $\reals^{\numdata \times \numdata}$ & (Prior) kernel matrix for training data $\bX$ \\
			$\trainK$ & $\reals^{\numdata \times \numdata}$ & (Prior) kernel matrix $\bK_{\bX\bX}$ plus observational noise $\sigma^2_\text{obs} \bI$ \\
			$\bk_{\bX\bxtest}$ & $\reals^{\numdata}$ & Prior covariance between training data $\bX$ and point $\bxtest$\\
			$\bfn$ & $\reals^\numdata$ & Latent function evaluated on training data $\bX$ \\
			\midrule
			$\bZ$ & $\reals^{\numinduc \times \numdim}$ & Locations of inducing points \\
			$\bK_{\bZ\bZ}$ & $\reals^{\numinduc \times \numinduc}$ & (Prior) kernel matrix for inducing points $\bZ$ \\
			$\bk_{\bZ\bx}$ & $\reals^{\numinduc}$ & Prior covariance between inducing points $\bZ$ and point $\bx$ \\
			$\bW_{\bX}$ & $\reals^{\numdata \times \numinduc}$ & Sparse inducing-interpolation matrix for training data $\bX$ (using \cref{eqn:ski}) \\
			$\bw_{\bx}$ & $\reals^{\numinduc}$ & Sparse inducing-interpolation vector for point $\bx$ (using \cref{eqn:ski}) \\
			\midrule
			$\widetilde k(\cdot, \cdot)$ & $\reals^\numdata \times \reals^\numdata \rightarrow \reals$ & Approximate kernel function (using \cref{eqn:inducing_interpolation}) \\
			$\widetilde \bK_{\bX\bX}$ & $\reals^{\numdata \times \numdata}$ & Approximate training kernel matrix (using \cref{eqn:inducing_interpolation}) \\
			\midrule
			$\meantest{ \cdot }$ & $\reals^\numdim \rightarrow \reals$ & Predictive mean function of the GP model \\
			$\vartest{ \cdot }$ & $\reals^\numdim \rightarrow \reals$ & Predictive variance function of the GP model\\
      $\covtest{ \cdot }{ \cdot }$ & $\reals^\numdim \rightarrow \reals$ & Predictive co-variance function of the GP model\\
			\bottomrule
		\end{tabular}
	}
\end{table}

\clearpage
