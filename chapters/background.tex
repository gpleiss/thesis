\chapter{Background}

For a majority of this thesis we will assume that we are building predictive models for supervised regression problems.
As a running example, imagine that we wish to predict a child's future weight $y \in \reals$ from some features about the child $\bx \in \reals^\numdim$---such as the their current weight, their mother's income level, etc.
We assume that the child's height $y$ can be explained by 1) some latent function $f(\bx)$ of the features and 2) some observational noise $\epsilon$:
%
\begin{equation}
  y = f \left( \bx \right) + \epsilon, \quad \epsilon \sim \normaldist{0}{\sigma_\text{obs}^2}.
  \label{eqn:regression}
\end{equation}
%
In particular, we assume the observational noise follows some Gaussian distribution with variance $\sigma_\text{obs}^2$.
At a high level, the goal of supervised regression is to find a function $f(\bx)$ (or a distribution of functions) that fits the available training data $\dset_\text{train} = \{(\bx_1, y_1), \ldots, (\bx_n, y_n)\}$ and makes accurate predictions on new data points $( \bxtest, \ytest )$ at test time.

This setup is one of the most studied problems in machine learning, and there are numerous different approaches to tackling this problem.
In this thesis we concern ourselves with the approach of \emph{Gaussian process regression}, which is a {\bf non-parametric} and {\bf Bayesian} approach to regression.
It is non-parametric because the class of possible functions $f(\cdot)$ is defined through relationships between pairs of training data pointsl
rather than through some prescribed functional form (e.g. linear functions, quadratic functions, neural networks, etc.)
It is Bayesian because its predictions come from the \emph{distribution} of all possible functions $f(\cdot)$.
In other words, the choice of $f(\cdot)$ is marginalized out.

\section{Gaussian Process Regression}

%For someone without much prior knowledge (pun intended) about Gaussian processes, the terminology can appear confusing and overloaded.
In the machine learning literature the term \emph{Gaussian process} describes two different objects.
In one context it refers to a distribution over functions that is used to define a prior over $f(\cdot)$.
It can also refer the class of predictive models that make use of Gaussian process priors on $f(\cdot)$.
As we will soon see, these two uses refer to the same family of distributions.
However, for clarity in this section we will distinguish when ``Gaussian process'' is describing a prior distribution over functions and when it is describing a class of predictive models.

\subsection{Gaussian Process Distributions}
A \emph{Gaussian process distribution} $f(\cdot) \sim \mathcal{GP}$ extends the multivariate-Gaussian distribution from finite-dimensional vectors to (infinite-dimensional) functions.
It is defined by a \emph{mean function} $\mu(\cdot)$ and a \emph{covariance function} or \emph{kernel function} $k(\cdot, \cdot)$.
%It can only be realized by applying $f$ to some finite set of points.
Given $N$ data points $\bX = [ \bx_1, \ldots, \bx_N ] \in \reals^{\numdata \times \numdim}$ (e.g. the $D$-dimensional features of $N$ different children in our running example),
the distribution over $\bfn = [ f(\bx_1), \ldots, f(\bx_N) ] \in \reals^\numdata$ is a multivariate Normal distribution:
%The multivariate Normal's mean $\bmu_\bX$ comes from a \emph{mean function} $\mu(\cdot)$ applied to each $\bx_i$, and its covariance matrix $\bK_{\bX\bX}$ comes from a \emph{covariance function} $k(\cdot, \cdot)$ (also referred to as a \emph{kernel function}) applied to every pair of data points:
\begin{equation}
 p \left( \bfn \right) = \normaldist{ \bfn ; \bmu_\bX}{ \bK_{\bX\bX} },
 \label{eqn:prior_finite}
\end{equation}
where $f^{(i)} = f(\bx_i)$, $\mu^{(i)}_\bX = \mu( \bx_i )$ and $\bK_{\bX\bX}^{(i,j)} = k(\bx_i, \bx_j)$.
(We will be using the above short-hand notations $\bfn$, $\bmu_\bX$, and $\bK_{\bX\bX}$ throughout the remainder of this thesis.)
The matrix $\bK_{\bX\bX} \in \reals^{\numdata \times \numdata}$ is often referred to as the \emph{kernel matrix} of $\bX$.

%Analogously to its finite-dimensional counterpart, a Gaussian process is completely defined by its mean and covariance functions $\mu(\cdot)$ and $k(\cdot, \cdot)$.
$\mu(\cdot)$ can be any real-valued function, though it is common simply to choose the zero-function (i.e. $\mu(\cdot) = 0$).
The covariance function $k(\cdot, \cdot)$ must be a valid kernel function, which means that all kernel matrices $\bK$ must be positive definite.
See \cref{sec:common_kernels} for common choices of $k(\cdot, \cdot)$.

\subsection{Gaussian Process Models}
\label{sec:gp_models}

Recall our high-level regression model from \cref{eqn:regression}: $y = f \left( \bx \right) + \epsilon$.
\emph{Gaussian process regression models} are a class of predictive models where
%
\begin{enumerate}
  \item $f ( \cdot )$ is modelled by a Gaussian process prior: $f (\cdot) \sim \GP{\mu(\cdot)}{k(\cdot, \cdot)}$, and
  \item $\epsilon$ is modelled by a Gaussian noise distribution: $\epsilon \sim \normaldist{0}{\sigma_\text{obs}^2}$.
\end{enumerate}
%
Together, these two items define the prior distribution of the data.
To make predictions on previously-unseen test points $\bxtest, \ytest$ (e.g. predict the future weight of a child), we \emph{condition} the prior model on a set of previously-seen training data $\bX, \by$ (e.g. the weights and features of other children).
In total the predictive model is fully defined by:
\begin{enumerate}
  \item the mean function $\mu(\cdot)$ and covariance function $k(\cdot, \cdot)$ of the GP prior,
  \item the amount of observational noise $\sigma_\text{obs}^2$, and
  \item training data $\dset_\text{train} = (\bX, \by)$.
\end{enumerate}
%
Note that the only learnable parameters of this model are $\sigma_\text{obs}^2$ and whatever parameters are required by the mean/covariance functions.

\paragraph{The predictive distribution.}
In many supervised regression paradigms (e.g. neural networks, ridge regression, etc.), it is common to learn a single latent function $f^*(\cdot)$ that best fits the training data $\dset_\text{train}$.
Under such a setup, the predictive distribution for a test point $(\bxtest, \ytest)$ is given by
$ p(\ytest \vert f^*(\bxtest)) = \normaldist{ \ytest; f^*(\bxtest) }{ \sigma^2_\text{obs} }. $
However, for Gaussian process regression models, it is most common to utilize full Bayesian inference and \emph{marginalize out} the choice of $f(\cdot)$.
Under this setup, the predictive distribution is given by:
\begin{equation*}
  p \left( \ytest \mid \bxtest, \dset_\text{train} \right)
  = \int_{f(\cdot)} p\left( y \mid f(\bxtest) \right) \: p\left( f(\bxtest) \mid \bxtest, \dset_\text{train} \right)
  \intd f(\cdot).
  %\label{eqn:marginalize}
\end{equation*}
%
This predictive distribution happens to be computable in closed form.\footnote{
  Note that this is rare; in general, full Bayesian inference is intractable for most ``interesting'' machine learning models.
}
Given the Gaussian process prior on $f(\cdot)$ and the Gaussian noise observation model for $p(\ytest \vert f(\bxtest))$, the prediction $p(\ytest \vert \bxtest, \dset_\text{train})$ for a new child's future weight is given by a Gaussian distribution:
%
\begin{equation}
  p \left(
    \ytest \mid \bxtest, \dset_\text{train}
  \right)
  = \normaldist{ \ytest ; \meantest{\bxtest}}{ \covtest{\bxtest} }
  \label{eqn:predictive}
\end{equation}
%
where $\meantest{\cdot}$ and $\covtest{\cdot}$ are given by:
%
\begin{align}
  \meantest{\bxtest}
  &= \mu\left( \bxtest \right) + \bk_{\bX \bxtest}^\top \left( \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI \right)^{-1} \left( \by - \bmu \right)
  \label{eqn:predictive_mean}
  \\
  \covtest{\bxtest}
  &= k ( \bxtest, \bxtest) + \sigma^2_\text{obs} - \bk_{\bX \bxtest}^\top \left( \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI \right)^{-1} \bk_{\bX \bxtest}.
  \label{eqn:predictive_var}
\end{align}
%
Here the shorthand $\bk_{\bX \bxtest} \in \reals^N$ is the vector of covariances between $\bxtest$ and all training points $\bX = [\bx_1, \ldots, \bx_N].$
Note that, under \cref{eqn:predictive_mean,eqn:predictive_var}, the Gaussian process' prediction for a child's weight depends on
\begin{enumerate*}
  \item the weights of previously-observed children and
  \item the similarities between the children's features (as determined by their prior covariances $\bk_{\bX \bxtest}$).
\end{enumerate*}
We will discuss a framework for efficiently computing the predictive distribution in \cref{chapter:bbmm,chapter:love}.

\paragraph{Short derivation of the predictive distribution.}
To understand where \cref{eqn:predictive_mean} and \cref{eqn:predictive_var} come from, we start by writing the joint prior distribution for $[\bfn, f(\bxtest)]$ using \cref{eqn:prior_finite}.
Under the Gaussian process prior of $f(\cdot)$, this joint distribution will be a multivariate-Gaussian.
%
\begin{align*}
  p \left(
    \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} \mid
    \begin{bmatrix} \bX \\ \bxtest \end{bmatrix}
  \right)
  &= \normaldist{
    \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} ;
    \begin{bmatrix} \bmu \\ \mu(\bxtest) \end{bmatrix}
  }{
    \begin{bmatrix}
      \bK_{\bX \bX}    & \bk_{\bX \bxtest} \\
      \bk_{\bX \bxtest}^\top   & k(\bxtest, \bxtest)
    \end{bmatrix}
  }.
\end{align*}
%
We then compute the joint marginal likelihood $p( [ \by, \ytest ] \mid [ \bX, \bxtest ] )$ by integrating out the dependence on $p( [\bfn, f(\bxtest)] )$.
Under the Gaussian noise observation model of \cref{eqn:regression}, it happens that the marginal likelihood is also multivariate Gaussian and can be computed in closed form.
%
\begin{align*}
  p \left(
    \begin{bmatrix} \by \\ \ytest \end{bmatrix} \mid
    \begin{bmatrix} \bX \\ \bxtest \end{bmatrix}
  \right)
  &= \int
    p \left(
      \begin{bmatrix} \by \\ \ytest \end{bmatrix} \mid
      \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix}
    \right)
    p \left(
      \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} \mid
      \begin{bmatrix} \bX \\ \bxtest \end{bmatrix}
    \right)
    \intd { \begin{bmatrix} \bfn \\ f(\bxtest) \end{bmatrix} }
  \\
  \vspace{0.5em}
  &= \normaldist{
    \begin{bmatrix} \by \\ \ytest \end{bmatrix} ;
    \begin{bmatrix} \bmu \\ \mu(\bxtest) \end{bmatrix}
  }{
    \begin{bmatrix}
      \bK_{\bX \bX} + \sigma_\text{obs}^2 \mathbf I   & \bk_{\bX \bxtest} \\
      \bk_{\bX \bxtest}^\top         & k(\bxtest, \bxtest) + \sigma_\text{obs}^2
    \end{bmatrix}
  }.
\end{align*}
where the $\sigma_\text{obs}^2$ terms come from the observational noise (which we assume is independent for all data points).
\gp{TODO: fix mid sign}
%
Therefore, the predictive distribution $p(\ytest \: \vert \: \bxtest, \bX, \by)$ is simply the conditional of a multivariate Gaussian.
Applying standard Gaussian conditioning rules \citep[see e.g.][]{bishop2006pattern,rasmussen2006gaussian} results in \cref{eqn:predictive_mean} and \cref{eqn:predictive_var}.

\paragraph{Predictions on multiple test points.}
If there are multiple test points $\bxtest$, $\bxtestprime$ of interest, the predictions $p([\ytest, \ytestprime] \mid [\bxtest, \bxtestprime], \dset_\text{train})$ are jointly Gaussian distributed.
The mean is given by \cref{eqn:predictive_mean} and the covariance between the two points is given as
\[ \covtest{\bxtest, \bxtestprime} = k(\bxtest, \bxtestprime) - \bk_{\bX\bxtest}^\top ( \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI )^{-1} \bk_{\bX\bxtestprime}. \]
In fact, any (finite) set of test points will be jointly Gaussian with the above covariance, meaning that the predictive distribution is also a Gaussian process.

\subsection{Training Gaussian Process Models}
\label{sec:gp_training}

The predictive distribution of Gaussian process regression model is non-parametric, which leaves us few terms that need to be learned.
However, we wish to pick the best mean function, kernel function, and observational noise that define a Gaussian process which best fits our training data $\dset_\text{train} = (\bX, \by)$.
Let $\btheta$ be the set of learnable \emph{hyperparameters} of these functions.
For example, $\btheta$ might include the lengthscale $\ell$ and outputscale $o^2$ of the kernel function (see \cref{sec:common_kernels}) as well as the observational noise parameter $\sigma_\text{obs}$.
In more complex Gaussian process models, $\btheta$ may also include inducing point locations \cite{titsias2009variational} or neural network parameters for deep kernel learning \cite{wilson2016deep}.
We can measure the fit of these parameters through the \emph{marginal log likelihood} of the Gaussian process applied to the training data $\bX, \by$:
%
\begin{align}
  -\log p( \by \mid \bX, \btheta)
  &= -\log \normaldist{ \by ; \bmu }{ \trainK }
  \nonumber{} \\
  &\propto \log \left \vert \trainK \right \vert + (\by - \bmu)^{\top} \trainK^{-1} (\by - \bmu),
  \label{eqn:log_lik}
\end{align}
%
where $\trainK$ is shorthand for the training kernel matrix with added observational noise: $\trainK = \bK_{\bX\bX} + \sigma_\text{obs}^2 \bI$.
Note that the dependence on $\btheta$ is implicitly absorbed into $\bmu$ and $\trainK$.
We can use \cref{eqn:log_lik} to learn the parameters $\btheta$ via gradient-based optimization or sampling.
Its derivative is given by
%
\begin{align}
  \frac{\partial - \log p( \by \mid \bX, \btheta )}{\partial \btheta} &\propto
   \tr{\trainK^{-1}\frac{\partial \trainK}{\partial \btheta}} -
	 (\by - \bmu)^{\top} \trainK^{-1}\frac{\partial \trainK}{\partial \btheta}\trainK^{-1} (\by - \bmu).
  \label{eqn:log_lik_deriv}
\end{align}
%
We will discuss how to efficiently compute these terms in \cref{chapter:bbmm}.

%We formalize this assumption by modelling $y$ with a \emph{Gaussian likelihood}:
%%
%\begin{equation}
  %p(y \mid f(\bx)) = \normaldist{ y; f(\bx)}{\sigma^2_\text{obs}}.
  %\label{eqn:likelihood}
%\end{equation}
%%
%where again $\sigma^2_\text{obs}$ is a parameter that defines the observational noise variance.

%In a \emph{Gaussian process model}, we assume assume a priori that the latent function $f(\cdot)$ is drawn from a Gaussian process distribution $\normaldist{\mu(\cdot)}{k(\cdot, \cdot)}$.
%Given a training dataset $\bX = [\bx_1, \ldots, \bx_n]$ and $\by = [y_1, \ldots, y_n]$,

%%%%%%%%%%%

%In general, we assume that the observational noise $\epsilon$ is independent for all data points.
%Therefore, given a training dataset $\bX = [\bx_1, \ldots, \bx_n]$ and $\by = [y_1, \ldots, y_n]$, we can write the likelihood for all data points as
%%
%\begin{equation}
  %p(\by \mid \bfn) = \normaldist{ \by; \bfn}{\sigma^2_\text{obs} \bI}.
  %\label{eqn:likelihood_all}
%\end{equation}

%Note that the likelihood is conditioned on our choice of latent function $f(\cdot)$.
%In a \emph{Gaussian process model}, we \emph{marginalizing} over all possible functions $f(\cdot)$.
%More formally, we assume a Gaussian process prior distribution over the model's function $f(\cdot) \sim \GP{\mu(\cdot)}{k(\cdot, \cdot)}$.
%We can then marginalize away any dependence on the function $f(\cdot)$ using the product rule of probability:
%%
%\begin{equation}
  %p( \by \mid \bX, \sigma^2_\text{obs}, \mu, k ) = \int_\bfn \: p( y \mid \bfn, \sigma^2_\text{obs} ) \: p( \bfn \mid \bX, \mu, k ) \intd \bfn.
  %\label{eqn:ml_full}
%\end{equation}
%%
%where $p( y \mid \bfn, \sigma^2_\text{obs})$ is given by \cref{eqn:likelihood_all} and $p( \bfn \mid \bX, \mu, k )$ is given by \cref{eqn:prior_finite}.
%Here we explicitly note the marginal likelihood's dependence on $\mu$, $k$, and $\sigma^2_\text{obs}$, though typically for brevity we will simply write it as $p(\by \mid \bX)$.
%Because $p( \bfn \mid \bX)$ and $p( y \mid \bfn)$ are both multivariate Gaussians, we can actually compute the integral in \cref{eqn:ml_full} in closed-form using common Gaussian identities (see e.g. \cite{bishop2006pattern,rasmussen2006gaussian}):
%%
%\begin{equation}
  %p( \by \mid \bX) = \normaldist{ \by; \bmu_\bX}{\trainK}, \quad
  %\trainK = \bK_{\bX\bX} + \sigma^2 \bI.
  %\label{eqn:ml_exact}
%\end{equation}

\subsection{Common Covariance Functions}
\label{sec:common_kernels}

\subsection{Approximate Gaussian Processes}

As we will discuss in detail in the next chapters, computing the marginal log likelihood (\cref{eqn:log_lik}) and the predictive distribution (\cref{eqn:predictive_mean,eqn:predictive_var}) has historically been computationally prohibitive for large datasets.
This has motivated the development of several approximate Gaussian process regression methods \citep[e.g.][]{quinonero2005unifying,snelson2006sparse,rahimi2008random,titsias2009variational,hensman2013gaussian,wilson2015kernel,gardner2018product,evans2018scalable} that trade off exactness for computational scalability.
Many of these methods approximate the training kernel matrix $\bK_{\bX\bX}$ with a structured or low-rank matrix $\approxK$ that affords faster matrix operations and requires less storage.

\paragraph{Inducing Points.}
A common mechanism for scalable methods is to introduce a set of pseudo-inputs, or \emph{inducing points}, which function as a sufficient statistic for the predictive distribution.
The kernel function for \emph{real}-inputs $\bx$, $\bx'$ is approximated by interpolating against the kernel matrix for the \emph{pseudo}-inputs $\bZ = [\bz_1, \ldots, \bz_M]$.
%
\begin{align}
  k(\bx, \bx') \approx \widetilde k(\bx, \bx') &= \:\:
		\overbracket{ \left( \bk_{\bZ\bx}^\top \bK_{\bZ\bZ}^{-1} \right) }^{\text{interpolation}} \:\: \bK_{\bZ\bZ} \:\:
		\overbracket{ \left( \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx'} 		 \right) }^{\text{interpolation}}
	\nonumber{} \\
	&= \:\:
	\bk_{\bZ\bx}^\top \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx'}.
	\label{eqn:inducing_interpolation}
\end{align}
%
In its simplest form, it is common to choose $M \ll N$ inducing points.
The approximate training kernel matrix $\widetilde \bK_{\bX\bX} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bx}$ will then have low-rank structure, allowing for efficient computation of \cref{eqn:predictive_mean,eqn:predictive_var,eqn:log_lik,eqn:log_lik_deriv}.
The locations of the inducing points $\bZ$ are considered to be additional parameters to optimize via \cref{eqn:log_lik} (typically with gradient descent).

Here we have presented inducing points as a mechanism for approximating kernel matrices.
Under this view, we can use the same training/prediction equations as presented in \cref{sec:gp_models,sec:gp_training}---replacing the kernel matrices/vectors with their approximations.
As we will discuss in \cref{chapter:bbmm}, this allows us to create a simple unifying framework to Gaussian process training and inference.
We do note it is also possible to motivate/derive inducing point methods in other ways---e.g. through a greedy approximation \cite{smola2001sparse} or a variational bound \cite{titsias2009variational,hensman2013gaussian}.
However, under these interpretations it is less straightforward to derive connections to \cref{eqn:predictive_mean,eqn:predictive_var,eqn:log_lik,eqn:log_lik_deriv}.

\paragraph{Variants.}
The basic formula of \cref{eqn:inducing_interpolation} can be extended to create a variety of approximate Gaussian process regression methods.
We briefly present three common variants here---all of which use $M \ll N$ inducing points and learn their locations through gradient descent of \cref{eqn:log_lik}.
In the next section we will discuss a fourth variant---Kernel Interpolation for Scalable Structured GPs (KISS-GP) \cite{wilson2015kernel}---which utilizes inducing points in a unique way.

\begin{itemize}
	\item {\bf Subset of Regressors (SoR)} \cite{silverman1985some,smola2001sparse} is arguably the most standard variant.
    Here, the kernel matrix $\bK_{\bX\bX}$ and all kernel vectors $\bk_{\bX\bxtest}$ are replaced with their approximate variants:
    \[ \widetilde \bK_{\bX\bX} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX}, \quad
       \widetilde \bk_{\bX\bxtest} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bk_{\bX\bxtest}. \]

	\item {\bf Fully Independent Training Conditional (FITC) } \cite{snelson2006sparse} can be interpreted as an extension of SoR
		where the approximate training kernel matrix has a diagonal correction term $\bLambda$.
    \[ \widetilde \bK_{\bX\bX} \approx \bK_{\bX\bZ} \bK_{\bZ\bZ}^{-1} \bK_{\bZ\bX} + \bLambda, \quad
       \Lambda^{(i,i)} = k(\bx_i, \bx_i) - \bk_{\bZ\bx_i}^\top \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx_i}. \]
    The diagonal correction term is motivated through a connection to approximate inference.

	\item {\bf Sparse Gaussian Process Regression (SGPR) } \cite{titsias2009variational} uses the same kernel approximations as SoR.
		However, when learning the inducing point locations and other parameters, SGPR uses an alternative objective function:
		the marginal log likelihood of \cref{eqn:log_lik} augmented with a diagonal correction term:
    \[ \loglik(\by \mid \bX, \btheta) = -\log p(\by \mid \bX, \btheta) + \sum_{i=1}^N \left( k(\bx_i, \bx_i) - \bk_{\bZ\bx_i}^\top \bK_{\bZ\bZ}^{-1} \bk_{\bZ\bx_i} \right). \]
    The diagonal correction term can be derived by optimizing a variational lower bound on the function values at inducing point locations.
\end{itemize}
%
We will discuss how to exploit this low-rank kernel matrix structure for faster inference in the next chapter.

\subsection{Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)}
Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP) \cite{wilson2015kernel} utilizes inducing points in a fundamentally different way.
While other methods tend to use $M \ll N$ inducing points, \citet{wilson2015kernel} demonstrate that it is possible to set $M \gg N$ by making the following restrictions:
\begin{enumerate}
  \item the interpolation terms in \cref{eqn:inducing_interpolation} are approximated by a \emph{sparse interpolation vector} $\bw_\bx \approx \bK^{-1}_{\bZ\bZ} \bk_{\bZ\bx};$
  \item the inducing points $\bZ = [\bz_1, \ldots, \bz_M]$ must lie on a \emph{regularly-spaced grid};
  \item the kernel function is \emph{stationary} (e.g. RBF, Mat\'ern, etc.).
\end{enumerate}
The resulting approximate kernel matrix has nice algebraic structure that can be exploited for efficient storage and fast computations.
In particular, the approximate kernel matrix $\widetilde \bK_{\bX\bX}$ affords fast \emph{matrix-vector multiplications} (MVMs), which will translate to fast Gaussian process training and inference (using the methods described in \cref{chapter:bbmm,chapter:love}).

In more detail, KISS-GP assumes that a data point $\bx$ is well-approximated as a \emph{local interpolation} of $\bZ$.
Using cubic interpolation \cite{keys1981cubic}, $\bx$ is expressed in terms of its 4 closest inducing points, and the interpolation weights are captured in the sparse vector $\bw_\bx$.
The $\bw_\bx$ vectors approximate the training kernel matrix $\bK_{\bX\bX} \approx \tilde \bK_{\bX\bX}$ via \cref{eqn:inducing_interpolation}:
%
\begin{align}
  \widetilde \bK_{\bX\bX} &= \:\:
    \left( \overbracket{ \bK_{\bZ\bX}^\top \bK_{\bZ\bZ}^{-1} }^{ \approx \bW_{\bX}^\top} \right)
    \bK_{\bZ\bZ}
    \left( \overbracket{ \bK_{\bZ\bX}^{-1} \bK_{\bZ\bX} }^{ \approx \bW_{\bX}} \right)
  \nonumber{} \\
  &\approx \:\: \bW_{\bX}^\top \: \bK_{\bZ\bZ} \: \bW_{\bX}.
  \label{eqn:ski}
\end{align}
%
Here, $\bW_{\bX} = [\bw_{\bx_1}, \ldots, \bw_{\bx_n}]$ contains the interpolation vectors for all $\bx_i$.
Performing a matrix-vector multiplication with $\widetilde \bK_{\bX\bX}$ (i.e. $( \bW_\bX^\top \bK_{\bZ\bZ} \bW_{\bX}) \bv$ for any vector $\bv$) takes \emph{near-linear} time.
To see why, a MVM with the matrix $\bW_\bX$ require $\bigo{N}$ time due to the $\bigo{N}$ sparsity of $\bW_\bX$.
Moreover, restrictions 2 and 3 (stationary kernel/grided inducing points) endow the inducing kernel matrix $\bK_{\bZ\bZ}$ with Toeplitz structure (or Kronecker/Toeplitz structure if the inputs $\bx_i$ are multi-dimensional).
By exploiting this structure, $\bK_{\bZ\bZ}$ requires $\bigo{m}$ storage and MVMs with $\bK_{\bZ\bZ}$ require $\bigo{m \log m}$ time \citep[see][for details]{cunningham2008fast,saatcci2012scalable}.
Thus, if we perform $( \bW_\bX^\top \bK_{\bZ\bZ} \bW_{\bX}) \bv$ from right-to-left, the entire multiplication takes $\bigo{N + M \log M}$ time.
As we will demonstrate in the next section, exploiting these fasts MVMs results in a near-linear training time complexity for KISS-GP.

%
\paragraph{Computing predictive means.}
One advantage of KISS-GP's structure is the ability to perform constant time predictive mean calculations \cite{wilson2015thoughts}.
Substituting the KISS-GP approximate kernel into \cref{eqn:predictive_mean} and assuming a prior mean of 0 for notational brevity, the predictive mean is given by
\begin{equation}
  \meantest{\bxtest} = \bw_{\bxtest}^\top {\blue{\underbracket{\bK_{\bZ\bZ} \bW_{\bX}(\bW_{\bX}^{\top}\bK_{\bZ\bZ}\bW_{\bX} + \sigma_\text{obs}^{2} \bI)^{-1} \by}_{\ba}}}.
  \label{eq:pred_mean_ski}
\end{equation}
(Here {\color{blue} blue} highlights computations that don't depend on test data.)
Because $\bw_{\bx^{*}}$ is the only term in \eqref{eq:pred_mean_ski} that depends on $\bx^{*}$, the remainder of the equation (denoted as $\blue\ba$) can be pre-computed: $\meantest{\bxtest} = \bw_{\bxtest}^\top \blue \ba$.
After pre-computing $\blue \ba$, the multiplication $\bw_{\bx^{*}}^\top \blue \ba$ requires $\bigo{1}$ time, as $\bw_{\bx^{*}}$ is sparse and has only four nonzero elements.

\section{Summary of Notation}

\begin{table}[h!]
  \centerfloat
  \caption{Summary of Notation}
	\resizebox{1.2\textwidth}{!}{%
		\begin{tabular}{ ccc }
			\toprule
			{\bf Notation} & {\bf Domain} & {\bf Description} \\
			\midrule
			\midrule
			$\numdata$ & $\ints$ & Number of training data points \\
			$\numdim$ & $\ints$ & Dimensionality of inputs \\
			$\numinduc$ & $\ints$ & Number of inducing data points \\
			\midrule
			$\bX$ & $\reals^{\numdata \times \numdim}$ & Training features \\
			$\by$ & $\reals^{\numdata}$ & Training targets \\
			$\bxtest$ & $\reals^{\numdim}$ & Features of a test data point \\
			$\ytest$ & $\reals$ & Target of a test data point \\
			\midrule
			$\mu( \cdot )$ & $\reals^\numdim \rightarrow \reals$ & The GP (prior) mean function \\
			$k( \cdot, \cdot )$ & $\reals^\numdim \times \reals^\numdim \rightarrow \reals$ & The GP (prior) covariance/kernel function \\
			$\sigma_\text{obs}^2$ & $\reals$ & Observational variance of the Gaussian likelihood \\
			$f( \cdot )$ & $\reals^\numdim \rightarrow \reals$ & Latent function (modelled with a GP prior) \\
			\midrule
			$\bmu_\bX$ & $\reals^\numdata$ & (Prior) mean vector for training data $\bX$ \\
			$\bK_{\bX\bX}$ & $\reals^{\numdata \times \numdata}$ & (Prior) kernel matrix for training data $\bX$ \\
			$\trainK$ & $\reals^{\numdata \times \numdata}$ & (Prior) kernel matrix $\bK_{\bX\bX}$ plus observational noise $\sigma^2_\text{obs} \bI$ \\
			$\bk_{\bX\bxtest}$ & $\reals^{\numdata}$ & Prior covariance between training data $\bX$ and point $\bxtest$\\
			$\bfn$ & $\reals^\numdata$ & Latent function evaluated on training data $\bX$ \\
			\midrule
			$\bZ$ & $\reals^{\numinduc \times \numdim}$ & Locations of inducing points \\
			$\bK_{\bZ\bZ}$ & $\reals^{\numinduc \times \numinduc}$ & (Prior) kernel matrix for inducing points $\bZ$ \\
			$\bk_{\bZ\bx}$ & $\reals^{\numinduc}$ & Prior covariance between inducing points $\bZ$ and point $\bx$ \\
			$\bW_{\bX}$ & $\reals^{\numdata \times \numinduc}$ & Sparse inducing-interpolation matrix for training data $\bX$ (using \cref{eqn:ski}) \\
			$\bw_{\bx}$ & $\reals^{\numinduc}$ & Sparse inducing-interpolation vector for point $\bx$ (using \cref{eqn:ski}) \\
			\midrule
			$\widetilde k(\cdot, \cdot)$ & $\reals^\numdata \times \reals^\numdata \rightarrow \reals$ & Approximate kernel function (using \cref{eqn:inducing_interpolation}) \\
			$\widetilde \bK_{\bX\bX}$ & $\reals^{\numdata \times \numdata}$ & Approximate training kernel matrix (using \cref{eqn:inducing_interpolation}) \\
			\midrule
			$\meantest{ \cdot }$ & $\reals^\numdim \rightarrow \reals$ & Predictive mean function of the GP model \\
			$\covtest{ \cdot }$ & $\reals^\numdim \rightarrow \reals$ & Predictive variance function of the GP model\\
			\bottomrule
		\end{tabular}
	}
\end{table}
